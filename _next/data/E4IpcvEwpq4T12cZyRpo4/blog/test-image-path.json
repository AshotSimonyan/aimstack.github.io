{"pageProps":{"post":{"title":"Test image path","date":"2023-01-14T18:17:57.545Z","author":"Ash","description":"test post","slug":"test-image-path","image":"/images/blog/sum.png","draft":false,"categories":["Test"],"body":{"raw":"<!--StartFragment-->\n\n## Where does it come from?\n\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the <h5> cites</h5> of the word in classical **literature**, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by <Highlight><Flex align='center' css={{ marginTop: '$10' }}>\n  <Icon name='back' size={20} />\n  <Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  </Text>\n</Flex></Highlight>\n\n<!--EndFragment-->\n\n```jsx\n<Flex align='center' css={{ marginTop: '$10' }}>\n  <Icon name='back' size={20} />\n  <Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  </Text>\n</Flex>\n```","html":"<h2>Where does it come from?</h2>\n<p>Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the  cites of the word in classical <strong>literature</strong>, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32</p>\n<p>The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by &#x3C;Flex align='center' css={{ marginTop: '$10' }}>\n\n&#x3C;Text size={3} css={{ fontWeight: '$3' }}>\nGo Back\n\n</p>\n<pre><code class=\"language-jsx\">&#x3C;Flex align='center' css={{ marginTop: '$10' }}>\n  &#x3C;Icon name='back' size={20} />\n  &#x3C;Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  &#x3C;/Text>\n&#x3C;/Flex>\n</code></pre>"},"_id":"posts/test-image-path.md","_raw":{"sourceFilePath":"posts/test-image-path.md","sourceFileName":"test-image-path.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/test-image-path"},"type":"Post"},"posts":[{"title":"Article post 17","date":"2022-03-20T17:11:14.000Z","author":"Rajdeep Singh","description":"Cat","slug":"article-post-17","image":"/images/contentlayer.png","draft":false,"tags":["neovim","Linux","linux editor"],"categories":["neovim","Linux","linux editor"],"body":{"raw":"B﻿ody\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_26BhViw28s\" title=\"YouTube video player\"\n              frameBorder=\"0\"\n              allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n              allowFullScreen/>\n","html":"<p>B﻿ody</p>"},"_id":"posts/article-post-17.md","_raw":{"sourceFilePath":"posts/article-post-17.md","sourceFileName":"article-post-17.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/article-post-17"},"type":"Post"},{"title":"Article post 18","date":"2022-03-21T17:11:14.000Z","author":"Rajdeep Singh","description":"","slug":"article-post-18","image":"/images/contentlayer.png","draft":false,"tags":["neovim","Linux","linux editor"],"categories":["neovim","Linux","linux editor"],"body":{"raw":"\narticle post 17","html":"<p>article post 17</p>"},"_id":"posts/article-post-18.md","_raw":{"sourceFilePath":"posts/article-post-18.md","sourceFileName":"article-post-18.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/article-post-18"},"type":"Post"},{"title":"Article post 19","date":"2022-03-19T17:11:14.000Z","author":"Rajdeep Singh","description":"","slug":"article-post-19","image":"/images/contentlayer.png","draft":false,"tags":["neovim","Linux","linux editor"],"categories":["neovim","Linux","linux editor"],"body":{"raw":"\narticle post 19","html":"<p>article post 19</p>"},"_id":"posts/article-post-19.md","_raw":{"sourceFilePath":"posts/article-post-19.md","sourceFileName":"article-post-19.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/article-post-19"},"type":"Post"},{"title":"Article post 20","date":"2022-03-19T17:11:14.000Z","author":"Rajdeep Singh","description":"","slug":"article-post-20","image":"/images/contentlayer.png","draft":false,"tags":["neovim","Linux","linux editor"],"categories":["neovim","Linux","linux editor"],"body":{"raw":"\narticle post 20","html":"<p>article post 20</p>"},"_id":"posts/article-post-20.md","_raw":{"sourceFilePath":"posts/article-post-20.md","sourceFileName":"article-post-20.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/article-post-20"},"type":"Post"},{"title":"Article post 21","date":"2022-03-25T17:11:14.000Z","author":"Rajdeep Singh","description":"","slug":"article-post-21","image":"/images/contentlayer.png","draft":false,"tags":["neovim","Linux","linux editor"],"categories":["neovim","Linux","linux editor"],"body":{"raw":"\narticle post 21","html":"<p>article post 21</p>"},"_id":"posts/article-post-21.md","_raw":{"sourceFilePath":"posts/article-post-21.md","sourceFileName":"article-post-21.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/article-post-21"},"type":"Post"},{"title":"Article post 22","date":"2022-03-19T17:11:14.000Z","author":"Rajdeep Singh","description":"","slug":"article-post-22","image":"/images/contentlayer.png","draft":false,"tags":["neovim","Linux","linux editor"],"categories":["neovim","Linux","linux editor"],"body":{"raw":"\narticle post 22\n","html":"<p>article post 22</p>"},"_id":"posts/article-post-22.md","_raw":{"sourceFilePath":"posts/article-post-22.md","sourceFileName":"article-post-22.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/article-post-22"},"type":"Post"},{"title":"Check image functinalty working or not","date":"2022-08-16T14:56:38.839Z","author":"Rajdeep singh","description":"react, test","slug":"check-image-functinalty-working-or-not","image":"/images/butterfly-7353884_960_720.webp","draft":false,"tags":["javascript","react"],"categories":["react","test"],"body":{"raw":"in this check Check image functinalty working or not","html":"<p>in this check Check image functinalty working or not</p>"},"_id":"posts/check-image-functinalty-working-or-not.md","_raw":{"sourceFilePath":"posts/check-image-functinalty-working-or-not.md","sourceFileName":"check-image-functinalty-working-or-not.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/check-image-functinalty-working-or-not"},"type":"Post"},{"title":"DONE","date":"2022-12-26T22:16:08.091Z","author":"Ashot","description":"aaaa","slug":"done","image":"/images/group-580.png","draft":false,"tags":["asd"],"categories":["test"],"body":{"raw":"<!--StartFragment-->\n\n## Where does it come from?\n\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\n\n<!--EndFragment-->","html":"<h2>Where does it come from?</h2>\n<p>Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.</p>\n<p>The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.</p>"},"_id":"posts/done.md","_raw":{"sourceFilePath":"posts/done.md","sourceFileName":"done.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/done"},"type":"Post"},{"title":"Finally working fine ","date":"2022-08-16T15:10:42.916Z","author":"Rajdeep Singh","description":"Check all working functionality ","slug":"finally-working-fine","image":"/images/birds-7327611_960_720.webp","draft":false,"tags":["ImageBlog"],"categories":["blog"],"body":{"raw":"Fully functional blog.\n\nHappy coding","html":"<p>Fully functional blog.</p>\n<p>Happy coding</p>"},"_id":"posts/finally-working-fine.md","_raw":{"sourceFilePath":"posts/finally-working-fine.md","sourceFileName":"finally-working-fine.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/finally-working-fine"},"type":"Post"},{"title":"Finaly testing all functinalty","date":"2022-08-16T14:21:07.718Z","author":"Rajdeep Singh","description":"Finally every thing is fine","slug":"finaly-testing-all-functinalty","image":"/images/contentlayer.png","draft":false,"tags":["Blog","written","testing"],"categories":["Blog","written","testing"],"body":{"raw":"Welcome to my article.\n\nHave nice day","html":"<p>Welcome to my article.</p>\n<p>Have nice day</p>"},"_id":"posts/finaly-testing-all-functinalty.md","_raw":{"sourceFilePath":"posts/finaly-testing-all-functinalty.md","sourceFileName":"finaly-testing-all-functinalty.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/finaly-testing-all-functinalty"},"type":"Post"},{"title":"First article is here how are you ","date":"2022-08-16T11:17:52.844Z","author":"Rajdeep Singh","description":"This is demo posts form netlify dashborad","slug":"first-article-is-here-how-are-you","image":"/images/patreon-cover.png","draft":false,"tags":["First","build","javascript"],"categories":["First","build","javascript"],"body":{"raw":"My first post here.Now i'm post from netlify dashborad with markdown. check working or not.\n\n","html":"<p>My first post here.Now i'm post from netlify dashborad with markdown. check working or not.</p>"},"_id":"posts/first-article-is-here-how-are-you.md","_raw":{"sourceFilePath":"posts/first-article-is-here-how-are-you.md","sourceFileName":"first-article-is-here-how-are-you.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/first-article-is-here-how-are-you"},"type":"Post"},{"title":"First Test Blog","date":"2022-12-26T18:25:52.971Z","author":"Ashot","description":"Test Post","slug":"first-test-blog","image":"/images/316131505_1324557514970643_449084528564027250_n.jpg","draft":false,"tags":["asd"],"categories":["Release"],"body":{"raw":"t﻿his is a code block\n\n![code](/images/sum.png \"code\")","html":"<p>t﻿his is a code block</p>\n<p><img src=\"/images/sum.png\" alt=\"code\" title=\"code\"></p>"},"_id":"posts/first-test-blog.md","_raw":{"sourceFilePath":"posts/first-test-blog.md","sourceFileName":"first-test-blog.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/first-test-blog"},"type":"Post"},{"title":"What is the npm install command?","date":"2022-03-20T13:09:24.000Z","author":"Rajdeep Singh","description":"Npm install command help to install package from npmjs.org","slug":"how-is-npm-install-command","id":1,"image":"/images/npm-init-command-1.png","draft":false,"tags":["npm","npm-cli","npm install command"],"categories":["npm","npm-cli","npm install command"],"body":{"raw":"Npm, install command help to install dependencies and devDependencies base on package.json and package-lock.json file.\n\n**The simple word npm install command help to download the package from** **[npmjs.org](https://www.npmjs.com/).**\n\n```javascript\nnpm install <Options> <flags>\n\nor\n\nnpm i <Options> <flags>\n\nor\n\nnpm add <Options> <flags>\n```\n\nYou can install any package base on two methods. I know there are other methods, but I created my way to quickly teach you the npm install command.\n\n> I cover the most important options which you use every day. npm I, npm install, and npm add is one command.\n\n1. Syntax\n2. Options or Flags\n\n## Syntax\n\n1. npm install\n2. npm package  **`<name>`**\n3. npm install **`<name>@<tag>`**\n4. npm install **`<name>@<version>`**\n5. npm install **`<git repo URL>`**\n\n### npm install\n\nnpm install command is the most used command in the npm world. The npm install command downloads all packages from the [npmjs](https://www.npmjs.com/) website and store them in the node_modules folder.\n\n```json\n{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"next\": \"12.1.0\",\n    \"react\": \"17.0.2\",\n    \"react-dom\": \"17.0.2\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"8.11.0\",\n    \"eslint-config-next\": \"12.1.0\"\n  }\n}\n```\n\nNpm, install command install package based on package.json file. Inside the package.json file, npm checks dependencies and the devDependencies section. Then, based on dependencies and devDependencies, npm starts installing the package locally.\n\n### npm package **`<name>`**\n\nYou can install a package based on the package name. So you mention the package name in the npm install command. Then, Npm directly installed the package into your **node_modules** folder locally.\n\nWhen downloading another package or starting with a new project, the npm-cli also updates your package.json file and mentions the package name by default in the dependencies section.\n\n```cmd\n    npm install react@latest\n```\n\n- - -\n\n```json\n{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {},\n  \"dependencies\": {\n    \"react\": \"17.0.2\"\n  },\n \n}\n```\n\n### npm install `<name>@<tag>`\n\nYou can install the package base on the tag if the author defines the tags in the package on uploading to [npmjs](https://www.npmjs.com/).\n\nIn a simple word, the tag is the name of the version. The package's author gives the name of a different version, i.e., version 1.0.0, the tag name first.\n\nBy default, npm provide the latest tag for every npm package. The latest tag means the current version of the package.\n\n```cmd\nnpm install react-dom@latest\n```\n\n### npm install `<name>@<version>`\n\nyou can install the package base on the version. Every package has a different version, and you can install a specific version of the npm package install in your project.\n\n```cmd\n    npm install react@16.1.1\n```\n\n### npm install `<git repo URL>`\n\nYou can install the package base on your git repo URL. npm install command helps to install the package directly from Github.\n\n```cmd\n     npm install https://github.com/facebook/react.git\n```\n\n> Firstly install code locally from GitHub and then run npm install command inside folder code folder.\n\n## Options or Flags\n\n1. \\-g option\n2. \\-P or --save-prod option\n3. \\-D or --save-dev option\n\n### \\-g option\n\n\\-g flag or option helps the install the package globally in your machine. in Syntax ways, npm installs all packages in the working node_modules folder.\n\nGlobally means you access package cli anywhere in laptop.\n\n```cmd\n  npx -g create-react-app \n```\n\n### \\-P or --save-prod option\n\nThe -P or --save-prod option helps add your package into the production or dependencies section in the package.json file.\n\nBy default, the npm install command adds the package into production.\n\n```cmd\n  npm install -P next@latest\n    \n    or\n    \n  npm install --save-prod next@latest\n```\n\n- - -\n\n```json\n{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"next\": \"^12.1.0\",\n    \"react\": \"17.0.2\",\n    \"react-dom\": \"17.0.2\"\n  }\n}\n```\n\n### \\-D or --save-dev option\n\nThe -D or --save-dev option helps add your package into the devDependencies section in the package.json file.\n\n```cmd\n  npm install -D eslint@latest\n    \n    or\n    \n  npm install --save-dev eslint@latest\n  \n```\n\n- - -\n\n```json\n{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"next\": \"^12.1.0\",\n    \"react\": \"17.0.2\",\n    \"react-dom\": \"17.0.2\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"8.11.0\",\n    \"eslint-config-next\": \"12.1.0\"\n  }\n}\n```\n\n- - -\n\n### References\n\nhttps://docs.npmjs.com/cli/v6/commands/npm-install\n\n- - -\n","html":"<p>Npm, install command help to install dependencies and devDependencies base on package.json and package-lock.json file.</p>\n<p><strong>The simple word npm install command help to download the package from</strong> <strong><a href=\"https://www.npmjs.com/\">npmjs.org</a>.</strong></p>\n<pre><code class=\"language-javascript\">npm install &#x3C;Options> &#x3C;flags>\n\nor\n\nnpm i &#x3C;Options> &#x3C;flags>\n\nor\n\nnpm add &#x3C;Options> &#x3C;flags>\n</code></pre>\n<p>You can install any package base on two methods. I know there are other methods, but I created my way to quickly teach you the npm install command.</p>\n<blockquote>\n<p>I cover the most important options which you use every day. npm I, npm install, and npm add is one command.</p>\n</blockquote>\n<ol>\n<li>Syntax</li>\n<li>Options or Flags</li>\n</ol>\n<h2>Syntax</h2>\n<ol>\n<li>npm install</li>\n<li>npm package  <strong><code>&#x3C;name></code></strong></li>\n<li>npm install <strong><code>&#x3C;name>@&#x3C;tag></code></strong></li>\n<li>npm install <strong><code>&#x3C;name>@&#x3C;version></code></strong></li>\n<li>npm install <strong><code>&#x3C;git repo URL></code></strong></li>\n</ol>\n<h3>npm install</h3>\n<p>npm install command is the most used command in the npm world. The npm install command downloads all packages from the <a href=\"https://www.npmjs.com/\">npmjs</a> website and store them in the node_modules folder.</p>\n<pre><code class=\"language-json\">{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"next\": \"12.1.0\",\n    \"react\": \"17.0.2\",\n    \"react-dom\": \"17.0.2\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"8.11.0\",\n    \"eslint-config-next\": \"12.1.0\"\n  }\n}\n</code></pre>\n<p>Npm, install command install package based on package.json file. Inside the package.json file, npm checks dependencies and the devDependencies section. Then, based on dependencies and devDependencies, npm starts installing the package locally.</p>\n<h3>npm package <strong><code>&#x3C;name></code></strong></h3>\n<p>You can install a package based on the package name. So you mention the package name in the npm install command. Then, Npm directly installed the package into your <strong>node_modules</strong> folder locally.</p>\n<p>When downloading another package or starting with a new project, the npm-cli also updates your package.json file and mentions the package name by default in the dependencies section.</p>\n<pre><code class=\"language-cmd\">    npm install react@latest\n</code></pre>\n<hr>\n<pre><code class=\"language-json\">{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {},\n  \"dependencies\": {\n    \"react\": \"17.0.2\"\n  },\n \n}\n</code></pre>\n<h3>npm install <code>&#x3C;name>@&#x3C;tag></code></h3>\n<p>You can install the package base on the tag if the author defines the tags in the package on uploading to <a href=\"https://www.npmjs.com/\">npmjs</a>.</p>\n<p>In a simple word, the tag is the name of the version. The package's author gives the name of a different version, i.e., version 1.0.0, the tag name first.</p>\n<p>By default, npm provide the latest tag for every npm package. The latest tag means the current version of the package.</p>\n<pre><code class=\"language-cmd\">npm install react-dom@latest\n</code></pre>\n<h3>npm install <code>&#x3C;name>@&#x3C;version></code></h3>\n<p>you can install the package base on the version. Every package has a different version, and you can install a specific version of the npm package install in your project.</p>\n<pre><code class=\"language-cmd\">    npm install react@16.1.1\n</code></pre>\n<h3>npm install <code>&#x3C;git repo URL></code></h3>\n<p>You can install the package base on your git repo URL. npm install command helps to install the package directly from Github.</p>\n<pre><code class=\"language-cmd\">     npm install https://github.com/facebook/react.git\n</code></pre>\n<blockquote>\n<p>Firstly install code locally from GitHub and then run npm install command inside folder code folder.</p>\n</blockquote>\n<h2>Options or Flags</h2>\n<ol>\n<li>-g option</li>\n<li>-P or --save-prod option</li>\n<li>-D or --save-dev option</li>\n</ol>\n<h3>-g option</h3>\n<p>-g flag or option helps the install the package globally in your machine. in Syntax ways, npm installs all packages in the working node_modules folder.</p>\n<p>Globally means you access package cli anywhere in laptop.</p>\n<pre><code class=\"language-cmd\">  npx -g create-react-app \n</code></pre>\n<h3>-P or --save-prod option</h3>\n<p>The -P or --save-prod option helps add your package into the production or dependencies section in the package.json file.</p>\n<p>By default, the npm install command adds the package into production.</p>\n<pre><code class=\"language-cmd\">  npm install -P next@latest\n    \n    or\n    \n  npm install --save-prod next@latest\n</code></pre>\n<hr>\n<pre><code class=\"language-json\">{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"next\": \"^12.1.0\",\n    \"react\": \"17.0.2\",\n    \"react-dom\": \"17.0.2\"\n  }\n}\n</code></pre>\n<h3>-D or --save-dev option</h3>\n<p>The -D or --save-dev option helps add your package into the devDependencies section in the package.json file.</p>\n<pre><code class=\"language-cmd\">  npm install -D eslint@latest\n    \n    or\n    \n  npm install --save-dev eslint@latest\n  \n</code></pre>\n<hr>\n<pre><code class=\"language-json\">{\n  \"name\": \"my-app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"next\": \"^12.1.0\",\n    \"react\": \"17.0.2\",\n    \"react-dom\": \"17.0.2\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"8.11.0\",\n    \"eslint-config-next\": \"12.1.0\"\n  }\n}\n</code></pre>\n<hr>\n<h3>References</h3>\n<p>https://docs.npmjs.com/cli/v6/commands/npm-install</p>\n<hr>"},"_id":"posts/how-is-npm-install-command.md","_raw":{"sourceFilePath":"posts/how-is-npm-install-command.md","sourceFileName":"how-is-npm-install-command.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-is-npm-install-command"},"type":"Post"},{"title":"How To Add CSS In Next js?","date":"2020-11-10T11:42:46.000Z","author":"Rajdeep Singh","description":"Easy Ways Add CSS in Next.js #SeriesPart2","slug":"how-to-add-css-in-next-js","image":"/images/next.js-add-css-code.jpg","draft":false,"tags":["Next.js","Next","Next.js Framework","Next.js Tutorial","React.js","react.js tutorial"],"categories":["Next.js","Next","Next.js Framework","Next.js Tutorial","React.js","react.js tutorial"],"body":{"raw":"\nIn this Next Series, we Learn How to add CSS's own Project with Easy Steps.\n\nGood News is that Next.js provides custom CSS functionality. You Use The next.js plugin inside your project and use it.\n\n## What's Next.js?\n\nMake sure Read Basic Introduction About Next.js #SeriesStart 💕\n\n[https://officialrajdeepsingh.dev/what-is-next.js/](https://officialrajdeepsingh.dev/what-is-next.js/ \"https://officialrajdeepsingh.dev/what-is-next.js/\")\n\n***\n\n## New Update:\n\nNext.js Version 9.3 **Support CSS Global Stylesheets.** Now you add CSS directly Import `.css` files as global stylesheets.\n\n```javascript\nimport './style.css'\n```\n\n***\n\n**Go To Github Download or Use Npm:**\n\n```cmd\nnpm install --save @zeit/next-css\nor\nyarn add @zeit/next-css\n```\n\n***\n\nCreate an `next.config.ts` At the root of your project\n\n## Default:\n\ndefault config use for import CSS Global stylesheet in custom _app.tsx\n\n```javascript\nconst withCSS = require('@zeit/next-css')\nmodule.exports = withCSS({})\n```\n\n## Custom:\n\nCustom config used for import CSS in other Components like header, footer like so.\n\n```javascript\nconst withCSS = require('@zeit/next-css')\nmodule.exports = withCSS({\ncssModules: true  // After true than use import statement in next.js\n})\n```\n\n***\n\n## How To add Global CSS:\n\nWhen you create your app, help with npm. in the next step, you create a global app. If you npm official command the by default app create in your project and import your Global CSS file in next.js custom _app.tsx\n\n```javascript\nimport '../styles.css'\nor\nimport '../styles.scss'\n// This default export is required in a new `pages/_app.tsx` file.\nexport default function MyApp({ Component, pageProps }) {\n  return <Component {...pageProps} />\n}\n```\n\n***\n\n# Demos:\n\n<iframe width=\"924\" height=\"500\" src=\"https://codesandbox.io/embed/add-css-innextjs-8pdds?from-embed\" style=\"border:0; border-radius: 4px; overflow:hidden;\" sandbox=\"allow-modals allow-forms allow-popups allow-scripts allow-same-origin\"></iframe>\n\n***\n\n## Reference:\n\nhttps://nextjs.org/docs/basic-features/built-in-css-support\n\nhttps://nextjs.org/blog/next-9-2\n\nhttps://nextjs.org/blog/next-10\n\n***\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://www.facebook.com/groups/JavaScriptcode/](https://www.facebook.com/groups/JavaScriptcode/)\n* [https://www.facebook.com/groups/pythoncodejoin/](https://www.facebook.com/groups/pythoncodejoin/)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n","html":"<p>In this Next Series, we Learn How to add CSS's own Project with Easy Steps.</p>\n<p>Good News is that Next.js provides custom CSS functionality. You Use The next.js plugin inside your project and use it.</p>\n<h2>What's Next.js?</h2>\n<p>Make sure Read Basic Introduction About Next.js #SeriesStart 💕</p>\n<p><a href=\"https://officialrajdeepsingh.dev/what-is-next.js/\" title=\"https://officialrajdeepsingh.dev/what-is-next.js/\">https://officialrajdeepsingh.dev/what-is-next.js/</a></p>\n<hr>\n<h2>New Update:</h2>\n<p>Next.js Version 9.3 <strong>Support CSS Global Stylesheets.</strong> Now you add CSS directly Import <code>.css</code> files as global stylesheets.</p>\n<pre><code class=\"language-javascript\">import './style.css'\n</code></pre>\n<hr>\n<p><strong>Go To Github Download or Use Npm:</strong></p>\n<pre><code class=\"language-cmd\">npm install --save @zeit/next-css\nor\nyarn add @zeit/next-css\n</code></pre>\n<hr>\n<p>Create an <code>next.config.ts</code> At the root of your project</p>\n<h2>Default:</h2>\n<p>default config use for import CSS Global stylesheet in custom _app.tsx</p>\n<pre><code class=\"language-javascript\">const withCSS = require('@zeit/next-css')\nmodule.exports = withCSS({})\n</code></pre>\n<h2>Custom:</h2>\n<p>Custom config used for import CSS in other Components like header, footer like so.</p>\n<pre><code class=\"language-javascript\">const withCSS = require('@zeit/next-css')\nmodule.exports = withCSS({\ncssModules: true  // After true than use import statement in next.js\n})\n</code></pre>\n<hr>\n<h2>How To add Global CSS:</h2>\n<p>When you create your app, help with npm. in the next step, you create a global app. If you npm official command the by default app create in your project and import your Global CSS file in next.js custom _app.tsx</p>\n<pre><code class=\"language-javascript\">import '../styles.css'\nor\nimport '../styles.scss'\n// This default export is required in a new `pages/_app.tsx` file.\nexport default function MyApp({ Component, pageProps }) {\n  return &#x3C;Component {...pageProps} />\n}\n</code></pre>\n<hr>\n<h1>Demos:</h1>\n<hr>\n<h2>Reference:</h2>\n<p>https://nextjs.org/docs/basic-features/built-in-css-support</p>\n<p>https://nextjs.org/blog/next-9-2</p>\n<p>https://nextjs.org/blog/next-10</p>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://www.facebook.com/groups/JavaScriptcode/\">https://www.facebook.com/groups/JavaScriptcode/</a></li>\n<li><a href=\"https://www.facebook.com/groups/pythoncodejoin/\">https://www.facebook.com/groups/pythoncodejoin/</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>"},"_id":"posts/how-to-add-css-in-next-js.md","_raw":{"sourceFilePath":"posts/how-to-add-css-in-next-js.md","sourceFileName":"how-to-add-css-in-next-js.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-add-css-in-next-js"},"type":"Post"},{"title":"How to capture screenshots in Raspberry PI 4","date":"2021-01-24T12:26:07.000Z","author":"Rajdeep Singh","description":"Raspi does not provide screenshots functionality by default. you use software and tool to take a screenshot in raspi.","slug":"how-to-capture-screenshots-in-raspberry-pi-4","id":4,"image":"/images/How-to-capture-screenshots-in-Raspberry-PI-4.png","draft":false,"tags":["Raspberry","Raspi 4","screensshots","capture screen shots","Gnome Screenshot","install gnome screenshot"],"categories":["Raspberry","Raspi 4","screensshots","capture screen shots","Gnome Screenshot","install gnome screenshot"],"body":{"raw":"\n\n\nIn Raspberry pi 4, you take the screenshot help of two software. The first is scort, and the second is Gnome Screenshot.\n\nI'm personally recommended you use Gnome Screenshot. Gnome Screenshot provides a graphical interface. You use a graphical interface to take screenshots very easily.\n\nOther hands scort is a command-line tool, and scort does not provide any graphical interface.So I'm personally again recommended you use Gnome Screenshot.\n\n### Install Gnome Screenshot in Raspbian or Raspberry pi 4:\n\n**let start it:**\n\nThe Raspi 4 does not provide by default Gnome Screenshot.\n\nFirstly Update your raspberry pi 4. this is a compulsory step for installing Gnome Screenshot.\n\n```cmd\nsudo apt update && sudo apt upgrade\n```\n\nYour update and upgrade command run successfully. Now you install Gnome Screenshot following command.\n\n```cmd\nsudo apt install gnome-screenshot\n```\n\nNow you can find Gnome Screenshot in your accessories menu, listed as \"Screenshot.\"\n\n![sceenshot](https://ephemeral-rolypoly-2f9988.netlify.app/images/geenome.png)\n\n\nGnome interface is very easy to understand. You use Gnome to get a Screenshot, add your own image name, and after save it. By default, all pic save inside your Picture folder.\n\n![interface look like](https://ephemeral-rolypoly-2f9988.netlify.app/images/gnome-screenshots.png)\n\n\n---\n\n### Uninstall Gnome Screenshot:\n\nIn some cases, you may uninstall the Gnome Screenshot. so follow this command\n\n```cmd\nsudo apt-get remove --purge gnome-screenshot\n```\n\n---\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://medium.com/officialrajdeepsingh](https://medium.com/officialrajdeepsingh)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n\n\n\n","html":"<p>In Raspberry pi 4, you take the screenshot help of two software. The first is scort, and the second is Gnome Screenshot.</p>\n<p>I'm personally recommended you use Gnome Screenshot. Gnome Screenshot provides a graphical interface. You use a graphical interface to take screenshots very easily.</p>\n<p>Other hands scort is a command-line tool, and scort does not provide any graphical interface.So I'm personally again recommended you use Gnome Screenshot.</p>\n<h3>Install Gnome Screenshot in Raspbian or Raspberry pi 4:</h3>\n<p><strong>let start it:</strong></p>\n<p>The Raspi 4 does not provide by default Gnome Screenshot.</p>\n<p>Firstly Update your raspberry pi 4. this is a compulsory step for installing Gnome Screenshot.</p>\n<pre><code class=\"language-cmd\">sudo apt update &#x26;&#x26; sudo apt upgrade\n</code></pre>\n<p>Your update and upgrade command run successfully. Now you install Gnome Screenshot following command.</p>\n<pre><code class=\"language-cmd\">sudo apt install gnome-screenshot\n</code></pre>\n<p>Now you can find Gnome Screenshot in your accessories menu, listed as \"Screenshot.\"</p>\n<p><img src=\"https://ephemeral-rolypoly-2f9988.netlify.app/images/geenome.png\" alt=\"sceenshot\"></p>\n<p>Gnome interface is very easy to understand. You use Gnome to get a Screenshot, add your own image name, and after save it. By default, all pic save inside your Picture folder.</p>\n<p><img src=\"https://ephemeral-rolypoly-2f9988.netlify.app/images/gnome-screenshots.png\" alt=\"interface look like\"></p>\n<hr>\n<h3>Uninstall Gnome Screenshot:</h3>\n<p>In some cases, you may uninstall the Gnome Screenshot. so follow this command</p>\n<pre><code class=\"language-cmd\">sudo apt-get remove --purge gnome-screenshot\n</code></pre>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://medium.com/officialrajdeepsingh\">https://medium.com/officialrajdeepsingh</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>"},"_id":"posts/how-to-capture-screenshots-in-raspberry-pi-4.md","_raw":{"sourceFilePath":"posts/how-to-capture-screenshots-in-raspberry-pi-4.md","sourceFileName":"how-to-capture-screenshots-in-raspberry-pi-4.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-capture-screenshots-in-raspberry-pi-4"},"type":"Post"},{"title":"How to check the snap store package available for raspberry pi 4 or not?","date":"2022-03-26T07:50:39.000Z","author":"Rajdeep Singh","description":"You can easily check the raspberry pi binary in the snap store without the command line and code.","slug":"how-to-check-the-snap-store-package-available-for-raspberry-pi-4-or-not","id":5,"image":"/images/the-snap-store.png","draft":false,"tags":["Raspberry pi 4","snapcraft","Snap store","Linux","Binary"],"categories":["Raspberry pi 4","snapcraft","Snap store","Linux","Binary"],"body":{"raw":"\n\nUbuntu built the snap store package Eco-system. The Snap store help to distribute your Linux base application and software across the Linux distro.\n\nFor checking the binary, you do not need any command. You need only two things. First is a web browser and a mouse.\n\nThere is two binary support by raspberry pi 4. first is **arm64** and **armhf**. If one of the binary is present, you can install the package in your raspberry pi 4 without any problem.\n\n\n\n**There is two way to check the package binary is available for raspberry pi 4 or not.**\n\n1. First way\n2. Second way\n\n\n\n> In this article, I use the snap chromium package for example purpose.\n\n\n## First way\n\nFirstly, you click on **the version drop-down list** and click again to **show the architecture** drop-down list and check all available binary forms for the list.\n\n* First step\n* Second step\n\n### First step\n\n![version drop-down list](https://contentlayer-iota.vercel.app/images/firstway.png)\n\n\n\n### Second step\n\n![show architecture drop-down list](https://contentlayer-iota.vercel.app/images/firstway-2.png)\n\n\n## Second way\n\nIn a second way, you scroll down the page, go to the Linux distribution section, and check all the Linux distributions.\n\n![Linux distribution section](https://contentlayer-iota.vercel.app/images/secondway.png)\n\n\n---\n\n### Reference\n\nhttps://snapcraft.io/chromium\n\nhttps://snapcraft.io/store\n\nhttps://snapcraft.io/docs\n\n---\n\n## Conclusion\n\nI hope my article solve your problem if you have any problem and then comment in the comment section.\n\n**For more updates, subscribe to our newsletter.**\n\n\n\n","html":"<p>Ubuntu built the snap store package Eco-system. The Snap store help to distribute your Linux base application and software across the Linux distro.</p>\n<p>For checking the binary, you do not need any command. You need only two things. First is a web browser and a mouse.</p>\n<p>There is two binary support by raspberry pi 4. first is <strong>arm64</strong> and <strong>armhf</strong>. If one of the binary is present, you can install the package in your raspberry pi 4 without any problem.</p>\n<p><strong>There is two way to check the package binary is available for raspberry pi 4 or not.</strong></p>\n<ol>\n<li>First way</li>\n<li>Second way</li>\n</ol>\n<blockquote>\n<p>In this article, I use the snap chromium package for example purpose.</p>\n</blockquote>\n<h2>First way</h2>\n<p>Firstly, you click on <strong>the version drop-down list</strong> and click again to <strong>show the architecture</strong> drop-down list and check all available binary forms for the list.</p>\n<ul>\n<li>First step</li>\n<li>Second step</li>\n</ul>\n<h3>First step</h3>\n<p><img src=\"https://contentlayer-iota.vercel.app/images/firstway.png\" alt=\"version drop-down list\"></p>\n<h3>Second step</h3>\n<p><img src=\"https://contentlayer-iota.vercel.app/images/firstway-2.png\" alt=\"show architecture drop-down list\"></p>\n<h2>Second way</h2>\n<p>In a second way, you scroll down the page, go to the Linux distribution section, and check all the Linux distributions.</p>\n<p><img src=\"https://contentlayer-iota.vercel.app/images/secondway.png\" alt=\"Linux distribution section\"></p>\n<hr>\n<h3>Reference</h3>\n<p>https://snapcraft.io/chromium</p>\n<p>https://snapcraft.io/store</p>\n<p>https://snapcraft.io/docs</p>\n<hr>\n<h2>Conclusion</h2>\n<p>I hope my article solve your problem if you have any problem and then comment in the comment section.</p>\n<p><strong>For more updates, subscribe to our newsletter.</strong></p>"},"_id":"posts/how-to-check-the-snap-store-package-available-for-raspberry-pi-4-or-not.md","_raw":{"sourceFilePath":"posts/how-to-check-the-snap-store-package-available-for-raspberry-pi-4-or-not.md","sourceFileName":"how-to-check-the-snap-store-package-available-for-raspberry-pi-4-or-not.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-check-the-snap-store-package-available-for-raspberry-pi-4-or-not"},"type":"Post"},{"title":"How to Install Go language in Raspberry pi 4 and Ubuntu?","date":"2022-03-15T10:59:50.000Z","author":"Rajdeep Singh","description":"","slug":"how-to-install-go-language-in-raspberry-pi-4-and-ubuntu","id":6,"image":"/images/install-go-langauge.png","draft":false,"tags":["go language","Linux","Raspberry pi 4","Ubuntu","install go language"],"categories":["go language","Linux","Raspberry pi 4","Ubuntu","install go language"],"body":{"raw":"\n\nYou install go language in raspberry pi 4, Ubuntu and Linux. The go language installation process is similar for all the Linux distros.\n\nThere is two way to install the go language in raspberry pi 4.\n\n1. Snap\n2. Command-line\n\n### Snap\n\nWith the help of the [Snap store](https://snapcraft.io/go), the easy way to install Go language in Raspberry pi 4 and ubuntu.\n\nSimple you paste only one snap command in raspberry pi 4, and your go language install and config path automatics in raspberry pi 4.\n\n```Command\nsudo snap install go --classic\n```\n\n[Install Go language raspberry pi and ubuntu](images/2022/03/carbon--3-.png)\n\n> https://snapcraft.io/go\n\n### Note\n\n> Install the go lang with Snap. Firstly, install snap cli in raspberry pi 4, and Ubuntu comes pre-install with a snap cli.\n\n---\n\n### Command-line\n\nThe command-line provides both armhf and armd binary format of go language. Moreover, the command line utility helps the config everything inside raspberry pi 4 and ubuntu.\n\n```\nsudo apt-get install golang-go\n```\n\n[Install go language with apt-get in raspberry pi and ubuntu](images/carbon--5-.png)\n\n---\n\n### Check the go language version in raspberry pi 4 and ubuntu.\n\n```\ngo version\n```\n\n[Check go language version in the terminal](images/carbon--6--1.png)\n\n\n\n---\n\n### How to remove the go language package?\n\n* Snap\n* command-line\n\n### Snap\n\nRemove all the go language configurations and packages in Raspberry pi 4 and ubuntu.\n\n```\nsnap remove go\n```\n[remove go language](images/2022/03/carbon--4-.png)\n\n### Note\n\n> If you install go with a snap then you remove go with a snap.\n\n\n\n### Command-line\n\nRemove or uninstall the go language package with all configuration use of the apt-get command.\n\n```command\nsudo apt-get purge golang-go\n```\n\n\n\n---\n\n### Conclusion\n\nI hope my article helps you. I'm not the go lang developer.\n\n\n\n","html":"<p>You install go language in raspberry pi 4, Ubuntu and Linux. The go language installation process is similar for all the Linux distros.</p>\n<p>There is two way to install the go language in raspberry pi 4.</p>\n<ol>\n<li>Snap</li>\n<li>Command-line</li>\n</ol>\n<h3>Snap</h3>\n<p>With the help of the <a href=\"https://snapcraft.io/go\">Snap store</a>, the easy way to install Go language in Raspberry pi 4 and ubuntu.</p>\n<p>Simple you paste only one snap command in raspberry pi 4, and your go language install and config path automatics in raspberry pi 4.</p>\n<pre><code class=\"language-Command\">sudo snap install go --classic\n</code></pre>\n<p><a href=\"images/2022/03/carbon--3-.png\">Install Go language raspberry pi and ubuntu</a></p>\n<blockquote>\n<p>https://snapcraft.io/go</p>\n</blockquote>\n<h3>Note</h3>\n<blockquote>\n<p>Install the go lang with Snap. Firstly, install snap cli in raspberry pi 4, and Ubuntu comes pre-install with a snap cli.</p>\n</blockquote>\n<hr>\n<h3>Command-line</h3>\n<p>The command-line provides both armhf and armd binary format of go language. Moreover, the command line utility helps the config everything inside raspberry pi 4 and ubuntu.</p>\n<pre><code>sudo apt-get install golang-go\n</code></pre>\n<p><a href=\"images/carbon--5-.png\">Install go language with apt-get in raspberry pi and ubuntu</a></p>\n<hr>\n<h3>Check the go language version in raspberry pi 4 and ubuntu.</h3>\n<pre><code>go version\n</code></pre>\n<p><a href=\"images/carbon--6--1.png\">Check go language version in the terminal</a></p>\n<hr>\n<h3>How to remove the go language package?</h3>\n<ul>\n<li>Snap</li>\n<li>command-line</li>\n</ul>\n<h3>Snap</h3>\n<p>Remove all the go language configurations and packages in Raspberry pi 4 and ubuntu.</p>\n<pre><code>snap remove go\n</code></pre>\n<p><a href=\"images/2022/03/carbon--4-.png\">remove go language</a></p>\n<h3>Note</h3>\n<blockquote>\n<p>If you install go with a snap then you remove go with a snap.</p>\n</blockquote>\n<h3>Command-line</h3>\n<p>Remove or uninstall the go language package with all configuration use of the apt-get command.</p>\n<pre><code class=\"language-command\">sudo apt-get purge golang-go\n</code></pre>\n<hr>\n<h3>Conclusion</h3>\n<p>I hope my article helps you. I'm not the go lang developer.</p>"},"_id":"posts/how-to-install-go-language-in-raspberry-pi-4-and-ubuntu.md","_raw":{"sourceFilePath":"posts/how-to-install-go-language-in-raspberry-pi-4-and-ubuntu.md","sourceFileName":"how-to-install-go-language-in-raspberry-pi-4-and-ubuntu.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-install-go-language-in-raspberry-pi-4-and-ubuntu"},"type":"Post"},{"title":"How to install neovim IDE in Raspberry pi 4 and Ubuntu?","date":"2022-03-18T17:11:14.000Z","author":"Rajdeep Singh","description":"","slug":"how-to-install-neovim-ide-in-raspberry-pi-4-and-ubuntu","id":7,"image":"/images/install--neovim.png","draft":false,"tags":["neovim","Linux","linux editor"],"categories":["neovim","Linux","linux editor"],"body":{"raw":"\n\nThe neovim ide base on linuix vim text editor. neovim ide work inside your terminal. You do not need to install a big IDE in raspberry pi 4. The Biggest IDE Eat more ram and resources. So that reason your raspberry pi is very slow. The slowness of raspberry pi gives motivation and strength to shift vscode Editor to neovim.\n\n**You can install neovim in two ways**\n\n1. Snap store\n2. Apt-get or Apt\n\n## Snap store\n\nYou install neovim is very easy with the snap store. You do not need to use other configurations. all the configurations handle snap cli.\n\n```\nsudo snap install  nvim --classic\n```\n\n### Note\n\n> Snap cli all ready install in ubuntu and raspberry pi user firstly install snap cli after run snap following neovim install command.\n\n## Apt-get or Apt\n\nYou can install neovim with inbuild apt-get command-line utility in Linux distro. Simple, you run the following command, and your neovim start to download in Linux.\n\n```\nsudo apt-get install neovim\n\n```\n\n> apt and apt-get is similar command. you can use one of them. some of distro support apt and some of apt-get. but raspberry pi and ubuntu support both of them.\n\n### How to run neovim in raspberry pi 4 and Ubuntu?\n\nYou can run neovim with terminal very easily.\n\n```\nnvim\n```\n\nOpen the file with neovim.\n\n```\nnvim snapPackage.txt \n\n```\n\n\n\n### How to uninstall or remove neovim?\n\nBased on the installation, you can easily remove neovim IDE.\n\n1. Snap\n2. Apt or Apt-get\n\n### Snap\n\nIf you use snap, you run the following command to remove neovim in raspberry pi and ubuntu.\n\n```\nsudo snap remove nvim\n```\n\n\n\n### Apt or Apt-get\n\nIf you use Apt or Apt-get, you can run the following command to remove neovim in raspberry pi and ubuntu.\n\n```\n sudo apt-get purge neovim\n```\n\n---\n\n### References\n\n> https://snapcraft.io/nvim\n\n\n\n> https://github.com/neovim/neovim\n\n\n---\n\n### Conclusion\n\nI hope my article helps you a lot. If you have any queries and ask me in the comment section.\n\n> **You can also subscribe our email for notification.**\n\n","html":"<p>The neovim ide base on linuix vim text editor. neovim ide work inside your terminal. You do not need to install a big IDE in raspberry pi 4. The Biggest IDE Eat more ram and resources. So that reason your raspberry pi is very slow. The slowness of raspberry pi gives motivation and strength to shift vscode Editor to neovim.</p>\n<p><strong>You can install neovim in two ways</strong></p>\n<ol>\n<li>Snap store</li>\n<li>Apt-get or Apt</li>\n</ol>\n<h2>Snap store</h2>\n<p>You install neovim is very easy with the snap store. You do not need to use other configurations. all the configurations handle snap cli.</p>\n<pre><code>sudo snap install  nvim --classic\n</code></pre>\n<h3>Note</h3>\n<blockquote>\n<p>Snap cli all ready install in ubuntu and raspberry pi user firstly install snap cli after run snap following neovim install command.</p>\n</blockquote>\n<h2>Apt-get or Apt</h2>\n<p>You can install neovim with inbuild apt-get command-line utility in Linux distro. Simple, you run the following command, and your neovim start to download in Linux.</p>\n<pre><code>sudo apt-get install neovim\n\n</code></pre>\n<blockquote>\n<p>apt and apt-get is similar command. you can use one of them. some of distro support apt and some of apt-get. but raspberry pi and ubuntu support both of them.</p>\n</blockquote>\n<h3>How to run neovim in raspberry pi 4 and Ubuntu?</h3>\n<p>You can run neovim with terminal very easily.</p>\n<pre><code>nvim\n</code></pre>\n<p>Open the file with neovim.</p>\n<pre><code>nvim snapPackage.txt \n\n</code></pre>\n<h3>How to uninstall or remove neovim?</h3>\n<p>Based on the installation, you can easily remove neovim IDE.</p>\n<ol>\n<li>Snap</li>\n<li>Apt or Apt-get</li>\n</ol>\n<h3>Snap</h3>\n<p>If you use snap, you run the following command to remove neovim in raspberry pi and ubuntu.</p>\n<pre><code>sudo snap remove nvim\n</code></pre>\n<h3>Apt or Apt-get</h3>\n<p>If you use Apt or Apt-get, you can run the following command to remove neovim in raspberry pi and ubuntu.</p>\n<pre><code> sudo apt-get purge neovim\n</code></pre>\n<hr>\n<h3>References</h3>\n<blockquote>\n<p>https://snapcraft.io/nvim</p>\n</blockquote>\n<blockquote>\n<p>https://github.com/neovim/neovim</p>\n</blockquote>\n<hr>\n<h3>Conclusion</h3>\n<p>I hope my article helps you a lot. If you have any queries and ask me in the comment section.</p>\n<blockquote>\n<p><strong>You can also subscribe our email for notification.</strong></p>\n</blockquote>"},"_id":"posts/how-to-install-neovim-ide-in-raspberry-pi-4-and-ubuntu.md","_raw":{"sourceFilePath":"posts/how-to-install-neovim-ide-in-raspberry-pi-4-and-ubuntu.md","sourceFileName":"how-to-install-neovim-ide-in-raspberry-pi-4-and-ubuntu.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-install-neovim-ide-in-raspberry-pi-4-and-ubuntu"},"type":"Post"},{"title":"How to install pycharm in raspi 4","date":"2021-01-20T07:10:27.000Z","author":"Rajdeep Singh","description":"","slug":"how-to-install-pycharm-in-raspi-4","id":8,"image":"/images/How-to-install-pycharm-in-raspi-4.png","draft":false,"tags":["Raspberry","Raspberry pi 4","ras pi","pycharm","pycharm in raspi","install pycharm"],"categories":["Raspberry","Raspberry pi 4","ras pi","pycharm","pycharm in raspi","install pycharm"],"body":{"raw":"\n\nHey!, Everyone my name is Rajdeep Singh. Welcome again to my new post on Raspberry pi 4.In this post, We Learn How to install Pycharm Community Version On Raspberry pi 4.\n\n**Watch This video after following my articles stepsConfig:**\n\n<iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/jAjwzkEDrgI?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n### Steps:\n\n1. Download\n2. Config\n\n### Download:\n\nGo to [Pycharm](https://www.jetbrains.com/pycharm/download/#section=linux) official website and download the latest Linux version in your Raspberry pi 4.\n\n[Download Pycharm Linux Version](images/2021/01/download-pycharm.png) \n\n---\n\n### Config:\n\n* Go to the **download folder** and Select your **download File**. then the **press right-click** after selecting the **extract here** option after your file **successfully extract**. make sure file extract takes time.\n* Open your Linux Terminal using **ctrl+alt+t.**\n\n```command\nctrl+alt+t\n```\n\n* Go to the **Downloads** folder using **cd Downloads.**\n\n```commands\ncd Downloads\n```\n\n[Go to cd Downloads folders](images/pycharm1.png)\n\n* paste **ls commands** inside the Downloads folder\n\n```command\nls\n```\n\n[paste ls command in terminal](images/pycharm-2.png)\n\n* after you see, your pycharm **pycharm-community-2020.3.2** file in the Downloads folder or **similar file** you see it.\n* Then **Move** your **pycharm file** to the pi opt folder. using the **mv command**\n\n```\n sudo mv pycharm-community-2020.3.2  /opt/pycharm-community-2020.3.2\n```\n\n[paste mv command inside terminal](images/pycharm-3.png)\n\n* Successfully move the file to the **opt folder**. then go back using the **cd command.**\n\n```\ncd\n```\n\n[go back in main root folder](images/pycharm-4.png) \n\n* **go to use cd /opt/**pycharm-community-2020.3.2/**bin** command into **bin folder**\n\n```command\ncd /opt/pycharm-community-2020.3.2/bin\n```\n\n[go to bin folder in raspi 4](images/pycharm-5.png) \n\n* after run **./pycharm.sh** pycharm Script and your pycharm successful Install now.\n\n```command\n ./pycharm.sh\n```\n\n[paste command to install pycharm](images/pycharm-6.png) \n\n\n\n### Note:\n\nyou alwayes open your pycharm using . /pycharm.sh command.this pycharm not be show in your rapberry pi **programing** folder\n\n```command\n. /pycharm.sh\n```\n\n---\n\n### Refrenece:\n\n[https://www.element14.com/community/community/raspberry-pi/blog/2019/09/12/installing-pycharm-on-raspberry-pi](https://www.element14.com/community/community/raspberry-pi/blog/2019/09/12/installing-pycharm-on-raspberry-pi)\n\n---\n\n### Hire me:\n\n**Fiverr:**[https://www.fiverr.com/users/officialrajdeep/](https://www.fiverr.com/users/officialrajdeep/)\n\n**Upwork:**[https://www.upwork.com/freelancers/~01a4e8ba7a41795229](https://www.upwork.com/freelancers/~01a4e8ba7a41795229)\n\n---\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://medium.com/officialrajdeepsingh](https://medium.com/officialrajdeepsingh)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n\n\n---\n\n\n\n","html":"<p>Hey!, Everyone my name is Rajdeep Singh. Welcome again to my new post on Raspberry pi 4.In this post, We Learn How to install Pycharm Community Version On Raspberry pi 4.</p>\n<p><strong>Watch This video after following my articles stepsConfig:</strong></p>\n<h3>Steps:</h3>\n<ol>\n<li>Download</li>\n<li>Config</li>\n</ol>\n<h3>Download:</h3>\n<p>Go to <a href=\"https://www.jetbrains.com/pycharm/download/#section=linux\">Pycharm</a> official website and download the latest Linux version in your Raspberry pi 4.</p>\n<p><a href=\"images/2021/01/download-pycharm.png\">Download Pycharm Linux Version</a></p>\n<hr>\n<h3>Config:</h3>\n<ul>\n<li>Go to the <strong>download folder</strong> and Select your <strong>download File</strong>. then the <strong>press right-click</strong> after selecting the <strong>extract here</strong> option after your file <strong>successfully extract</strong>. make sure file extract takes time.</li>\n<li>Open your Linux Terminal using <strong>ctrl+alt+t.</strong></li>\n</ul>\n<pre><code class=\"language-command\">ctrl+alt+t\n</code></pre>\n<ul>\n<li>Go to the <strong>Downloads</strong> folder using <strong>cd Downloads.</strong></li>\n</ul>\n<pre><code class=\"language-commands\">cd Downloads\n</code></pre>\n<p><a href=\"images/pycharm1.png\">Go to cd Downloads folders</a></p>\n<ul>\n<li>paste <strong>ls commands</strong> inside the Downloads folder</li>\n</ul>\n<pre><code class=\"language-command\">ls\n</code></pre>\n<p><a href=\"images/pycharm-2.png\">paste ls command in terminal</a></p>\n<ul>\n<li>after you see, your pycharm <strong>pycharm-community-2020.3.2</strong> file in the Downloads folder or <strong>similar file</strong> you see it.</li>\n<li>Then <strong>Move</strong> your <strong>pycharm file</strong> to the pi opt folder. using the <strong>mv command</strong></li>\n</ul>\n<pre><code> sudo mv pycharm-community-2020.3.2  /opt/pycharm-community-2020.3.2\n</code></pre>\n<p><a href=\"images/pycharm-3.png\">paste mv command inside terminal</a></p>\n<ul>\n<li>Successfully move the file to the <strong>opt folder</strong>. then go back using the <strong>cd command.</strong></li>\n</ul>\n<pre><code>cd\n</code></pre>\n<p><a href=\"images/pycharm-4.png\">go back in main root folder</a></p>\n<ul>\n<li>**go to use cd /opt/**pycharm-community-2020.3.2/<strong>bin</strong> command into <strong>bin folder</strong></li>\n</ul>\n<pre><code class=\"language-command\">cd /opt/pycharm-community-2020.3.2/bin\n</code></pre>\n<p><a href=\"images/pycharm-5.png\">go to bin folder in raspi 4</a></p>\n<ul>\n<li>after run <strong>./pycharm.sh</strong> pycharm Script and your pycharm successful Install now.</li>\n</ul>\n<pre><code class=\"language-command\"> ./pycharm.sh\n</code></pre>\n<p><a href=\"images/pycharm-6.png\">paste command to install pycharm</a></p>\n<h3>Note:</h3>\n<p>you alwayes open your pycharm using . /pycharm.sh command.this pycharm not be show in your rapberry pi <strong>programing</strong> folder</p>\n<pre><code class=\"language-command\">. /pycharm.sh\n</code></pre>\n<hr>\n<h3>Refrenece:</h3>\n<p><a href=\"https://www.element14.com/community/community/raspberry-pi/blog/2019/09/12/installing-pycharm-on-raspberry-pi\">https://www.element14.com/community/community/raspberry-pi/blog/2019/09/12/installing-pycharm-on-raspberry-pi</a></p>\n<hr>\n<h3>Hire me:</h3>\n<p><strong>Fiverr:</strong><a href=\"https://www.fiverr.com/users/officialrajdeep/\">https://www.fiverr.com/users/officialrajdeep/</a></p>\n<p><strong>Upwork:</strong><a href=\"https://www.upwork.com/freelancers/~01a4e8ba7a41795229\">https://www.upwork.com/freelancers/~01a4e8ba7a41795229</a></p>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://medium.com/officialrajdeepsingh\">https://medium.com/officialrajdeepsingh</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>\n<hr>"},"_id":"posts/how-to-install-pycharm-in-raspi-4.md","_raw":{"sourceFilePath":"posts/how-to-install-pycharm-in-raspi-4.md","sourceFileName":"how-to-install-pycharm-in-raspi-4.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-install-pycharm-in-raspi-4"},"type":"Post"},{"title":"How to install rust language in raspberry pi 4 and ubuntu?","date":"2022-03-17T07:05:24.000Z","author":"Rajdeep Singh","description":"","slug":"how-to-install-rust-language-in-raspberry-pi-4-and-ubuntu","id":9,"image":"/images/install-Rust-langauge.png","draft":false,"tags":["Rust","Linux","Raspberry pi 4","Install Rust in Linux","Ubuntu"],"categories":["Rust","Linux","Raspberry pi 4","Install Rust in Linux","Ubuntu"],"body":{"raw":"\n\nInstallation of Rust in raspberry pi 4 and Ubuntu is a very easy process for all  Linux distros. You can install Rust with one single command. So simple, you copy and paste in your terminal.\n\n### Install Rust\n\n```command\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\nAfter you paste the command in a terminal, the rust setup asks one question to start the installation rust in your distro.\n\n```\nrajdeepsingh@rajdeepsingh:/$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\ninfo: downloading installer\nWelcome to Rust!\n....\nCurrent installation options:\ndefault host triple: aarch64-unknown-linux-gnu\n     default toolchain: stable (default)\n               profile: default\n  modify PATH variable: yes\n1) Proceed with installation (default)\n2) Customize installation\n3) Cancel installation\n> 1\n```\n\nI select default rust installation in raspberry pi 4. so everything is mange by rust lang.\n\n### The rust command output\n\n```\nrajdeepsingh@officialrajdeepsingh:/$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\ninfo: downloading installer\nWelcome to Rust!\nThis will download and install the official compiler for the Rust\nprogramming language, and its package manager, Cargo.\nRustup metadata and toolchains will be installed into the Rustup\nhome directory, located at:\n/home/rajdeepsingh/.rustup\nThis can be modified with the RUSTUP_HOME environment variable.\nThe Cargo home directory located at:\n/home/rajdeepsingh/.cargo\nThis can be modified with the CARGO_HOME environment variable.\nThe cargo, rustc, rustup and other commands will be added to\nCargo's bin directory, located at:\n/home/rajdeepsingh/.cargo/bin\nThis path will then be added to your PATH environment variable by\nmodifying the profile files located at:\n/home/rajdeepsingh/.profile\n  /home/rajdeepsingh/.bashrc\nYou can uninstall at any time with rustup self uninstall and\nthese changes will be reverted.\nCurrent installation options:\ndefault host triple: aarch64-unknown-linux-gnu\n     default toolchain: stable (default)\n               profile: default\n  modify PATH variable: yes\n1) Proceed with installation (default)\n2) Customize installation\n3) Cancel installation\n>1\ninfo: profile set to 'default'\ninfo: default host triple is aarch64-unknown-linux-gnu\ninfo: syncing channel updates for 'stable-aarch64-unknown-linux-gnu'\n686.2 KiB / 686.2 KiB (100 %) 170.5 KiB/s in  5s ETA:  0s\ninfo: latest update on 2022-02-24, rust version 1.59.0 (9d1b2106e 2022-02-23)\ninfo: downloading component 'cargo'\n  5.5 MiB /   5.5 MiB (100 %) 267.0 KiB/s in 23s ETA:  0s\ninfo: downloading component 'clippy'\n  2.3 MiB /   2.3 MiB (100 %) 275.9 KiB/s in 11s ETA:  0s\ninfo: downloading component 'rust-docs'\n 19.3 MiB /  19.3 MiB (100 %) 321.3 KiB/s in 59s ETA:  0s    \ninfo: downloading component 'rust-std'\n 31.6 MiB /  31.6 MiB (100 %) 540.1 KiB/s in  1m 38s ETA:  0s    \ninfo: downloading component 'rustc'\n 73.1 MiB /  73.1 MiB (100 %) 270.2 KiB/s in  4m 41s ETA:  0s    \ninfo: downloading component 'rustfmt'\n  3.4 MiB /   3.4 MiB (100 %) 294.1 KiB/s in 13s ETA:  0s\ninfo: installing component 'cargo'\n  5.5 MiB /   5.5 MiB (100 %)   5.3 MiB/s in  1s ETA:  0s\ninfo: installing component 'clippy'\ninfo: installing component 'rust-docs'\n 19.3 MiB /  19.3 MiB (100 %) 851.2 KiB/s in 38s ETA:  0s\n  8 IO-ops /   8 IO-ops (100 %)   5 IOPS in  1s ETA:  0s    \ninfo: installing component 'rust-std'\n 31.6 MiB /  31.6 MiB (100 %)   5.1 MiB/s in 25s ETA:  0s\ninfo: installing component 'rustc'\n 73.1 MiB /  73.1 MiB (100 %)   4.6 MiB/s in 35s ETA:  0s\n  5 IO-ops /   5 IO-ops (100 %)   0 IOPS in  8m  2s ETA: Unknown\ninfo: installing component 'rustfmt'\n  1 IO-ops /   1 IO-ops (100 %)   0 IOPS in 10s ETA: Unknown\ninfo: default toolchain set to 'stable-aarch64-unknown-linux-gnu'\nstable-aarch64-unknown-linux-gnu installed - rustc 1.59.0 (9d1b2106e 2022-02-23)\nRust is installed now. Great!\nTo get started you may need to restart your current shell.\nThis would reload your PATH environment variable to include\nCargo's bin directory ($HOME/.cargo/bin).\nTo configure your current shell, run:\nsource $HOME/.cargo/env\nrajdeepsingh@officialrajdeepsingh:/$\n```\n\n### How to delete or uninstall the rust lang?\n\nYou can uninstall the Rust with rustup. rustup in inbuild command utility in rust lang.\n\n```\nrustup self uninstall\n\n```\n\n---\n\n### Reference\n\n> https://www.rust-lang.org/tools/install\n\n---\n\n### Conclusion\n\nI hope my article solve your problem. If you have any queries, then ask in the comment section.\n\n\n\n","html":"<p>Installation of Rust in raspberry pi 4 and Ubuntu is a very easy process for all  Linux distros. You can install Rust with one single command. So simple, you copy and paste in your terminal.</p>\n<h3>Install Rust</h3>\n<pre><code class=\"language-command\">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre>\n<p>After you paste the command in a terminal, the rust setup asks one question to start the installation rust in your distro.</p>\n<pre><code>rajdeepsingh@rajdeepsingh:/$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\ninfo: downloading installer\nWelcome to Rust!\n....\nCurrent installation options:\ndefault host triple: aarch64-unknown-linux-gnu\n     default toolchain: stable (default)\n               profile: default\n  modify PATH variable: yes\n1) Proceed with installation (default)\n2) Customize installation\n3) Cancel installation\n> 1\n</code></pre>\n<p>I select default rust installation in raspberry pi 4. so everything is mange by rust lang.</p>\n<h3>The rust command output</h3>\n<pre><code>rajdeepsingh@officialrajdeepsingh:/$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\ninfo: downloading installer\nWelcome to Rust!\nThis will download and install the official compiler for the Rust\nprogramming language, and its package manager, Cargo.\nRustup metadata and toolchains will be installed into the Rustup\nhome directory, located at:\n/home/rajdeepsingh/.rustup\nThis can be modified with the RUSTUP_HOME environment variable.\nThe Cargo home directory located at:\n/home/rajdeepsingh/.cargo\nThis can be modified with the CARGO_HOME environment variable.\nThe cargo, rustc, rustup and other commands will be added to\nCargo's bin directory, located at:\n/home/rajdeepsingh/.cargo/bin\nThis path will then be added to your PATH environment variable by\nmodifying the profile files located at:\n/home/rajdeepsingh/.profile\n  /home/rajdeepsingh/.bashrc\nYou can uninstall at any time with rustup self uninstall and\nthese changes will be reverted.\nCurrent installation options:\ndefault host triple: aarch64-unknown-linux-gnu\n     default toolchain: stable (default)\n               profile: default\n  modify PATH variable: yes\n1) Proceed with installation (default)\n2) Customize installation\n3) Cancel installation\n>1\ninfo: profile set to 'default'\ninfo: default host triple is aarch64-unknown-linux-gnu\ninfo: syncing channel updates for 'stable-aarch64-unknown-linux-gnu'\n686.2 KiB / 686.2 KiB (100 %) 170.5 KiB/s in  5s ETA:  0s\ninfo: latest update on 2022-02-24, rust version 1.59.0 (9d1b2106e 2022-02-23)\ninfo: downloading component 'cargo'\n  5.5 MiB /   5.5 MiB (100 %) 267.0 KiB/s in 23s ETA:  0s\ninfo: downloading component 'clippy'\n  2.3 MiB /   2.3 MiB (100 %) 275.9 KiB/s in 11s ETA:  0s\ninfo: downloading component 'rust-docs'\n 19.3 MiB /  19.3 MiB (100 %) 321.3 KiB/s in 59s ETA:  0s    \ninfo: downloading component 'rust-std'\n 31.6 MiB /  31.6 MiB (100 %) 540.1 KiB/s in  1m 38s ETA:  0s    \ninfo: downloading component 'rustc'\n 73.1 MiB /  73.1 MiB (100 %) 270.2 KiB/s in  4m 41s ETA:  0s    \ninfo: downloading component 'rustfmt'\n  3.4 MiB /   3.4 MiB (100 %) 294.1 KiB/s in 13s ETA:  0s\ninfo: installing component 'cargo'\n  5.5 MiB /   5.5 MiB (100 %)   5.3 MiB/s in  1s ETA:  0s\ninfo: installing component 'clippy'\ninfo: installing component 'rust-docs'\n 19.3 MiB /  19.3 MiB (100 %) 851.2 KiB/s in 38s ETA:  0s\n  8 IO-ops /   8 IO-ops (100 %)   5 IOPS in  1s ETA:  0s    \ninfo: installing component 'rust-std'\n 31.6 MiB /  31.6 MiB (100 %)   5.1 MiB/s in 25s ETA:  0s\ninfo: installing component 'rustc'\n 73.1 MiB /  73.1 MiB (100 %)   4.6 MiB/s in 35s ETA:  0s\n  5 IO-ops /   5 IO-ops (100 %)   0 IOPS in  8m  2s ETA: Unknown\ninfo: installing component 'rustfmt'\n  1 IO-ops /   1 IO-ops (100 %)   0 IOPS in 10s ETA: Unknown\ninfo: default toolchain set to 'stable-aarch64-unknown-linux-gnu'\nstable-aarch64-unknown-linux-gnu installed - rustc 1.59.0 (9d1b2106e 2022-02-23)\nRust is installed now. Great!\nTo get started you may need to restart your current shell.\nThis would reload your PATH environment variable to include\nCargo's bin directory ($HOME/.cargo/bin).\nTo configure your current shell, run:\nsource $HOME/.cargo/env\nrajdeepsingh@officialrajdeepsingh:/$\n</code></pre>\n<h3>How to delete or uninstall the rust lang?</h3>\n<p>You can uninstall the Rust with rustup. rustup in inbuild command utility in rust lang.</p>\n<pre><code>rustup self uninstall\n\n</code></pre>\n<hr>\n<h3>Reference</h3>\n<blockquote>\n<p>https://www.rust-lang.org/tools/install</p>\n</blockquote>\n<hr>\n<h3>Conclusion</h3>\n<p>I hope my article solve your problem. If you have any queries, then ask in the comment section.</p>"},"_id":"posts/how-to-install-rust-language-in-raspberry-pi-4-and-ubuntu.md","_raw":{"sourceFilePath":"posts/how-to-install-rust-language-in-raspberry-pi-4-and-ubuntu.md","sourceFileName":"how-to-install-rust-language-in-raspberry-pi-4-and-ubuntu.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/how-to-install-rust-language-in-raspberry-pi-4-and-ubuntu"},"type":"Post"},{"title":"HTML Version History?","date":"2020-12-24T11:13:05.000Z","author":"Rajdeep Singh","description":"HTML History Very Complicated. But I Try To Explain Very Easy Way.","slug":"html-version-history","id":10,"image":"/images/HTML-Version-History.jpg","draft":false,"tags":["html","html 5","HTML History","Html Version","Who Create Html"],"categories":["html","html 5","HTML History","Html Version","Who Create Html"],"body":{"raw":"\n\n\nAccording to Wikipedia, HTML was Created By _**Tim Berners-Lee** in **1991**._ Launch official standard HTML Version in December 1999.\n\nIn 1989, Berners-Lee also created an Internet-based hypertext system._HTML Comes in many versions. But_ HTML 4.01 Widely Use HTML.\n\n### History:\n\n* **HTML 2 : November 24, 1995,**\n* **HTML 3 : January 14, 1997,**\n* **HTML 4 : December 18, 1997,**\n* **HTML 5 : October 28, 2014,**\n\n\n\n### HTML 1:\n\nThe original version of HTML 1.0 Announcing with few greatly limited features. Use HTML 1, and You Never be a creation of a Good Looking website.\n\n### HTML 2:\n\nHTML 2.0 then arrived and included all the HTML 1.0 plus several new features for web page design. Now Use HTML 2.0 You Design A Website.\n\n### HTML 3:\n\nHTML 2.0 served its purpose very well, but many Programmers or designing web pages wanted more control over their web pages and more ways to mark up their text and enhance Website appearance And Look On Browser.\n\n### HTML 4:\n\nIn the early days, **_HTML 4.0_** was code-named **_COUGAR_**. This version introduces new functionality, most of which comes from the expired HTML 3.0 draft.  W3C  Recommendation Explorer has done a Good job in implementing the many New features Into HTML 4.0. After 4.0, One More HTML Version Come People Know as XHTML. Which More Popular.\n\n### HTML 5:\n\nHTML 5 Now A standard version and secondly More Popular On Earth. HTML 5 Give Lots Of Tags Help To Browser Understand Text Format and Secondly Build Unique beautiful Website Design.\n\n---\n\n### Reference:\n\n\n\nhttp://www.codefreetutorial.com/learn-html/76-different-versions-of-html\n\n---\n\n\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://medium.com/officialrajdeepsingh](https://medium.com/officialrajdeepsingh)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n\n\n\n","html":"<p>According to Wikipedia, HTML was Created By <em><strong>Tim Berners-Lee</strong> in <strong>1991</strong>.</em> Launch official standard HTML Version in December 1999.</p>\n<p>In 1989, Berners-Lee also created an Internet-based hypertext system.<em>HTML Comes in many versions. But</em> HTML 4.01 Widely Use HTML.</p>\n<h3>History:</h3>\n<ul>\n<li><strong>HTML 2 : November 24, 1995,</strong></li>\n<li><strong>HTML 3 : January 14, 1997,</strong></li>\n<li><strong>HTML 4 : December 18, 1997,</strong></li>\n<li><strong>HTML 5 : October 28, 2014,</strong></li>\n</ul>\n<h3>HTML 1:</h3>\n<p>The original version of HTML 1.0 Announcing with few greatly limited features. Use HTML 1, and You Never be a creation of a Good Looking website.</p>\n<h3>HTML 2:</h3>\n<p>HTML 2.0 then arrived and included all the HTML 1.0 plus several new features for web page design. Now Use HTML 2.0 You Design A Website.</p>\n<h3>HTML 3:</h3>\n<p>HTML 2.0 served its purpose very well, but many Programmers or designing web pages wanted more control over their web pages and more ways to mark up their text and enhance Website appearance And Look On Browser.</p>\n<h3>HTML 4:</h3>\n<p>In the early days, <strong><em>HTML 4.0</em></strong> was code-named <strong><em>COUGAR</em></strong>. This version introduces new functionality, most of which comes from the expired HTML 3.0 draft.  W3C  Recommendation Explorer has done a Good job in implementing the many New features Into HTML 4.0. After 4.0, One More HTML Version Come People Know as XHTML. Which More Popular.</p>\n<h3>HTML 5:</h3>\n<p>HTML 5 Now A standard version and secondly More Popular On Earth. HTML 5 Give Lots Of Tags Help To Browser Understand Text Format and Secondly Build Unique beautiful Website Design.</p>\n<hr>\n<h3>Reference:</h3>\n<p>http://www.codefreetutorial.com/learn-html/76-different-versions-of-html</p>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://medium.com/officialrajdeepsingh\">https://medium.com/officialrajdeepsingh</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>"},"_id":"posts/html-version-history.md","_raw":{"sourceFilePath":"posts/html-version-history.md","sourceFileName":"html-version-history.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/html-version-history"},"type":"Post"},{"title":"All Keyboard Shortcuts For Linux Terminal?","date":"2021-01-08T08:05:57.000Z","author":"Rajdeep Singh","description":"Linux Keyboard Help To Enhance Your Working Speed Inside Command Tool","slug":"keyboard-shortcut-keys-for-linux-terminal","id":11,"image":"/images/Linux-Basic-Introduction--1-.png","draft":false,"tags":["Linux","Keyboard Shortcut","Shortcut keys","Linux Terminal"],"categories":["Linux","Keyboard Shortcut","Shortcut keys","Linux Terminal"],"body":{"raw":"\n\n\n## Ctrl + Alt + T :\n\nUse This Shortcut Key to Open Linux Terminal Inside  Your Laptop || Pc || Machine.\n\n### All Shoctkeys In Linux Terminal:\n\n* **Ctrl + L**: Clears the screen, similar to the clear command in the terminal.\n* **Ctrl + S:** Stop all output to the screen. When You run Commands with longs output. But  You don't stop it.\n* **Tab**: tab Help Automatic Fill Your Name.\n* **Ctrl + A**: Cursor goes to Start Of Word\n* **Ctrl + E**: Cursor goes to End Of Word.\n* **Ctrl + F**: Move the cursor forward one by one character.\n* **Ctrl + B**: Move the Cursor backward one by one Character.\n* **Alt + F**: Move the cursor forward one by one Word.\n* **Alt + B**: Move the Cursor backward one by one Word.\n* **Alt + U**: Change Character or Word Into Uppercase.\n* **Alt + l**: Change  Character Or Word Into Lower Case.\n* **Alt  + T**: Swap the last two words before the cursor.\n* **Alt + C**: Use Capitalize  Words.\n* **Alt + D**: Delete to end of word starting at the cursor\n* **Ctrl + K**: Cut Word/Line from the current position to the end of the line. Also, adding it to the clipboard, use ctrl + y to paste it again.\n* **Ctrl + W**: Delete the word before the Cursor Position. Also, adding it to the clipboard, use ctrl + y paste on it.\n* **Ctrl + Y**: Paste the last thing from the clipboard that you cut recently.\n* **Ctrl + D**: Delete Character By Character.\n* **Ctrl + T**: Remove White Space.\n* **Ctrl + Shift + W**: close terminal tab.\n* **Ctrl + Shift + Q**: close the entire terminal.\n* **Shift+Ctrl + N**: Open New Window.\n* **Shift + Ctrl +T**: Open New Tab In Window.\n* **Shift + Ctrl + W**: Close Tab.\n* **Shift + Ctrl + Q**: Close Window.\n* **Shift + Ctrl + C**: Use For Copy Text Inside Terminal.\n* **Shift + Ctrl +  V**: Paste Text Inside Terminal.\n* **Shift + Ctrl + +**: Zoom In.\n* **Shift + Ctrl + -**: Zoom Out\n* **Shift + Ctrl + )**: Zoom Reset\n* **Shift + Ctrl + I**: Add Name Off Each Open Tab.\n\n### Note:\n\nIf i  Forget Some Keyboard Shortcuts, and You Do Not find them on this page so Tell me in the comment box.\n\n---\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://medium.com/officialrajdeepsingh](https://medium.com/officialrajdeepsingh)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n\n\n\n---\n\n\n\n","html":"<h2>Ctrl + Alt + T :</h2>\n<p>Use This Shortcut Key to Open Linux Terminal Inside  Your Laptop || Pc || Machine.</p>\n<h3>All Shoctkeys In Linux Terminal:</h3>\n<ul>\n<li><strong>Ctrl + L</strong>: Clears the screen, similar to the clear command in the terminal.</li>\n<li><strong>Ctrl + S:</strong> Stop all output to the screen. When You run Commands with longs output. But  You don't stop it.</li>\n<li><strong>Tab</strong>: tab Help Automatic Fill Your Name.</li>\n<li><strong>Ctrl + A</strong>: Cursor goes to Start Of Word</li>\n<li><strong>Ctrl + E</strong>: Cursor goes to End Of Word.</li>\n<li><strong>Ctrl + F</strong>: Move the cursor forward one by one character.</li>\n<li><strong>Ctrl + B</strong>: Move the Cursor backward one by one Character.</li>\n<li><strong>Alt + F</strong>: Move the cursor forward one by one Word.</li>\n<li><strong>Alt + B</strong>: Move the Cursor backward one by one Word.</li>\n<li><strong>Alt + U</strong>: Change Character or Word Into Uppercase.</li>\n<li><strong>Alt + l</strong>: Change  Character Or Word Into Lower Case.</li>\n<li><strong>Alt  + T</strong>: Swap the last two words before the cursor.</li>\n<li><strong>Alt + C</strong>: Use Capitalize  Words.</li>\n<li><strong>Alt + D</strong>: Delete to end of word starting at the cursor</li>\n<li><strong>Ctrl + K</strong>: Cut Word/Line from the current position to the end of the line. Also, adding it to the clipboard, use ctrl + y to paste it again.</li>\n<li><strong>Ctrl + W</strong>: Delete the word before the Cursor Position. Also, adding it to the clipboard, use ctrl + y paste on it.</li>\n<li><strong>Ctrl + Y</strong>: Paste the last thing from the clipboard that you cut recently.</li>\n<li><strong>Ctrl + D</strong>: Delete Character By Character.</li>\n<li><strong>Ctrl + T</strong>: Remove White Space.</li>\n<li><strong>Ctrl + Shift + W</strong>: close terminal tab.</li>\n<li><strong>Ctrl + Shift + Q</strong>: close the entire terminal.</li>\n<li><strong>Shift+Ctrl + N</strong>: Open New Window.</li>\n<li><strong>Shift + Ctrl +T</strong>: Open New Tab In Window.</li>\n<li><strong>Shift + Ctrl + W</strong>: Close Tab.</li>\n<li><strong>Shift + Ctrl + Q</strong>: Close Window.</li>\n<li><strong>Shift + Ctrl + C</strong>: Use For Copy Text Inside Terminal.</li>\n<li><strong>Shift + Ctrl +  V</strong>: Paste Text Inside Terminal.</li>\n<li><strong>Shift + Ctrl + +</strong>: Zoom In.</li>\n<li><strong>Shift + Ctrl + -</strong>: Zoom Out</li>\n<li><strong>Shift + Ctrl + )</strong>: Zoom Reset</li>\n<li><strong>Shift + Ctrl + I</strong>: Add Name Off Each Open Tab.</li>\n</ul>\n<h3>Note:</h3>\n<p>If i  Forget Some Keyboard Shortcuts, and You Do Not find them on this page so Tell me in the comment box.</p>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://medium.com/officialrajdeepsingh\">https://medium.com/officialrajdeepsingh</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>\n<hr>"},"_id":"posts/keyboard-shortcut-keys-for-linux-terminal.md","_raw":{"sourceFilePath":"posts/keyboard-shortcut-keys-for-linux-terminal.md","sourceFileName":"keyboard-shortcut-keys-for-linux-terminal.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/keyboard-shortcut-keys-for-linux-terminal"},"type":"Post"},{"title":"Mundial 2022","date":"2022-11-14T11:42:50.515Z","author":"Ashot","description":"World Cup is coming!!","slug":"mundial-2022","image":"/images/qatar-2022-s-32-teams-graphic.webp","draft":false,"tags":["goal"],"categories":["Football"],"body":{"raw":"The 2022 FIFA World Cup is scheduled to be the 22nd running of the FIFA World Cup competition, the quadrennial international men's football championship contested by the senior national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 20 November to 18 December 2022.","html":"<p>The 2022 FIFA World Cup is scheduled to be the 22nd running of the FIFA World Cup competition, the quadrennial international men's football championship contested by the senior national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 20 November to 18 December 2022.</p>"},"_id":"posts/mundial-2022.md","_raw":{"sourceFilePath":"posts/mundial-2022.md","sourceFileName":"mundial-2022.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/mundial-2022"},"type":"Post"},{"title":"neeeew neeeew","date":"2022-11-14T18:08:29.851Z","author":"Ashot","description":"c","slug":"neeeew-neeeew","image":"/images/group-578.png","draft":false,"tags":["neeeew"],"categories":["neeeew"],"body":{"raw":"neeeewneeeewneeeewneeeewneeeew","html":"<p>neeeewneeeewneeeewneeeewneeeew</p>"},"_id":"posts/neeeew-neeeew.md","_raw":{"sourceFilePath":"posts/neeeew-neeeew.md","sourceFileName":"neeeew-neeeew.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/neeeew-neeeew"},"type":"Post"},{"title":"New Post","date":"2023-01-25T13:06:43.382Z","author":"ASh","description":"very short description","slug":"new-post","image":"/images/dynamic/aim-logo.svg","draft":false,"categories":["test"],"body":{"raw":"<div align=\"center\">\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/154338760-edfe1885-06f3-4e02-87fe-4b13a403516b.png\">\r\n  <h3>An easy-to-use & supercharged open-source experiment tracker</h3>\r\n  Aim logs your training runs, enables a beautiful UI to compare them and an API to query them programmatically.\r\n</div>\r\n\r\n<br/>\r\n\r\n<img src=\"https://user-images.githubusercontent.com/13848158/154338753-34484cda-95b8-4da8-a610-7fdf198c05fd.png\">\r\n\r\n<p align=\"center\">\r\n  <a href=\"#about-aim\"><b>About</b></a> &bull;\r\n  <a href=\"#why-use-aim\"><b>Features</b></a> &bull;\r\n  <a href=\"#demos\"><b>Demos</b></a> &bull;\r\n  <a href=\"https://github.com/aimhubio/aim/tree/main/examples\"><b>Examples</b></a> &bull;\r\n  <a href=\"#quick-start\"><b>Quick Start</b></a> &bull;\r\n  <a href=\"https://aimstack.readthedocs.io/en/latest/\"><b>Documentation</b></a> &bull;\r\n  <a href=\"#roadmap\"><b>Roadmap</b></a> &bull;\r\n  <a href=\"https://community.aimstack.io/\"><b>Discord Community</b></a> &bull;\r\n  <a href=\"https://twitter.com/aimstackio\"><b>Twitter</b></a>\r\n</p>\r\n\r\n<div align=\"center\">\r\n  \r\n  [![Platform Support](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue)]()\r\n  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aim)](https://pypi.org/project/aim/)\r\n  [![PyPI Package](https://img.shields.io/pypi/v/aim?color=yellow)](https://pypi.org/project/aim/)\r\n  [![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)\r\n  [![PyPI Downloads](https://img.shields.io/pypi/dw/aim?color=green)](https://pypi.org/project/aim/)\r\n  [![Issues](https://img.shields.io/github/issues/aimhubio/aim)](http://github.com/aimhubio/aim/issues)\r\n  \r\n</div>\r\n\r\n<div align=\"center\">\r\n  <sub>Integrates seamlessly with your favorite tools</sub>\r\n  <br/>\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354389-d0301620-77ea-4629-a743-f7aa249e14b5.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354496-b39d7b1c-63ef-40f0-9e59-c08d2c5e337c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354380-3755c741-6960-42ca-b93e-84a8791f088c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354342-7df0ef5e-63d2-4df7-b9f1-d2fc0e95f53f.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354392-afbff3de-c845-4d86-855d-53df569f91d1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354355-89210506-e7e5-4d37-b2d6-ad3fda62ef13.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354397-8af8e1d3-4067-405e-9d42-1f131663ed22.png\" width=\"60\" />\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354513-f7486146-3891-4f3f-934f-e58bbf9ce695.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354500-c0471ce6-b2ce-4172-b9e4-07a197256303.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354361-9f911785-008d-4b75-877e-651e026cf47e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354373-1879ae61-b5d1-41f0-a4f1-04b639b6f05e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354483-75d9853f-7154-4d95-8190-9ad7a73d6654.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354329-cf7c3352-a72a-478d-82a7-04e3833b03b7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354349-dcdf3bc3-d7a9-4f34-8258-4824a57f59c7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354471-518f1814-7a41-4b23-9caf-e516507343f1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165162736-2cc5da39-38aa-4093-874f-e56d0ba9cea2.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165074282-36ad18eb-1124-434d-8439-728c22cd7ac7.png\" width=\"60\" />\r\n</div>\r\n\r\n<div align=\"center\">\r\n  <br/>\r\n  <kbd>\r\n    <img width=\"650px\" src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" />\r\n  </kbd>\r\n</div>\r\n\r\n# About Aim\r\n\r\n| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n| <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337794-e9310239-6614-41b3-a95b-bb91f0bb6c4f.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337788-03fe5b31-0fa3-44af-ae79-2861707d8602.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337793-85175c78-5659-4dd0-bb2d-05017278e2fa.png\"> |\r\n\r\nAim is an open-source, self-hosted ML experiment tracking tool. \r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\r\n\r\nYou can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically. \r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.\r\n\r\n\r\nAim's mission is to democratize AI dev tools.\r\n\r\n# Why use Aim?\r\n\r\n### Compare 100s of runs in a few clicks - build models faster\r\n\r\n- Compare, group and aggregate 100s of metrics thanks to effective visualizations.\r\n- Analyze, learn correlations and patterns between hparams and metrics.\r\n- Easy pythonic search to query the runs you want to explore.\r\n\r\n### Deep dive into details of each run for easy debugging\r\n\r\n- Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.\r\n- Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.\r\n- Analyze system resource usage to effectively utilize computational resources.\r\n\r\n### Have all relevant information organised and accessible for easy governance\r\n\r\n- Centralized dashboard to holistically view all your runs, their hparams and results.\r\n- Use SDK to query/access all your runs and tracked metadata.\r\n- You own your data - Aim is open source and self hosted.\r\n\r\n# Demos\r\n\r\n| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10001/metrics?grouping=HQGdK9Xxy35e6sY1CYkCmk1WbWMN2AsCNfJJ3d1RJYLtrVPMoF5UpGiA6CF8bEJnfzRsKpqespf3AEuKSVrhUYvYk9MxzNGA9XZWYUf6phEg8AMbZGLRVDXnAPDuo8tueqsST1ZLizWzQwDYJWHUza6pyB2Eojt9uWqNHUdb858TqDRnCJzqiVJXKXEzFWUyvU8MckJo1qpqWWCTb4GpYN6DUJZx2GXDGR21e2xxd4m7PmNUnbA9B3apLttZoipJF6c3v7tNUKmb6irpqnNB3yc57tqYDa1XZuKfDxkMtyFdQ1x95K4jjsTVwhftEWLze35QNcxNXRCGGS9o9yEfTLG26GUX2zjPZFCjjMGU6vV7z1xRccK8MyoGrLSgAQCbvk68dTGBHpXUBvCRq8N&chart=FviZzVrt4fVQPjpCLr9sVGGrcR5etSroyqambiKpm3nTgpyv4eQxKuwNX9uN8UtKmzYUhUyTMBEANHmtbwjLApkvnYeNbxGNC6PVcoqi65m1XJnSrvgt8WiD89BapFAWRUwAGx6SWD7KZPsk3RQyysU7W7FjD3Q99NusxFGhsEfD6HXc7i8xH9KHDRGjLwh6x9VTtSp4FS8HEvpLSiiJoX7LCTi8pB7dXvrQ8G5w3jPsFz4qXYFdsVaCNL1BpFFZuiqQNkfbnM84gEq7UmiV1VzM4oS3AgQHxADG3kpBVp6eKTey9F1Swd4FcUkFA9QEPjgQgqwRGjkquZ2bdDDVLBnCh7JPvboP2kifCiZZ5MDdV9MMx6PKHp4DusWyWLXiHQYPkpGPWBiuccMUXDsuJaCWJbuABdY7CyiJMv1jdHYkjabygSxehPVyEDefWAtjBfv2vaeM1xv63jadbmpKYFxft7qmuT9HvVxiGvRgs4RQFxy8K4rtFBca3HNs1mDaaY81gy9MGXyw7BS5Fniu92jaJpsWDdg6Y3AQBLZtrpJy2obEZ4yzJaCVT7JUNPAyyCUNLck393VFLoEkaD9CU5npK5R7tj1c1G3gkMNQXnSXy5NpSj8deMmXV5qz3JKu1nq2caGQKcqjzy2gLkExdm674AMFjSg9yFjK6VqASXQ17NKtWRUvaYoxGbHDAFQaMKWKh8QLm22QA9mKT8NksLptWozbgDvafnQLNMvezLU5bvKV5o75PAWYiRB56RcYfEhzaB6YWdgL7TJicyY5rFi6Az8UZ7wqB3N5iMuZdpxhKn5KbZDxyuUMuvVt24i5LVPPmmwQtqxMoJ4aLo48a2YvDW6TAkdQjNjvn6KcEEz6GTixujb1YHhMUD8v4AepWKEwKz1ddEca1P2wLQjbpihCuaqbxeohnuZZLogJdUBojBEDgrnrrVpPBaLLEkGSpkJbtrsKUuEeBo1AF3yNgHftLbynGpobVF5DhmsmddmiA6c8vSTokJxHhjpnW8mAcNHBRtmVJCT7VkdHSAhNypM4Hivwfx5jCccG9LauKmCeRMDzHiA57TX9W6ttcPHSvUyQorARQAd2oeNY4H83hZjHh9Bt8iwKZRt4xK6hrTR8tif7hq8eURXrGH9Ys7TzykXK8FHHWvLNzNnYf3E4a9NkD43MjfKvMM1hj4Q2K8MHbmRCqrmFrHP5kim9shq6mhLPTgwha32nvnrBkfPQVPwpGTzKuwE&select=CdsQ7jVNkogQhRzQR3e28Ek39AZ4Ma2y37k5zJaf9EZmQhMjy8GtGm4LGU6dRFuAVG7mYww5xDrQAE74KHQ3Kk1e6661RmcmNALAUjtHyCmrTVBMCnBGNiuq1y7EzmxoodYHU1BV1rnoefQAw2kTBtbWi11hV1P4LcwFCcXfUWF6rpRC7ehEnUCTqUV4bkGVJPLcmk9mdmiGwa2YgmnSShNGPVGZiEi1rMVECyngSRVdqdZwAeXBGWFLfqF1KbZeCo4MTF4SSmFupJ9zLhYbuojEbopyFWHQ6xs3sq9epPeaQziLM4Js7oFYRmuFWUYdFqnZngmewXWmi7tQAgVqhiT6dMjG2eTdfgX6WuRSuoHALkh2XJhHA6GfZLUcxC5Ni9YyKuBTamtaYarbNNJJ8z15WWvuUkLpjgHdEpE2h924xFdu8aoZNuiQxYGvcndaW1BTGMXS5fTKPqYfe2n8Ky2HWPkcX3hEXtyawu1F9BndKNaXLPgsdAoFBArBZnSe28YtSmTa5LRucKVBAxakvv5MWMXchAmpaGFQbZyYUoMgQLcJd7Y96x6zSR7nhwr5Ar81BrmqYz2WFLuk7osUbwsc9HbSG6CQt8p6Vg2u7DjKaZXW8pjkPHAKrHWtHEDiJPJ5rj6VsdFm3\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340796-c9e91b13-8ee0-4a67-bcde-8cf3aaa7ba99.jpg\"> </a> | <a href=\"http://play.aimstack.io:10002/images?grouping=E1zQzcmtDR3wibEa1MVysTvCyZEv1T8ixkCxTWExCyMnHtX2HyiF9eszvPgfd2xdJ5TUTKGpSs1bsLVq5tHAV3uWtsZmmckn6HjNtVCMyQDJpwhiEy5tAyw&select=2NEXuD7fFoaLcwRjymjA1wLmUrGs9s3AiXcCW82C367SwJt18CAB6xzkMGowrUDuDwggE1huaPVcQJpQUsmAQx1CnGiqCUBp2jPMd5mMNPX2QKQMcmvu9ZykBNkeBvCQFPd9ERuQD2g1EjWuvyJ3H53mAZTfp94LCXvR9CUsG5ei2CjQUzfZLM6DCyUr1GPaEVnY5f1EwzicNxXuoutkBgqCqaobJ7Do4q4eHAA6ooiWU6ekS3D2sLj6qYwhVTjfGCPfbWwBiH83nFkY3fLExzdeTY2zeUHeeYikQR9S7xHbVD8WvjekdQVp8X4dNLJZxiVmEqHpPRnU3ZrYsMhE7yFAAgjJwPNUzLTt6YFrtZBcmc4rwAC2oyrqysUSEr6gzL6LcJ6yuqDGf9D5tzftHbTLDkhc8B2sCgTS&images=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\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340790-bc7b7a21-e8a1-43a1-809d-4060b5bfb60f.jpg\"> </a> |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |\r\n\r\n| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/audios\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340778-dbe19620-2f27-4298-b0cb-caf3904760f1.jpg\"> </a> | <a href=\"http://play.aimstack.io:10003/runs/7f083da898624a2c98e0f363/distributions\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340785-a7e4d9fd-d048-4207-8cd1-c4edff9cca6a.jpg\"> </a> |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |\r\n\r\n# Quick Start\r\n\r\nFollow the steps below to get started with Aim.\r\n\r\n**1. Install Aim on your training environment**\r\n\r\n```shell\r\npip3 install aim\r\n```\r\n\r\n**2. Integrate Aim with your code**\r\n\r\n```python\r\nfrom aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\r\n```\r\n\r\n_See the full list of supported trackable objects(e.g. images, text, etc) [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html)._\r\n\r\n**3. Run the training as usual and start Aim UI**\r\n\r\n```shell\r\naim up\r\n```\r\n\r\n**4. Or query runs programmatically via SDK**\r\n\r\n```python\r\nfrom aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\r\n```\r\n\r\n# Integrations\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Lightning\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Hugging Face\r\n</summary>\r\n\r\n```python\r\nfrom aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Keras & tf.keras\r\n</summary>\r\n\r\n```python\r\nimport aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate KerasTuner\r\n</summary>\r\n\r\n```python\r\nfrom aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate XGBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate CatBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost)._\r\n</details>\r\n\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate fastai\r\n</summary>\r\n\r\n```python\r\nfrom aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate LightGBM\r\n</summary>\r\n\r\n```python\r\nfrom aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Ignite\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite)._\r\n</details>\r\n\r\n# Comparisons to familiar tools\r\n\r\n### Tensorboard\r\n**Training run comparison**\r\n\r\nOrder of magnitude faster training run comparison with Aim\r\n- The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.\r\n- With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. **TensorBoard doesn't have features to group, aggregate the metrics**\r\n\r\n**Scalability**\r\n\r\n- Aim is built to handle 1000s of training runs - both on the backend and on the UI.\r\n- TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\r\n\r\n**Beloved TB visualizations to be added on Aim**\r\n\r\n- Embedding projector.\r\n- Neural network visualization.\r\n\r\n### MLFlow\r\nMLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.\r\n\r\n**Run comparison**\r\n\r\n- Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.\r\n- MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.\r\n\r\n**UI Scalability**\r\n\r\n- Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\r\n- MLflow UI becomes slow to use when there are a few hundreds of runs.\r\n\r\n### Weights and Biases\r\n\r\nHosted vs self-hosted\r\n- Weights and Biases is a hosted closed-source MLOps platform.\r\n- Aim is self-hosted, free and open-source experiment tracking tool.\r\n\r\n# Roadmap\r\n\r\n## Detailed Sprints\r\n\r\n:sparkle: The [Aim product roadmap](https://github.com/orgs/aimhubio/projects/3)\r\n\r\n- The `Backlog` contains the issues we are going to choose from and prioritize weekly\r\n- The issues are mainly prioritized by the highly-requested features\r\n\r\n## High-level roadmap\r\n\r\nThe high-level features we are going to work on the next few months\r\n\r\n### Done\r\n  - [x] Live updates (Shipped: _Oct 18 2021_)\r\n  - [x] Images tracking and visualization (Start: _Oct 18 2021_, Shipped: _Nov 19 2021_)\r\n  - [x] Distributions tracking and visualization (Start: _Nov 10 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Jupyter integration (Start: _Nov 18 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Audio tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Transcripts tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Plotly integration (Start: _Dec 1 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Colab integration (Start: _Nov 18 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Centralized tracking server (Start: _Oct 18 2021_, Shipped: _Jan 22 2022_)\r\n  - [x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: _Dec 17 2021_, Shipped: _Feb 3 2022_)\r\n  - [x] Track git info, env vars, CLI arguments, dependencies (Start: _Jan 17 2022_, Shipped: _Feb 3 2022_)\r\n  - [x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Activeloop Hub integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] PyTorch-Ignite integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Run summary and overview info(system params, CLI args, git info, ...) (Start: _Feb 14 2022_, Shipped: _Mar 9 2022_)\r\n  - [x] Add DVC related metadata into aim run (Start: _Mar 7 2022_, Shipped: _Mar 26 2022_)\r\n  - [x] Ability to attach notes to Run from UI (Start: _Mar 7 2022_, Shipped: _Apr 29 2022_)\r\n  - [x] Fairseq integration (Start: _Mar 27 2022_, Shipped: _Mar 29 2022_)\r\n  - [x] LightGBM integration (Start: _Apr 14 2022_, Shipped: _May 17 2022_)\r\n  - [x] CatBoost integration (Start: _Apr 20 2022_, Shipped: _May 17 2022_)\r\n  - [x] Run execution details(display stdout/stderr logs) (Start: _Apr 25 2022_, Shipped: _May 17 2022_)\r\n  - [x] Long sequences(up to 5M of steps) support (Start: _Apr 25 2022_, Shipped: _Jun 22 2022_)\r\n  - [x] Figures Explorer (Start: _Mar 1 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Notify on stuck runs (Start: _Jul 22 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with KerasTuner (Start: _Aug 10 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with WandB (Start: _Aug 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Stable remote tracking server (Start: _Jun 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with fast.ai (Start: _Aug 22 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Integration with MXNet (Start: _Sep 20 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Project overview page (Start: _Sep 1 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Remote tracking server scaling (Start: _Sep 11 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with PaddlePaddle (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with Optuna (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Audios Explorer (Start: _Oct 30 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Experiment page (Start: _Nov 9 2022_, Shipped: _Nov 26 2022_)\r\n\r\n### In Progress\r\n  - [ ] Aim SDK low-level interface (Start: _Aug 22 2022_, )\r\n  - [ ] HuggingFace datasets (Start: _Dec 29 2022_, )\r\n\r\n### To Do\r\n\r\n**Aim UI**\r\n\r\n- Runs management\r\n    - Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard\r\n- Explorers\r\n    - Text Explorer\r\n    - Distributions Explorer\r\n- Dashboards – customizable layouts with embedded explorers\r\n\r\n**SDK and Storage**\r\n\r\n- Scalability\r\n    - Smooth UI and SDK experience with over 10.000 runs\r\n- Runs management\r\n    - CLI interfaces\r\n        - Reporting - runs summary and run details in a CLI compatible format\r\n        - Manipulations – copy, move, delete runs, params and sequences\r\n\r\n**Integrations**\r\n\r\n- ML Frameworks:\r\n    - Shortlist: MONAI, SpaCy, Raytune\r\n- Resource management tools\r\n    - Shortlist: Kubeflow, Slurm\r\n- Workflow orchestration tools\r\n- Others: Hydra, Google MLMD, Streamlit, ...\r\n\r\n### On hold\r\n\r\n- scikit-learn integration\r\n- Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: _Mar 21 2022_)\r\n- Artifact storage – store files, model checkpoints, and beyond (Start: _Mar 21 2022_)\r\n\r\n## Community\r\n\r\n### If you have questions\r\n\r\n1. [Read the docs](https://aimstack.readthedocs.io/en/latest/)\r\n2. [Open a feature request or report a bug](https://github.com/aimhubio/aim/issues)\r\n3. [Join Discord community server](https://community.aimstack.io/)<div align=\"center\">\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/154338760-edfe1885-06f3-4e02-87fe-4b13a403516b.png\">\r\n  <h3>An easy-to-use & supercharged open-source experiment tracker</h3>\r\n  Aim logs your training runs, enables a beautiful UI to compare them and an API to query them programmatically.\r\n</div>\r\n\r\n<br/>\r\n\r\n<img src=\"https://user-images.githubusercontent.com/13848158/154338753-34484cda-95b8-4da8-a610-7fdf198c05fd.png\">\r\n\r\n<p align=\"center\">\r\n  <a href=\"#about-aim\"><b>About</b></a> &bull;\r\n  <a href=\"#why-use-aim\"><b>Features</b></a> &bull;\r\n  <a href=\"#demos\"><b>Demos</b></a> &bull;\r\n  <a href=\"https://github.com/aimhubio/aim/tree/main/examples\"><b>Examples</b></a> &bull;\r\n  <a href=\"#quick-start\"><b>Quick Start</b></a> &bull;\r\n  <a href=\"https://aimstack.readthedocs.io/en/latest/\"><b>Documentation</b></a> &bull;\r\n  <a href=\"#roadmap\"><b>Roadmap</b></a> &bull;\r\n  <a href=\"https://community.aimstack.io/\"><b>Discord Community</b></a> &bull;\r\n  <a href=\"https://twitter.com/aimstackio\"><b>Twitter</b></a>\r\n</p>\r\n\r\n<div align=\"center\">\r\n  \r\n  [![Platform Support](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue)]()\r\n  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aim)](https://pypi.org/project/aim/)\r\n  [![PyPI Package](https://img.shields.io/pypi/v/aim?color=yellow)](https://pypi.org/project/aim/)\r\n  [![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)\r\n  [![PyPI Downloads](https://img.shields.io/pypi/dw/aim?color=green)](https://pypi.org/project/aim/)\r\n  [![Issues](https://img.shields.io/github/issues/aimhubio/aim)](http://github.com/aimhubio/aim/issues)\r\n  \r\n</div>\r\n\r\n<div align=\"center\">\r\n  <sub>Integrates seamlessly with your favorite tools</sub>\r\n  <br/>\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354389-d0301620-77ea-4629-a743-f7aa249e14b5.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354496-b39d7b1c-63ef-40f0-9e59-c08d2c5e337c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354380-3755c741-6960-42ca-b93e-84a8791f088c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354342-7df0ef5e-63d2-4df7-b9f1-d2fc0e95f53f.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354392-afbff3de-c845-4d86-855d-53df569f91d1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354355-89210506-e7e5-4d37-b2d6-ad3fda62ef13.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354397-8af8e1d3-4067-405e-9d42-1f131663ed22.png\" width=\"60\" />\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354513-f7486146-3891-4f3f-934f-e58bbf9ce695.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354500-c0471ce6-b2ce-4172-b9e4-07a197256303.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354361-9f911785-008d-4b75-877e-651e026cf47e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354373-1879ae61-b5d1-41f0-a4f1-04b639b6f05e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354483-75d9853f-7154-4d95-8190-9ad7a73d6654.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354329-cf7c3352-a72a-478d-82a7-04e3833b03b7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354349-dcdf3bc3-d7a9-4f34-8258-4824a57f59c7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354471-518f1814-7a41-4b23-9caf-e516507343f1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165162736-2cc5da39-38aa-4093-874f-e56d0ba9cea2.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165074282-36ad18eb-1124-434d-8439-728c22cd7ac7.png\" width=\"60\" />\r\n</div>\r\n\r\n<div align=\"center\">\r\n  <br/>\r\n  <kbd>\r\n    <img width=\"650px\" src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" />\r\n  </kbd>\r\n</div>\r\n\r\n# About Aim\r\n\r\n| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n| <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337794-e9310239-6614-41b3-a95b-bb91f0bb6c4f.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337788-03fe5b31-0fa3-44af-ae79-2861707d8602.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337793-85175c78-5659-4dd0-bb2d-05017278e2fa.png\"> |\r\n\r\nAim is an open-source, self-hosted ML experiment tracking tool. \r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\r\n\r\nYou can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically. \r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.\r\n\r\n\r\nAim's mission is to democratize AI dev tools.\r\n\r\n# Why use Aim?\r\n\r\n### Compare 100s of runs in a few clicks - build models faster\r\n\r\n- Compare, group and aggregate 100s of metrics thanks to effective visualizations.\r\n- Analyze, learn correlations and patterns between hparams and metrics.\r\n- Easy pythonic search to query the runs you want to explore.\r\n\r\n### Deep dive into details of each run for easy debugging\r\n\r\n- Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.\r\n- Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.\r\n- Analyze system resource usage to effectively utilize computational resources.\r\n\r\n### Have all relevant information organised and accessible for easy governance\r\n\r\n- Centralized dashboard to holistically view all your runs, their hparams and results.\r\n- Use SDK to query/access all your runs and tracked metadata.\r\n- You own your data - Aim is open source and self hosted.\r\n\r\n# Demos\r\n\r\n| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10001/metrics?grouping=HQGdK9Xxy35e6sY1CYkCmk1WbWMN2AsCNfJJ3d1RJYLtrVPMoF5UpGiA6CF8bEJnfzRsKpqespf3AEuKSVrhUYvYk9MxzNGA9XZWYUf6phEg8AMbZGLRVDXnAPDuo8tueqsST1ZLizWzQwDYJWHUza6pyB2Eojt9uWqNHUdb858TqDRnCJzqiVJXKXEzFWUyvU8MckJo1qpqWWCTb4GpYN6DUJZx2GXDGR21e2xxd4m7PmNUnbA9B3apLttZoipJF6c3v7tNUKmb6irpqnNB3yc57tqYDa1XZuKfDxkMtyFdQ1x95K4jjsTVwhftEWLze35QNcxNXRCGGS9o9yEfTLG26GUX2zjPZFCjjMGU6vV7z1xRccK8MyoGrLSgAQCbvk68dTGBHpXUBvCRq8N&chart=FviZzVrt4fVQPjpCLr9sVGGrcR5etSroyqambiKpm3nTgpyv4eQxKuwNX9uN8UtKmzYUhUyTMBEANHmtbwjLApkvnYeNbxGNC6PVcoqi65m1XJnSrvgt8WiD89BapFAWRUwAGx6SWD7KZPsk3RQyysU7W7FjD3Q99NusxFGhsEfD6HXc7i8xH9KHDRGjLwh6x9VTtSp4FS8HEvpLSiiJoX7LCTi8pB7dXvrQ8G5w3jPsFz4qXYFdsVaCNL1BpFFZuiqQNkfbnM84gEq7UmiV1VzM4oS3AgQHxADG3kpBVp6eKTey9F1Swd4FcUkFA9QEPjgQgqwRGjkquZ2bdDDVLBnCh7JPvboP2kifCiZZ5MDdV9MMx6PKHp4DusWyWLXiHQYPkpGPWBiuccMUXDsuJaCWJbuABdY7CyiJMv1jdHYkjabygSxehPVyEDefWAtjBfv2vaeM1xv63jadbmpKYFxft7qmuT9HvVxiGvRgs4RQFxy8K4rtFBca3HNs1mDaaY81gy9MGXyw7BS5Fniu92jaJpsWDdg6Y3AQBLZtrpJy2obEZ4yzJaCVT7JUNPAyyCUNLck393VFLoEkaD9CU5npK5R7tj1c1G3gkMNQXnSXy5NpSj8deMmXV5qz3JKu1nq2caGQKcqjzy2gLkExdm674AMFjSg9yFjK6VqASXQ17NKtWRUvaYoxGbHDAFQaMKWKh8QLm22QA9mKT8NksLptWozbgDvafnQLNMvezLU5bvKV5o75PAWYiRB56RcYfEhzaB6YWdgL7TJicyY5rFi6Az8UZ7wqB3N5iMuZdpxhKn5KbZDxyuUMuvVt24i5LVPPmmwQtqxMoJ4aLo48a2YvDW6TAkdQjNjvn6KcEEz6GTixujb1YHhMUD8v4AepWKEwKz1ddEca1P2wLQjbpihCuaqbxeohnuZZLogJdUBojBEDgrnrrVpPBaLLEkGSpkJbtrsKUuEeBo1AF3yNgHftLbynGpobVF5DhmsmddmiA6c8vSTokJxHhjpnW8mAcNHBRtmVJCT7VkdHSAhNypM4Hivwfx5jCccG9LauKmCeRMDzHiA57TX9W6ttcPHSvUyQorARQAd2oeNY4H83hZjHh9Bt8iwKZRt4xK6hrTR8tif7hq8eURXrGH9Ys7TzykXK8FHHWvLNzNnYf3E4a9NkD43MjfKvMM1hj4Q2K8MHbmRCqrmFrHP5kim9shq6mhLPTgwha32nvnrBkfPQVPwpGTzKuwE&select=CdsQ7jVNkogQhRzQR3e28Ek39AZ4Ma2y37k5zJaf9EZmQhMjy8GtGm4LGU6dRFuAVG7mYww5xDrQAE74KHQ3Kk1e6661RmcmNALAUjtHyCmrTVBMCnBGNiuq1y7EzmxoodYHU1BV1rnoefQAw2kTBtbWi11hV1P4LcwFCcXfUWF6rpRC7ehEnUCTqUV4bkGVJPLcmk9mdmiGwa2YgmnSShNGPVGZiEi1rMVECyngSRVdqdZwAeXBGWFLfqF1KbZeCo4MTF4SSmFupJ9zLhYbuojEbopyFWHQ6xs3sq9epPeaQziLM4Js7oFYRmuFWUYdFqnZngmewXWmi7tQAgVqhiT6dMjG2eTdfgX6WuRSuoHALkh2XJhHA6GfZLUcxC5Ni9YyKuBTamtaYarbNNJJ8z15WWvuUkLpjgHdEpE2h924xFdu8aoZNuiQxYGvcndaW1BTGMXS5fTKPqYfe2n8Ky2HWPkcX3hEXtyawu1F9BndKNaXLPgsdAoFBArBZnSe28YtSmTa5LRucKVBAxakvv5MWMXchAmpaGFQbZyYUoMgQLcJd7Y96x6zSR7nhwr5Ar81BrmqYz2WFLuk7osUbwsc9HbSG6CQt8p6Vg2u7DjKaZXW8pjkPHAKrHWtHEDiJPJ5rj6VsdFm3\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340796-c9e91b13-8ee0-4a67-bcde-8cf3aaa7ba99.jpg\"> </a> | <a href=\"http://play.aimstack.io:10002/images?grouping=E1zQzcmtDR3wibEa1MVysTvCyZEv1T8ixkCxTWExCyMnHtX2HyiF9eszvPgfd2xdJ5TUTKGpSs1bsLVq5tHAV3uWtsZmmckn6HjNtVCMyQDJpwhiEy5tAyw&select=2NEXuD7fFoaLcwRjymjA1wLmUrGs9s3AiXcCW82C367SwJt18CAB6xzkMGowrUDuDwggE1huaPVcQJpQUsmAQx1CnGiqCUBp2jPMd5mMNPX2QKQMcmvu9ZykBNkeBvCQFPd9ERuQD2g1EjWuvyJ3H53mAZTfp94LCXvR9CUsG5ei2CjQUzfZLM6DCyUr1GPaEVnY5f1EwzicNxXuoutkBgqCqaobJ7Do4q4eHAA6ooiWU6ekS3D2sLj6qYwhVTjfGCPfbWwBiH83nFkY3fLExzdeTY2zeUHeeYikQR9S7xHbVD8WvjekdQVp8X4dNLJZxiVmEqHpPRnU3ZrYsMhE7yFAAgjJwPNUzLTt6YFrtZBcmc4rwAC2oyrqysUSEr6gzL6LcJ6yuqDGf9D5tzftHbTLDkhc8B2sCgTS&images=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\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340790-bc7b7a21-e8a1-43a1-809d-4060b5bfb60f.jpg\"> </a> |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |\r\n\r\n| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/audios\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340778-dbe19620-2f27-4298-b0cb-caf3904760f1.jpg\"> </a> | <a href=\"http://play.aimstack.io:10003/runs/7f083da898624a2c98e0f363/distributions\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340785-a7e4d9fd-d048-4207-8cd1-c4edff9cca6a.jpg\"> </a> |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |\r\n\r\n# Quick Start\r\n\r\nFollow the steps below to get started with Aim.\r\n\r\n**1. Install Aim on your training environment**\r\n\r\n```shell\r\npip3 install aim\r\n```\r\n\r\n**2. Integrate Aim with your code**\r\n\r\n```python\r\nfrom aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\r\n```\r\n\r\n_See the full list of supported trackable objects(e.g. images, text, etc) [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html)._\r\n\r\n**3. Run the training as usual and start Aim UI**\r\n\r\n```shell\r\naim up\r\n```\r\n\r\n**4. Or query runs programmatically via SDK**\r\n\r\n```python\r\nfrom aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\r\n```\r\n\r\n# Integrations\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Lightning\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Hugging Face\r\n</summary>\r\n\r\n```python\r\nfrom aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Keras & tf.keras\r\n</summary>\r\n\r\n```python\r\nimport aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate KerasTuner\r\n</summary>\r\n\r\n```python\r\nfrom aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate XGBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate CatBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost)._\r\n</details>\r\n\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate fastai\r\n</summary>\r\n\r\n```python\r\nfrom aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate LightGBM\r\n</summary>\r\n\r\n```python\r\nfrom aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Ignite\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite)._\r\n</details>\r\n\r\n# Comparisons to familiar tools\r\n\r\n### Tensorboard\r\n**Training run comparison**\r\n\r\nOrder of magnitude faster training run comparison with Aim\r\n- The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.\r\n- With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. **TensorBoard doesn't have features to group, aggregate the metrics**\r\n\r\n**Scalability**\r\n\r\n- Aim is built to handle 1000s of training runs - both on the backend and on the UI.\r\n- TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\r\n\r\n**Beloved TB visualizations to be added on Aim**\r\n\r\n- Embedding projector.\r\n- Neural network visualization.\r\n\r\n### MLFlow\r\nMLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.\r\n\r\n**Run comparison**\r\n\r\n- Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.\r\n- MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.\r\n\r\n**UI Scalability**\r\n\r\n- Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\r\n- MLflow UI becomes slow to use when there are a few hundreds of runs.\r\n\r\n### Weights and Biases\r\n\r\nHosted vs self-hosted\r\n- Weights and Biases is a hosted closed-source MLOps platform.\r\n- Aim is self-hosted, free and open-source experiment tracking tool.\r\n\r\n# Roadmap\r\n\r\n## Detailed Sprints\r\n\r\n:sparkle: The [Aim product roadmap](https://github.com/orgs/aimhubio/projects/3)\r\n\r\n- The `Backlog` contains the issues we are going to choose from and prioritize weekly\r\n- The issues are mainly prioritized by the highly-requested features\r\n\r\n## High-level roadmap\r\n\r\nThe high-level features we are going to work on the next few months\r\n\r\n### Done\r\n  - [x] Live updates (Shipped: _Oct 18 2021_)\r\n  - [x] Images tracking and visualization (Start: _Oct 18 2021_, Shipped: _Nov 19 2021_)\r\n  - [x] Distributions tracking and visualization (Start: _Nov 10 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Jupyter integration (Start: _Nov 18 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Audio tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Transcripts tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Plotly integration (Start: _Dec 1 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Colab integration (Start: _Nov 18 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Centralized tracking server (Start: _Oct 18 2021_, Shipped: _Jan 22 2022_)\r\n  - [x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: _Dec 17 2021_, Shipped: _Feb 3 2022_)\r\n  - [x] Track git info, env vars, CLI arguments, dependencies (Start: _Jan 17 2022_, Shipped: _Feb 3 2022_)\r\n  - [x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Activeloop Hub integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] PyTorch-Ignite integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Run summary and overview info(system params, CLI args, git info, ...) (Start: _Feb 14 2022_, Shipped: _Mar 9 2022_)\r\n  - [x] Add DVC related metadata into aim run (Start: _Mar 7 2022_, Shipped: _Mar 26 2022_)\r\n  - [x] Ability to attach notes to Run from UI (Start: _Mar 7 2022_, Shipped: _Apr 29 2022_)\r\n  - [x] Fairseq integration (Start: _Mar 27 2022_, Shipped: _Mar 29 2022_)\r\n  - [x] LightGBM integration (Start: _Apr 14 2022_, Shipped: _May 17 2022_)\r\n  - [x] CatBoost integration (Start: _Apr 20 2022_, Shipped: _May 17 2022_)\r\n  - [x] Run execution details(display stdout/stderr logs) (Start: _Apr 25 2022_, Shipped: _May 17 2022_)\r\n  - [x] Long sequences(up to 5M of steps) support (Start: _Apr 25 2022_, Shipped: _Jun 22 2022_)\r\n  - [x] Figures Explorer (Start: _Mar 1 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Notify on stuck runs (Start: _Jul 22 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with KerasTuner (Start: _Aug 10 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with WandB (Start: _Aug 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Stable remote tracking server (Start: _Jun 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with fast.ai (Start: _Aug 22 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Integration with MXNet (Start: _Sep 20 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Project overview page (Start: _Sep 1 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Remote tracking server scaling (Start: _Sep 11 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with PaddlePaddle (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with Optuna (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Audios Explorer (Start: _Oct 30 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Experiment page (Start: _Nov 9 2022_, Shipped: _Nov 26 2022_)\r\n\r\n### In Progress\r\n  - [ ] Aim SDK low-level interface (Start: _Aug 22 2022_, )\r\n  - [ ] HuggingFace datasets (Start: _Dec 29 2022_, )\r\n\r\n### To Do\r\n\r\n**Aim UI**\r\n\r\n- Runs management\r\n    - Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard\r\n- Explorers\r\n    - Text Explorer\r\n    - Distributions Explorer\r\n- Dashboards – customizable layouts with embedded explorers\r\n\r\n**SDK and Storage**\r\n\r\n- Scalability\r\n    - Smooth UI and SDK experience with over 10.000 runs\r\n- Runs management\r\n    - CLI interfaces\r\n        - Reporting - runs summary and run details in a CLI compatible format\r\n        - Manipulations – copy, move, delete runs, params and sequences\r\n\r\n**Integrations**\r\n\r\n- ML Frameworks:\r\n    - Shortlist: MONAI, SpaCy, Raytune\r\n- Resource management tools\r\n    - Shortlist: Kubeflow, Slurm\r\n- Workflow orchestration tools\r\n- Others: Hydra, Google MLMD, Streamlit, ...\r\n\r\n### On hold\r\n\r\n- scikit-learn integration\r\n- Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: _Mar 21 2022_)\r\n- Artifact storage – store files, model checkpoints, and beyond (Start: _Mar 21 2022_)\r\n\r\n## Community\r\n\r\n### If you have questions\r\n\r\n1. [Read the docs](https://aimstack.readthedocs.io/en/latest/)\r\n2. [Open a feature request or report a bug](https://github.com/aimhubio/aim/issues)\r\n3. [Join Discord community server](https://community.aimstack.io/)","html":"<p><a href=\"\"><img src=\"https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue\" alt=\"Platform Support\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/pyversions/aim\" alt=\"PyPI - Python Version\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/v/aim?color=yellow\" alt=\"PyPI Package\"></a>\r\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-orange.svg\" alt=\"License\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/dw/aim?color=green\" alt=\"PyPI Downloads\"></a>\r\n<a href=\"http://github.com/aimhubio/aim/issues\"><img src=\"https://img.shields.io/github/issues/aimhubio/aim\" alt=\"Issues\"></a></p>\n<h1>About Aim</h1>\n<p>| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n|  |  |  |</p>\n<p>Aim is an open-source, self-hosted ML experiment tracking tool.\r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.</p>\n<p>You can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically.\r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.</p>\n<p>Aim's mission is to democratize AI dev tools.</p>\n<h1>Why use Aim?</h1>\n<h3>Compare 100s of runs in a few clicks - build models faster</h3>\n<ul>\n<li>Compare, group and aggregate 100s of metrics thanks to effective visualizations.</li>\n<li>Analyze, learn correlations and patterns between hparams and metrics.</li>\n<li>Easy pythonic search to query the runs you want to explore.</li>\n</ul>\n<h3>Deep dive into details of each run for easy debugging</h3>\n<ul>\n<li>Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.</li>\n<li>Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.</li>\n<li>Analyze system resource usage to effectively utilize computational resources.</li>\n</ul>\n<h3>Have all relevant information organised and accessible for easy governance</h3>\n<ul>\n<li>Centralized dashboard to holistically view all your runs, their hparams and results.</li>\n<li>Use SDK to query/access all your runs and tracked metadata.</li>\n<li>You own your data - Aim is open source and self hosted.</li>\n</ul>\n<h1>Demos</h1>\n<p>| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |</p>\n<p>| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |</p>\n<h1>Quick Start</h1>\n<p>Follow the steps below to get started with Aim.</p>\n<p><strong>1. Install Aim on your training environment</strong></p>\n<pre><code class=\"language-shell\">pip3 install aim\n</code></pre>\n<p><strong>2. Integrate Aim with your code</strong></p>\n<pre><code class=\"language-python\">from aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n</code></pre>\n<p><em>See the full list of supported trackable objects(e.g. images, text, etc) <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html\">here</a>.</em></p>\n<p><strong>3. Run the training as usual and start Aim UI</strong></p>\n<pre><code class=\"language-shell\">aim up\n</code></pre>\n<p><strong>4. Or query runs programmatically via SDK</strong></p>\n<pre><code class=\"language-python\">from aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\n</code></pre>\n<h1>Integrations</h1>\n<pre><code class=\"language-python\">from aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face\">here</a>.</em></p>\n<pre><code class=\"language-python\">import aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite\">here</a>.</em></p>\n<h1>Comparisons to familiar tools</h1>\n<h3>Tensorboard</h3>\n<p><strong>Training run comparison</strong></p>\n<p>Order of magnitude faster training run comparison with Aim</p>\n<ul>\n<li>The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.</li>\n<li>With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. <strong>TensorBoard doesn't have features to group, aggregate the metrics</strong></li>\n</ul>\n<p><strong>Scalability</strong></p>\n<ul>\n<li>Aim is built to handle 1000s of training runs - both on the backend and on the UI.</li>\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\n</ul>\n<p><strong>Beloved TB visualizations to be added on Aim</strong></p>\n<ul>\n<li>Embedding projector.</li>\n<li>Neural network visualization.</li>\n</ul>\n<h3>MLFlow</h3>\n<p>MLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\n<p><strong>Run comparison</strong></p>\n<ul>\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.</li>\n<li>MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.</li>\n</ul>\n<p><strong>UI Scalability</strong></p>\n<ul>\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\n</ul>\n<h3>Weights and Biases</h3>\n<p>Hosted vs self-hosted</p>\n<ul>\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\n</ul>\n<h1>Roadmap</h1>\n<h2>Detailed Sprints</h2>\n<p>:sparkle: The <a href=\"https://github.com/orgs/aimhubio/projects/3\">Aim product roadmap</a></p>\n<ul>\n<li>The <code>Backlog</code> contains the issues we are going to choose from and prioritize weekly</li>\n<li>The issues are mainly prioritized by the highly-requested features</li>\n</ul>\n<h2>High-level roadmap</h2>\n<p>The high-level features we are going to work on the next few months</p>\n<h3>Done</h3>\n<ul>\n<li>[x] Live updates (Shipped: <em>Oct 18 2021</em>)</li>\n<li>[x] Images tracking and visualization (Start: <em>Oct 18 2021</em>, Shipped: <em>Nov 19 2021</em>)</li>\n<li>[x] Distributions tracking and visualization (Start: <em>Nov 10 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Jupyter integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Audio tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Transcripts tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Plotly integration (Start: <em>Dec 1 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Colab integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Centralized tracking server (Start: <em>Oct 18 2021</em>, Shipped: <em>Jan 22 2022</em>)</li>\n<li>[x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: <em>Dec 17 2021</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] Track git info, env vars, CLI arguments, dependencies (Start: <em>Jan 17 2022</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Activeloop Hub integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] PyTorch-Ignite integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Run summary and overview info(system params, CLI args, git info, ...) (Start: <em>Feb 14 2022</em>, Shipped: <em>Mar 9 2022</em>)</li>\n<li>[x] Add DVC related metadata into aim run (Start: <em>Mar 7 2022</em>, Shipped: <em>Mar 26 2022</em>)</li>\n<li>[x] Ability to attach notes to Run from UI (Start: <em>Mar 7 2022</em>, Shipped: <em>Apr 29 2022</em>)</li>\n<li>[x] Fairseq integration (Start: <em>Mar 27 2022</em>, Shipped: <em>Mar 29 2022</em>)</li>\n<li>[x] LightGBM integration (Start: <em>Apr 14 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] CatBoost integration (Start: <em>Apr 20 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Run execution details(display stdout/stderr logs) (Start: <em>Apr 25 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Long sequences(up to 5M of steps) support (Start: <em>Apr 25 2022</em>, Shipped: <em>Jun 22 2022</em>)</li>\n<li>[x] Figures Explorer (Start: <em>Mar 1 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Notify on stuck runs (Start: <em>Jul 22 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with KerasTuner (Start: <em>Aug 10 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with WandB (Start: <em>Aug 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Stable remote tracking server (Start: <em>Jun 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with fast.ai (Start: <em>Aug 22 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Integration with MXNet (Start: <em>Sep 20 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Project overview page (Start: <em>Sep 1 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Remote tracking server scaling (Start: <em>Sep 11 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with PaddlePaddle (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with Optuna (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Audios Explorer (Start: <em>Oct 30 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Experiment page (Start: <em>Nov 9 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n</ul>\n<h3>In Progress</h3>\n<ul>\n<li>[ ] Aim SDK low-level interface (Start: <em>Aug 22 2022</em>, )</li>\n<li>[ ] HuggingFace datasets (Start: <em>Dec 29 2022</em>, )</li>\n</ul>\n<h3>To Do</h3>\n<p><strong>Aim UI</strong></p>\n<ul>\n<li>Runs management\n<ul>\n<li>Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard</li>\n</ul>\n</li>\n<li>Explorers\n<ul>\n<li>Text Explorer</li>\n<li>Distributions Explorer</li>\n</ul>\n</li>\n<li>Dashboards – customizable layouts with embedded explorers</li>\n</ul>\n<p><strong>SDK and Storage</strong></p>\n<ul>\n<li>Scalability\n<ul>\n<li>Smooth UI and SDK experience with over 10.000 runs</li>\n</ul>\n</li>\n<li>Runs management\n<ul>\n<li>CLI interfaces\n<ul>\n<li>Reporting - runs summary and run details in a CLI compatible format</li>\n<li>Manipulations – copy, move, delete runs, params and sequences</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Integrations</strong></p>\n<ul>\n<li>ML Frameworks:\n<ul>\n<li>Shortlist: MONAI, SpaCy, Raytune</li>\n</ul>\n</li>\n<li>Resource management tools\n<ul>\n<li>Shortlist: Kubeflow, Slurm</li>\n</ul>\n</li>\n<li>Workflow orchestration tools</li>\n<li>Others: Hydra, Google MLMD, Streamlit, ...</li>\n</ul>\n<h3>On hold</h3>\n<ul>\n<li>scikit-learn integration</li>\n<li>Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: <em>Mar 21 2022</em>)</li>\n<li>Artifact storage – store files, model checkpoints, and beyond (Start: <em>Mar 21 2022</em>)</li>\n</ul>\n<h2>Community</h2>\n<h3>If you have questions</h3>\n<ol>\n<li><a href=\"https://aimstack.readthedocs.io/en/latest/\">Read the docs</a></li>\n<li><a href=\"https://github.com/aimhubio/aim/issues\">Open a feature request or report a bug</a></li>\n<li><a href=\"https://community.aimstack.io/\">Join Discord community server</a></li>\n</ol>\n<p><a href=\"\"><img src=\"https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue\" alt=\"Platform Support\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/pyversions/aim\" alt=\"PyPI - Python Version\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/v/aim?color=yellow\" alt=\"PyPI Package\"></a>\r\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-orange.svg\" alt=\"License\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/dw/aim?color=green\" alt=\"PyPI Downloads\"></a>\r\n<a href=\"http://github.com/aimhubio/aim/issues\"><img src=\"https://img.shields.io/github/issues/aimhubio/aim\" alt=\"Issues\"></a></p>\n<h1>About Aim</h1>\n<p>| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n|  |  |  |</p>\n<p>Aim is an open-source, self-hosted ML experiment tracking tool.\r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.</p>\n<p>You can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically.\r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.</p>\n<p>Aim's mission is to democratize AI dev tools.</p>\n<h1>Why use Aim?</h1>\n<h3>Compare 100s of runs in a few clicks - build models faster</h3>\n<ul>\n<li>Compare, group and aggregate 100s of metrics thanks to effective visualizations.</li>\n<li>Analyze, learn correlations and patterns between hparams and metrics.</li>\n<li>Easy pythonic search to query the runs you want to explore.</li>\n</ul>\n<h3>Deep dive into details of each run for easy debugging</h3>\n<ul>\n<li>Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.</li>\n<li>Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.</li>\n<li>Analyze system resource usage to effectively utilize computational resources.</li>\n</ul>\n<h3>Have all relevant information organised and accessible for easy governance</h3>\n<ul>\n<li>Centralized dashboard to holistically view all your runs, their hparams and results.</li>\n<li>Use SDK to query/access all your runs and tracked metadata.</li>\n<li>You own your data - Aim is open source and self hosted.</li>\n</ul>\n<h1>Demos</h1>\n<p>| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |</p>\n<p>| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |</p>\n<h1>Quick Start</h1>\n<p>Follow the steps below to get started with Aim.</p>\n<p><strong>1. Install Aim on your training environment</strong></p>\n<pre><code class=\"language-shell\">pip3 install aim\n</code></pre>\n<p><strong>2. Integrate Aim with your code</strong></p>\n<pre><code class=\"language-python\">from aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n</code></pre>\n<p><em>See the full list of supported trackable objects(e.g. images, text, etc) <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html\">here</a>.</em></p>\n<p><strong>3. Run the training as usual and start Aim UI</strong></p>\n<pre><code class=\"language-shell\">aim up\n</code></pre>\n<p><strong>4. Or query runs programmatically via SDK</strong></p>\n<pre><code class=\"language-python\">from aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\n</code></pre>\n<h1>Integrations</h1>\n<pre><code class=\"language-python\">from aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face\">here</a>.</em></p>\n<pre><code class=\"language-python\">import aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite\">here</a>.</em></p>\n<h1>Comparisons to familiar tools</h1>\n<h3>Tensorboard</h3>\n<p><strong>Training run comparison</strong></p>\n<p>Order of magnitude faster training run comparison with Aim</p>\n<ul>\n<li>The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.</li>\n<li>With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. <strong>TensorBoard doesn't have features to group, aggregate the metrics</strong></li>\n</ul>\n<p><strong>Scalability</strong></p>\n<ul>\n<li>Aim is built to handle 1000s of training runs - both on the backend and on the UI.</li>\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\n</ul>\n<p><strong>Beloved TB visualizations to be added on Aim</strong></p>\n<ul>\n<li>Embedding projector.</li>\n<li>Neural network visualization.</li>\n</ul>\n<h3>MLFlow</h3>\n<p>MLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\n<p><strong>Run comparison</strong></p>\n<ul>\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.</li>\n<li>MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.</li>\n</ul>\n<p><strong>UI Scalability</strong></p>\n<ul>\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\n</ul>\n<h3>Weights and Biases</h3>\n<p>Hosted vs self-hosted</p>\n<ul>\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\n</ul>\n<h1>Roadmap</h1>\n<h2>Detailed Sprints</h2>\n<p>:sparkle: The <a href=\"https://github.com/orgs/aimhubio/projects/3\">Aim product roadmap</a></p>\n<ul>\n<li>The <code>Backlog</code> contains the issues we are going to choose from and prioritize weekly</li>\n<li>The issues are mainly prioritized by the highly-requested features</li>\n</ul>\n<h2>High-level roadmap</h2>\n<p>The high-level features we are going to work on the next few months</p>\n<h3>Done</h3>\n<ul>\n<li>[x] Live updates (Shipped: <em>Oct 18 2021</em>)</li>\n<li>[x] Images tracking and visualization (Start: <em>Oct 18 2021</em>, Shipped: <em>Nov 19 2021</em>)</li>\n<li>[x] Distributions tracking and visualization (Start: <em>Nov 10 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Jupyter integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Audio tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Transcripts tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Plotly integration (Start: <em>Dec 1 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Colab integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Centralized tracking server (Start: <em>Oct 18 2021</em>, Shipped: <em>Jan 22 2022</em>)</li>\n<li>[x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: <em>Dec 17 2021</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] Track git info, env vars, CLI arguments, dependencies (Start: <em>Jan 17 2022</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Activeloop Hub integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] PyTorch-Ignite integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Run summary and overview info(system params, CLI args, git info, ...) (Start: <em>Feb 14 2022</em>, Shipped: <em>Mar 9 2022</em>)</li>\n<li>[x] Add DVC related metadata into aim run (Start: <em>Mar 7 2022</em>, Shipped: <em>Mar 26 2022</em>)</li>\n<li>[x] Ability to attach notes to Run from UI (Start: <em>Mar 7 2022</em>, Shipped: <em>Apr 29 2022</em>)</li>\n<li>[x] Fairseq integration (Start: <em>Mar 27 2022</em>, Shipped: <em>Mar 29 2022</em>)</li>\n<li>[x] LightGBM integration (Start: <em>Apr 14 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] CatBoost integration (Start: <em>Apr 20 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Run execution details(display stdout/stderr logs) (Start: <em>Apr 25 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Long sequences(up to 5M of steps) support (Start: <em>Apr 25 2022</em>, Shipped: <em>Jun 22 2022</em>)</li>\n<li>[x] Figures Explorer (Start: <em>Mar 1 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Notify on stuck runs (Start: <em>Jul 22 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with KerasTuner (Start: <em>Aug 10 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with WandB (Start: <em>Aug 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Stable remote tracking server (Start: <em>Jun 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with fast.ai (Start: <em>Aug 22 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Integration with MXNet (Start: <em>Sep 20 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Project overview page (Start: <em>Sep 1 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Remote tracking server scaling (Start: <em>Sep 11 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with PaddlePaddle (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with Optuna (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Audios Explorer (Start: <em>Oct 30 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Experiment page (Start: <em>Nov 9 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n</ul>\n<h3>In Progress</h3>\n<ul>\n<li>[ ] Aim SDK low-level interface (Start: <em>Aug 22 2022</em>, )</li>\n<li>[ ] HuggingFace datasets (Start: <em>Dec 29 2022</em>, )</li>\n</ul>\n<h3>To Do</h3>\n<p><strong>Aim UI</strong></p>\n<ul>\n<li>Runs management\n<ul>\n<li>Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard</li>\n</ul>\n</li>\n<li>Explorers\n<ul>\n<li>Text Explorer</li>\n<li>Distributions Explorer</li>\n</ul>\n</li>\n<li>Dashboards – customizable layouts with embedded explorers</li>\n</ul>\n<p><strong>SDK and Storage</strong></p>\n<ul>\n<li>Scalability\n<ul>\n<li>Smooth UI and SDK experience with over 10.000 runs</li>\n</ul>\n</li>\n<li>Runs management\n<ul>\n<li>CLI interfaces\n<ul>\n<li>Reporting - runs summary and run details in a CLI compatible format</li>\n<li>Manipulations – copy, move, delete runs, params and sequences</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Integrations</strong></p>\n<ul>\n<li>ML Frameworks:\n<ul>\n<li>Shortlist: MONAI, SpaCy, Raytune</li>\n</ul>\n</li>\n<li>Resource management tools\n<ul>\n<li>Shortlist: Kubeflow, Slurm</li>\n</ul>\n</li>\n<li>Workflow orchestration tools</li>\n<li>Others: Hydra, Google MLMD, Streamlit, ...</li>\n</ul>\n<h3>On hold</h3>\n<ul>\n<li>scikit-learn integration</li>\n<li>Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: <em>Mar 21 2022</em>)</li>\n<li>Artifact storage – store files, model checkpoints, and beyond (Start: <em>Mar 21 2022</em>)</li>\n</ul>\n<h2>Community</h2>\n<h3>If you have questions</h3>\n<ol>\n<li><a href=\"https://aimstack.readthedocs.io/en/latest/\">Read the docs</a></li>\n<li><a href=\"https://github.com/aimhubio/aim/issues\">Open a feature request or report a bug</a></li>\n<li><a href=\"https://community.aimstack.io/\">Join Discord community server</a></li>\n</ol>"},"_id":"posts/new-post.md","_raw":{"sourceFilePath":"posts/new-post.md","sourceFileName":"new-post.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/new-post"},"type":"Post"},{"title":"post with new markdown","date":"2023-01-25T14:33:02.283Z","author":"Ashot","description":"description","slug":"post-with-new-markdown","image":"/images/dynamic/video-thumbnail.png","draft":false,"categories":["test"],"body":{"raw":"b﻿arev dzez\n\n<div align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/13848158/154338760-edfe1885-06f3-4e02-87fe-4b13a403516b.png\"/>\n  <h3>An easy-to-use & supercharged open-source experiment tracker</h3>\n  Aim logs your training runs, enables a beautiful UI to compare them and an API to query them programmatically.\n</div>\n\n<br/>\n\n<img src=\"https://user-images.githubusercontent.com/13848158/154338753-34484cda-95b8-4da8-a610-7fdf198c05fd.png\"/>\n\n<p align=\"center\">\n  <a href=\"#about-aim\"><b>About</b></a> &bull;\n  <a href=\"#why-use-aim\"><b>Features</b></a> &bull;\n  <a href=\"#demos\"><b>Demos</b></a> &bull;\n  <a href=\"https://github.com/aimhubio/aim/tree/main/examples\"><b>Examples</b></a> &bull;\n  <a href=\"#quick-start\"><b>Quick Start</b></a> &bull;\n  <a href=\"https://aimstack.readthedocs.io/en/latest/\"><b>Documentation</b></a> &bull;\n  <a href=\"#roadmap\"><b>Roadmap</b></a> &bull;\n  <a href=\"https://community.aimstack.io/\"><b>Discord Community</b></a> &bull;\n  <a href=\"https://twitter.com/aimstackio\"><b>Twitter</b></a>\n</p>\n\n<div align=\"center\">\n  \n  \\[![Platform Support](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue)]()\n  \\[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aim)](https://pypi.org/project/aim/)\n  \\[![PyPI Package](https://img.shields.io/pypi/v/aim?color=yellow)](https://pypi.org/project/aim/)\n  \\[![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)\n  \\[![PyPI Downloads](https://img.shields.io/pypi/dw/aim?color=green)](https://pypi.org/project/aim/)\n  \\[![Issues](https://img.shields.io/github/issues/aimhubio/aim)](http://github.com/aimhubio/aim/issues)\n  \n</div>\n\n<div align=\"center\">\n  <sub>Integrates seamlessly with your favorite tools</sub>\n  <br/>\n  <br/>\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354389-d0301620-77ea-4629-a743-f7aa249e14b5.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354496-b39d7b1c-63ef-40f0-9e59-c08d2c5e337c.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354380-3755c741-6960-42ca-b93e-84a8791f088c.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354342-7df0ef5e-63d2-4df7-b9f1-d2fc0e95f53f.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354392-afbff3de-c845-4d86-855d-53df569f91d1.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354355-89210506-e7e5-4d37-b2d6-ad3fda62ef13.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354397-8af8e1d3-4067-405e-9d42-1f131663ed22.png\" width=\"60\" />\n  <br/>\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354513-f7486146-3891-4f3f-934f-e58bbf9ce695.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354500-c0471ce6-b2ce-4172-b9e4-07a197256303.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354361-9f911785-008d-4b75-877e-651e026cf47e.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354373-1879ae61-b5d1-41f0-a4f1-04b639b6f05e.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354483-75d9853f-7154-4d95-8190-9ad7a73d6654.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354329-cf7c3352-a72a-478d-82a7-04e3833b03b7.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354349-dcdf3bc3-d7a9-4f34-8258-4824a57f59c7.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354471-518f1814-7a41-4b23-9caf-e516507343f1.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/48801049/165162736-2cc5da39-38aa-4093-874f-e56d0ba9cea2.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/48801049/165074282-36ad18eb-1124-434d-8439-728c22cd7ac7.png\" width=\"60\" />\n</div>\n\n<div align=\"center\">\n  <br/>\n  <kbd>\n    <img width=\"650px\" src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" />\n  </kbd>\n</div>\n\n# About Aim\n\n| Track and version ML runs                                                                                                        | Visualize runs via beautiful UI                                                                                                  | Query runs metadata via SDK                                                                                                      |\n| -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n| <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337794-e9310239-6614-41b3-a95b-bb91f0bb6c4f.png\"/> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337788-03fe5b31-0fa3-44af-ae79-2861707d8602.png\"/> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337793-85175c78-5659-4dd0-bb2d-05017278e2fa.png\"/> |\n\nAim is an open-source, self-hosted ML experiment tracking tool. \nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\n\nYou can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically. \nThat's especially useful for automations and additional analysis on a Jupyter Notebook.\n\nAim's mission is to democratize AI dev tools.\n\n# Why use Aim?\n\n### Compare 100s of runs in a few clicks - build models faster\n\n* Compare, group and aggregate 100s of metrics thanks to effective visualizations.\n* Analyze, learn correlations and patterns between hparams and metrics.\n* Easy pythonic search to query the runs you want to explore.\n\n### Deep dive into details of each run for easy debugging\n\n* Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.\n* Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.\n* Analyze system resource usage to effectively utilize computational resources.\n\n### Have all relevant information organised and accessible for easy governance\n\n* Centralized dashboard to holistically view all your runs, their hparams and results.\n* Use SDK to query/access all your runs and tracked metadata.\n* You own your data - Aim is open source and self hosted.\n\n# Demos\n\n| Machine translation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | lightweight-GAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <a href=\"http://play.aimstack.io:10001/metrics?grouping=HQGdK9Xxy35e6sY1CYkCmk1WbWMN2AsCNfJJ3d1RJYLtrVPMoF5UpGiA6CF8bEJnfzRsKpqespf3AEuKSVrhUYvYk9MxzNGA9XZWYUf6phEg8AMbZGLRVDXnAPDuo8tueqsST1ZLizWzQwDYJWHUza6pyB2Eojt9uWqNHUdb858TqDRnCJzqiVJXKXEzFWUyvU8MckJo1qpqWWCTb4GpYN6DUJZx2GXDGR21e2xxd4m7PmNUnbA9B3apLttZoipJF6c3v7tNUKmb6irpqnNB3yc57tqYDa1XZuKfDxkMtyFdQ1x95K4jjsTVwhftEWLze35QNcxNXRCGGS9o9yEfTLG26GUX2zjPZFCjjMGU6vV7z1xRccK8MyoGrLSgAQCbvk68dTGBHpXUBvCRq8N&chart=FviZzVrt4fVQPjpCLr9sVGGrcR5etSroyqambiKpm3nTgpyv4eQxKuwNX9uN8UtKmzYUhUyTMBEANHmtbwjLApkvnYeNbxGNC6PVcoqi65m1XJnSrvgt8WiD89BapFAWRUwAGx6SWD7KZPsk3RQyysU7W7FjD3Q99NusxFGhsEfD6HXc7i8xH9KHDRGjLwh6x9VTtSp4FS8HEvpLSiiJoX7LCTi8pB7dXvrQ8G5w3jPsFz4qXYFdsVaCNL1BpFFZuiqQNkfbnM84gEq7UmiV1VzM4oS3AgQHxADG3kpBVp6eKTey9F1Swd4FcUkFA9QEPjgQgqwRGjkquZ2bdDDVLBnCh7JPvboP2kifCiZZ5MDdV9MMx6PKHp4DusWyWLXiHQYPkpGPWBiuccMUXDsuJaCWJbuABdY7CyiJMv1jdHYkjabygSxehPVyEDefWAtjBfv2vaeM1xv63jadbmpKYFxft7qmuT9HvVxiGvRgs4RQFxy8K4rtFBca3HNs1mDaaY81gy9MGXyw7BS5Fniu92jaJpsWDdg6Y3AQBLZtrpJy2obEZ4yzJaCVT7JUNPAyyCUNLck393VFLoEkaD9CU5npK5R7tj1c1G3gkMNQXnSXy5NpSj8deMmXV5qz3JKu1nq2caGQKcqjzy2gLkExdm674AMFjSg9yFjK6VqASXQ17NKtWRUvaYoxGbHDAFQaMKWKh8QLm22QA9mKT8NksLptWozbgDvafnQLNMvezLU5bvKV5o75PAWYiRB56RcYfEhzaB6YWdgL7TJicyY5rFi6Az8UZ7wqB3N5iMuZdpxhKn5KbZDxyuUMuvVt24i5LVPPmmwQtqxMoJ4aLo48a2YvDW6TAkdQjNjvn6KcEEz6GTixujb1YHhMUD8v4AepWKEwKz1ddEca1P2wLQjbpihCuaqbxeohnuZZLogJdUBojBEDgrnrrVpPBaLLEkGSpkJbtrsKUuEeBo1AF3yNgHftLbynGpobVF5DhmsmddmiA6c8vSTokJxHhjpnW8mAcNHBRtmVJCT7VkdHSAhNypM4Hivwfx5jCccG9LauKmCeRMDzHiA57TX9W6ttcPHSvUyQorARQAd2oeNY4H83hZjHh9Bt8iwKZRt4xK6hrTR8tif7hq8eURXrGH9Ys7TzykXK8FHHWvLNzNnYf3E4a9NkD43MjfKvMM1hj4Q2K8MHbmRCqrmFrHP5kim9shq6mhLPTgwha32nvnrBkfPQVPwpGTzKuwE&select=CdsQ7jVNkogQhRzQR3e28Ek39AZ4Ma2y37k5zJaf9EZmQhMjy8GtGm4LGU6dRFuAVG7mYww5xDrQAE74KHQ3Kk1e6661RmcmNALAUjtHyCmrTVBMCnBGNiuq1y7EzmxoodYHU1BV1rnoefQAw2kTBtbWi11hV1P4LcwFCcXfUWF6rpRC7ehEnUCTqUV4bkGVJPLcmk9mdmiGwa2YgmnSShNGPVGZiEi1rMVECyngSRVdqdZwAeXBGWFLfqF1KbZeCo4MTF4SSmFupJ9zLhYbuojEbopyFWHQ6xs3sq9epPeaQziLM4Js7oFYRmuFWUYdFqnZngmewXWmi7tQAgVqhiT6dMjG2eTdfgX6WuRSuoHALkh2XJhHA6GfZLUcxC5Ni9YyKuBTamtaYarbNNJJ8z15WWvuUkLpjgHdEpE2h924xFdu8aoZNuiQxYGvcndaW1BTGMXS5fTKPqYfe2n8Ky2HWPkcX3hEXtyawu1F9BndKNaXLPgsdAoFBArBZnSe28YtSmTa5LRucKVBAxakvv5MWMXchAmpaGFQbZyYUoMgQLcJd7Y96x6zSR7nhwr5Ar81BrmqYz2WFLuk7osUbwsc9HbSG6CQt8p6Vg2u7DjKaZXW8pjkPHAKrHWtHEDiJPJ5rj6VsdFm3\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340796-c9e91b13-8ee0-4a67-bcde-8cf3aaa7ba99.jpg\"/> </a> | <a href=\"http://play.aimstack.io:10002/images?grouping=E1zQzcmtDR3wibEa1MVysTvCyZEv1T8ixkCxTWExCyMnHtX2HyiF9eszvPgfd2xdJ5TUTKGpSs1bsLVq5tHAV3uWtsZmmckn6HjNtVCMyQDJpwhiEy5tAyw&select=2NEXuD7fFoaLcwRjymjA1wLmUrGs9s3AiXcCW82C367SwJt18CAB6xzkMGowrUDuDwggE1huaPVcQJpQUsmAQx1CnGiqCUBp2jPMd5mMNPX2QKQMcmvu9ZykBNkeBvCQFPd9ERuQD2g1EjWuvyJ3H53mAZTfp94LCXvR9CUsG5ei2CjQUzfZLM6DCyUr1GPaEVnY5f1EwzicNxXuoutkBgqCqaobJ7Do4q4eHAA6ooiWU6ekS3D2sLj6qYwhVTjfGCPfbWwBiH83nFkY3fLExzdeTY2zeUHeeYikQR9S7xHbVD8WvjekdQVp8X4dNLJZxiVmEqHpPRnU3ZrYsMhE7yFAAgjJwPNUzLTt6YFrtZBcmc4rwAC2oyrqysUSEr6gzL6LcJ6yuqDGf9D5tzftHbTLDkhc8B2sCgTS&images=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\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340790-bc7b7a21-e8a1-43a1-809d-4060b5bfb60f.jpg\"> </a> |\n| Training logs of a neural translation model(from WMT'19 competition).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Training logs of 'lightweight' GAN, proposed in ICLR 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n\n| FastSpeech 2                                                                                                                                                                                                        | Simple MNIST                                                                                                                                                                                                               |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/audios\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340778-dbe19620-2f27-4298-b0cb-caf3904760f1.jpg\"/> </a> | <a href=\"http://play.aimstack.io:10003/runs/7f083da898624a2c98e0f363/distributions\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340785-a7e4d9fd-d048-4207-8cd1-c4edff9cca6a.jpg\"/> </a> |\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\".                                                                                                                       | Simple MNIST training logs.                                                                                                                                                                                                |\n\n# Quick Start\n\nFollow the steps below to get started with Aim.\n\n**1. Install Aim on your training environment**\n\n```shell\npip3 install aim\n```\n\n**2. Integrate Aim with your code**\n\n```python\nfrom aim import Run\n\n# Initialize a new run\nrun = Run()\n\n# Log run parameters\nrun[\"hparams\"] = {\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n}\n\n# Log metrics\nfor i in range(10):\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n```\n\n*See the full list of supported trackable objects(e.g. images, text, etc) [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html).*\n\n**3. Run the training as usual and start Aim UI**\n\n```shell\naim up\n```\n\n**4. Or query runs programmatically via SDK**\n\n```python\nfrom aim import Repo\n\nmy_repo = Repo('/path/to/aim/repo')\n\nquery = \"metric.name == 'loss'\" # Example query\n\n# Get collection of metrics\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\n    for metric in run_metrics_collection:\n        # Get run params\n        params = metric.run[...]\n        # Get metric values\n        steps, metric_values = metric.values.sparse_numpy()\n```\n\n# Integrations\n\n<details>\n<summary>\n  Integrate PyTorch Lightning\n</summary>\n\n```python\nfrom aim.pytorch_lightning import AimLogger\n\n# ...\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning).*\n\n</details>\n\n<details>\n<summary>\n  Integrate Hugging Face\n</summary>\n\n```python\nfrom aim.hugging_face import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    callbacks=[aim_callback],\n    # ...\n)\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face).*\n\n</details>\n\n<details>\n<summary>\n  Integrate Keras & tf.keras\n</summary>\n\n```python\nimport aim\n\n# ...\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n    \n    # Use aim.tensorflow.AimCallback in case of tf.keras\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n])\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras).*\n\n</details>\n\n<details>\n<summary>\n  Integrate KerasTuner\n</summary>\n\n```python\nfrom aim.keras_tuner import AimCallback\n\n# ...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner).*\n\n</details>\n\n<details>\n<summary>\n  Integrate XGBoost\n</summary>\n\n```python\nfrom aim.xgboost import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost).*\n\n</details>\n\n<details>\n<summary>\n  Integrate CatBoost\n</summary>\n\n```python\nfrom aim.catboost import AimLogger\n\n# ...\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost).*\n\n</details>\n\n<details>\n<summary>\n  Integrate fastai\n</summary>\n\n```python\nfrom aim.fastai import AimCallback\n\n# ...\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai).*\n\n</details>\n\n<details>\n<summary>\n  Integrate LightGBM\n</summary>\n\n```python\nfrom aim.lightgbm import AimCallback\n\n# ...\naim_callback = AimCallback(experiment='lgb_test')\naim_callback.experiment['hparams'] = params\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm).*\n\n</details>\n\n<details>\n<summary>\n  Integrate PyTorch Ignite\n</summary>\n\n```python\nfrom aim.pytorch_ignite import AimLogger\n\n# ...\naim_logger = AimLogger()\n\naim_logger.log_params({\n    \"model\": model.__class__.__name__,\n    \"pytorch_version\": str(torch.__version__),\n    \"ignite_version\": str(ignite.__version__),\n})\n\naim_logger.attach_output_handler(\n    trainer,\n    event_name=Events.ITERATION_COMPLETED,\n    tag=\"train\",\n    output_transform=lambda loss: {'loss': loss}\n)\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite).*\n\n</details>\n\n# Comparisons to familiar tools\n\n### Tensorboard\n\n**Training run comparison**\n\nOrder of magnitude faster training run comparison with Aim\n\n* The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.\n* With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. **TensorBoard doesn't have features to group, aggregate the metrics**\n\n**Scalability**\n\n* Aim is built to handle 1000s of training runs - both on the backend and on the UI.\n* TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\n\n**Beloved TB visualizations to be added on Aim**\n\n* Embedding projector.\n* Neural network visualization.\n\n### MLFlow\n\nMLFlow is an end-to-end ML Lifecycle tool.\nAim is focused on training tracking.\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.\n\n**Run comparison**\n\n* Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.\n* MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.\n\n**UI Scalability**\n\n* Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\n* MLflow UI becomes slow to use when there are a few hundreds of runs.\n\n### Weights and Biases\n\nHosted vs self-hosted\n\n* Weights and Biases is a hosted closed-source MLOps platform.\n* Aim is self-hosted, free and open-source experiment tracking tool.\n\n# Roadmap\n\n## Detailed Sprints\n\n:sparkle: The [Aim product roadmap](https://github.com/orgs/aimhubio/projects/3)\n\n* The `Backlog` contains the issues we are going to choose from and prioritize weekly\n* The issues are mainly prioritized by the highly-requested features\n\n## High-level roadmap\n\nThe high-level features we are going to work on the next few months\n\n### Done\n\n* Live updates (Shipped: *Oct 18 2021*)\n* Images tracking and visualization (Start: *Oct 18 2021*, Shipped: *Nov 19 2021*)\n* Distributions tracking and visualization (Start: *Nov 10 2021*, Shipped: *Dec 3 2021*)\n* Jupyter integration (Start: *Nov 18 2021*, Shipped: *Dec 3 2021*)\n* Audio tracking and visualization (Start: *Dec 6 2021*, Shipped: *Dec 17 2021*)\n* Transcripts tracking and visualization (Start: *Dec 6 2021*, Shipped: *Dec 17 2021*)\n* Plotly integration (Start: *Dec 1 2021*, Shipped: *Dec 17 2021*)\n* Colab integration (Start: *Nov 18 2021*, Shipped: *Dec 17 2021*)\n* Centralized tracking server (Start: *Oct 18 2021*, Shipped: *Jan 22 2022*)\n* Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: *Dec 17 2021*, Shipped: *Feb 3 2022*)\n* Track git info, env vars, CLI arguments, dependencies (Start: *Jan 17 2022*, Shipped: *Feb 3 2022*)\n* MLFlow adaptor (visualize MLflow logs with Aim) (Start: *Feb 14 2022*, Shipped: *Feb 22 2022*)\n* Activeloop Hub integration (Start: *Feb 14 2022*, Shipped: *Feb 22 2022*)\n* PyTorch-Ignite integration (Start: *Feb 14 2022*, Shipped: *Feb 22 2022*)\n* Run summary and overview info(system params, CLI args, git info, ...) (Start: *Feb 14 2022*, Shipped: *Mar 9 2022*)\n* Add DVC related metadata into aim run (Start: *Mar 7 2022*, Shipped: *Mar 26 2022*)\n* Ability to attach notes to Run from UI (Start: *Mar 7 2022*, Shipped: *Apr 29 2022*)\n* Fairseq integration (Start: *Mar 27 2022*, Shipped: *Mar 29 2022*)\n* LightGBM integration (Start: *Apr 14 2022*, Shipped: *May 17 2022*)\n* CatBoost integration (Start: *Apr 20 2022*, Shipped: *May 17 2022*)\n* Run execution details(display stdout/stderr logs) (Start: *Apr 25 2022*, Shipped: *May 17 2022*)\n* Long sequences(up to 5M of steps) support (Start: *Apr 25 2022*, Shipped: *Jun 22 2022*)\n* Figures Explorer (Start: *Mar 1 2022*, Shipped: *Aug 21 2022*)\n* Notify on stuck runs (Start: *Jul 22 2022*, Shipped: *Aug 21 2022*)\n* Integration with KerasTuner (Start: *Aug 10 2022*, Shipped: *Aug 21 2022*)\n* Integration with WandB (Start: *Aug 15 2022*, Shipped: *Aug 21 2022*)\n* Stable remote tracking server (Start: *Jun 15 2022*, Shipped: *Aug 21 2022*)\n* Integration with fast.ai (Start: *Aug 22 2022*, Shipped: *Oct 6 2022*)\n* Integration with MXNet (Start: *Sep 20 2022*, Shipped: *Oct 6 2022*)\n* Project overview page (Start: *Sep 1 2022*, Shipped: *Oct 6 2022*)\n* Remote tracking server scaling (Start: *Sep 11 2022*, Shipped: *Nov 26 2022*)\n* Integration with PaddlePaddle (Start: *Oct 2 2022*, Shipped: *Nov 26 2022*)\n* Integration with Optuna (Start: *Oct 2 2022*, Shipped: *Nov 26 2022*)\n* Audios Explorer (Start: *Oct 30 2022*, Shipped: *Nov 26 2022*)\n* Experiment page (Start: *Nov 9 2022*, Shipped: *Nov 26 2022*)\n\n### In Progress\n\n* Aim SDK low-level interface (Start: *Aug 22 2022*, )\n* HuggingFace datasets (Start: *Dec 29 2022*, )\n\n### To Do\n\n**Aim UI**\n\n* Runs management\n\n  * Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard\n* Explorers\n\n  * Text Explorer\n  * Distributions Explorer\n* Dashboards – customizable layouts with embedded explorers\n\n**SDK and Storage**\n\n* Scalability\n\n  * Smooth UI and SDK experience with over 10.000 runs\n* Runs management\n\n  * CLI interfaces\n\n    * Reporting - runs summary and run details in a CLI compatible format\n    * Manipulations – copy, move, delete runs, params and sequences\n\n**Integrations**\n\n* ML Frameworks:\n\n  * Shortlist: MONAI, SpaCy, Raytune\n* Resource management tools\n\n  * Shortlist: Kubeflow, Slurm\n* Workflow orchestration tools\n* Others: Hydra, Google MLMD, Streamlit, ...\n\n### On hold\n\n* scikit-learn integration\n* Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: *Mar 21 2022*)\n* Artifact storage – store files, model checkpoints, and beyond (Start: *Mar 21 2022*)\n\n## Community\n\n### If you have questions\n\n1. [Read the docs](https://aimstack.readthedocs.io/en/latest/)\n2. [Open a feature request or report a bug](https://github.com/aimhubio/aim/issues)\n3. [Join Discord community server](https://community.aimstack.io/)\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_26BhViw28s\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n","html":"<p>b﻿arev dzez</p>\n<p>[<img src=\"https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue\" alt=\"Platform Support\">]()\n[<img src=\"https://img.shields.io/pypi/pyversions/aim\" alt=\"PyPI - Python Version\">](https://pypi.org/project/aim/)\n[<img src=\"https://img.shields.io/pypi/v/aim?color=yellow\" alt=\"PyPI Package\">](https://pypi.org/project/aim/)\n[<img src=\"https://img.shields.io/badge/License-Apache%202.0-orange.svg\" alt=\"License\">](https://opensource.org/licenses/Apache-2.0)\n[<img src=\"https://img.shields.io/pypi/dw/aim?color=green\" alt=\"PyPI Downloads\">](https://pypi.org/project/aim/)\n[<img src=\"https://img.shields.io/github/issues/aimhubio/aim\" alt=\"Issues\">](http://github.com/aimhubio/aim/issues)</p>\n<h1>About Aim</h1>\n<p>| Track and version ML runs                                                                                                        | Visualize runs via beautiful UI                                                                                                  | Query runs metadata via SDK                                                                                                      |\n| -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n|  |  |  |</p>\n<p>Aim is an open-source, self-hosted ML experiment tracking tool.\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.</p>\n<p>You can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically.\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.</p>\n<p>Aim's mission is to democratize AI dev tools.</p>\n<h1>Why use Aim?</h1>\n<h3>Compare 100s of runs in a few clicks - build models faster</h3>\n<ul>\n<li>Compare, group and aggregate 100s of metrics thanks to effective visualizations.</li>\n<li>Analyze, learn correlations and patterns between hparams and metrics.</li>\n<li>Easy pythonic search to query the runs you want to explore.</li>\n</ul>\n<h3>Deep dive into details of each run for easy debugging</h3>\n<ul>\n<li>Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.</li>\n<li>Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.</li>\n<li>Analyze system resource usage to effectively utilize computational resources.</li>\n</ul>\n<h3>Have all relevant information organised and accessible for easy governance</h3>\n<ul>\n<li>Centralized dashboard to holistically view all your runs, their hparams and results.</li>\n<li>Use SDK to query/access all your runs and tracked metadata.</li>\n<li>You own your data - Aim is open source and self hosted.</li>\n</ul>\n<h1>Demos</h1>\n<p>| Machine translation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | lightweight-GAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|    |    |\n| Training logs of a neural translation model(from WMT'19 competition).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Training logs of 'lightweight' GAN, proposed in ICLR 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |</p>\n<p>| FastSpeech 2                                                                                                                                                                                                        | Simple MNIST                                                                                                                                                                                                               |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|    |    |\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\".                                                                                                                       | Simple MNIST training logs.                                                                                                                                                                                                |</p>\n<h1>Quick Start</h1>\n<p>Follow the steps below to get started with Aim.</p>\n<p><strong>1. Install Aim on your training environment</strong></p>\n<pre><code class=\"language-shell\">pip3 install aim\n</code></pre>\n<p><strong>2. Integrate Aim with your code</strong></p>\n<pre><code class=\"language-python\">from aim import Run\n\n# Initialize a new run\nrun = Run()\n\n# Log run parameters\nrun[\"hparams\"] = {\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n}\n\n# Log metrics\nfor i in range(10):\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n</code></pre>\n<p><em>See the full list of supported trackable objects(e.g. images, text, etc) <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html\">here</a>.</em></p>\n<p><strong>3. Run the training as usual and start Aim UI</strong></p>\n<pre><code class=\"language-shell\">aim up\n</code></pre>\n<p><strong>4. Or query runs programmatically via SDK</strong></p>\n<pre><code class=\"language-python\">from aim import Repo\n\nmy_repo = Repo('/path/to/aim/repo')\n\nquery = \"metric.name == 'loss'\" # Example query\n\n# Get collection of metrics\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\n    for metric in run_metrics_collection:\n        # Get run params\n        params = metric.run[...]\n        # Get metric values\n        steps, metric_values = metric.values.sparse_numpy()\n</code></pre>\n<h1>Integrations</h1>\n<pre><code class=\"language-python\">from aim.pytorch_lightning import AimLogger\n\n# ...\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.hugging_face import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    callbacks=[aim_callback],\n    # ...\n)\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face\">here</a>.</em></p>\n<pre><code class=\"language-python\">import aim\n\n# ...\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n    \n    # Use aim.tensorflow.AimCallback in case of tf.keras\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n])\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.keras_tuner import AimCallback\n\n# ...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.xgboost import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.catboost import AimLogger\n\n# ...\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.fastai import AimCallback\n\n# ...\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.lightgbm import AimCallback\n\n# ...\naim_callback = AimCallback(experiment='lgb_test')\naim_callback.experiment['hparams'] = params\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.pytorch_ignite import AimLogger\n\n# ...\naim_logger = AimLogger()\n\naim_logger.log_params({\n    \"model\": model.__class__.__name__,\n    \"pytorch_version\": str(torch.__version__),\n    \"ignite_version\": str(ignite.__version__),\n})\n\naim_logger.attach_output_handler(\n    trainer,\n    event_name=Events.ITERATION_COMPLETED,\n    tag=\"train\",\n    output_transform=lambda loss: {'loss': loss}\n)\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite\">here</a>.</em></p>\n<h1>Comparisons to familiar tools</h1>\n<h3>Tensorboard</h3>\n<p><strong>Training run comparison</strong></p>\n<p>Order of magnitude faster training run comparison with Aim</p>\n<ul>\n<li>The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.</li>\n<li>With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. <strong>TensorBoard doesn't have features to group, aggregate the metrics</strong></li>\n</ul>\n<p><strong>Scalability</strong></p>\n<ul>\n<li>Aim is built to handle 1000s of training runs - both on the backend and on the UI.</li>\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\n</ul>\n<p><strong>Beloved TB visualizations to be added on Aim</strong></p>\n<ul>\n<li>Embedding projector.</li>\n<li>Neural network visualization.</li>\n</ul>\n<h3>MLFlow</h3>\n<p>MLFlow is an end-to-end ML Lifecycle tool.\nAim is focused on training tracking.\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\n<p><strong>Run comparison</strong></p>\n<ul>\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.</li>\n<li>MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.</li>\n</ul>\n<p><strong>UI Scalability</strong></p>\n<ul>\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\n</ul>\n<h3>Weights and Biases</h3>\n<p>Hosted vs self-hosted</p>\n<ul>\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\n</ul>\n<h1>Roadmap</h1>\n<h2>Detailed Sprints</h2>\n<p>:sparkle: The <a href=\"https://github.com/orgs/aimhubio/projects/3\">Aim product roadmap</a></p>\n<ul>\n<li>The <code>Backlog</code> contains the issues we are going to choose from and prioritize weekly</li>\n<li>The issues are mainly prioritized by the highly-requested features</li>\n</ul>\n<h2>High-level roadmap</h2>\n<p>The high-level features we are going to work on the next few months</p>\n<h3>Done</h3>\n<ul>\n<li>Live updates (Shipped: <em>Oct 18 2021</em>)</li>\n<li>Images tracking and visualization (Start: <em>Oct 18 2021</em>, Shipped: <em>Nov 19 2021</em>)</li>\n<li>Distributions tracking and visualization (Start: <em>Nov 10 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>Jupyter integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>Audio tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Transcripts tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Plotly integration (Start: <em>Dec 1 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Colab integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Centralized tracking server (Start: <em>Oct 18 2021</em>, Shipped: <em>Jan 22 2022</em>)</li>\n<li>Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: <em>Dec 17 2021</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>Track git info, env vars, CLI arguments, dependencies (Start: <em>Jan 17 2022</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>MLFlow adaptor (visualize MLflow logs with Aim) (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>Activeloop Hub integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>PyTorch-Ignite integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>Run summary and overview info(system params, CLI args, git info, ...) (Start: <em>Feb 14 2022</em>, Shipped: <em>Mar 9 2022</em>)</li>\n<li>Add DVC related metadata into aim run (Start: <em>Mar 7 2022</em>, Shipped: <em>Mar 26 2022</em>)</li>\n<li>Ability to attach notes to Run from UI (Start: <em>Mar 7 2022</em>, Shipped: <em>Apr 29 2022</em>)</li>\n<li>Fairseq integration (Start: <em>Mar 27 2022</em>, Shipped: <em>Mar 29 2022</em>)</li>\n<li>LightGBM integration (Start: <em>Apr 14 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>CatBoost integration (Start: <em>Apr 20 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>Run execution details(display stdout/stderr logs) (Start: <em>Apr 25 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>Long sequences(up to 5M of steps) support (Start: <em>Apr 25 2022</em>, Shipped: <em>Jun 22 2022</em>)</li>\n<li>Figures Explorer (Start: <em>Mar 1 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Notify on stuck runs (Start: <em>Jul 22 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Integration with KerasTuner (Start: <em>Aug 10 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Integration with WandB (Start: <em>Aug 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Stable remote tracking server (Start: <em>Jun 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Integration with fast.ai (Start: <em>Aug 22 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>Integration with MXNet (Start: <em>Sep 20 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>Project overview page (Start: <em>Sep 1 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>Remote tracking server scaling (Start: <em>Sep 11 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Integration with PaddlePaddle (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Integration with Optuna (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Audios Explorer (Start: <em>Oct 30 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Experiment page (Start: <em>Nov 9 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n</ul>\n<h3>In Progress</h3>\n<ul>\n<li>Aim SDK low-level interface (Start: <em>Aug 22 2022</em>, )</li>\n<li>HuggingFace datasets (Start: <em>Dec 29 2022</em>, )</li>\n</ul>\n<h3>To Do</h3>\n<p><strong>Aim UI</strong></p>\n<ul>\n<li>\n<p>Runs management</p>\n<ul>\n<li>Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard</li>\n</ul>\n</li>\n<li>\n<p>Explorers</p>\n<ul>\n<li>Text Explorer</li>\n<li>Distributions Explorer</li>\n</ul>\n</li>\n<li>\n<p>Dashboards – customizable layouts with embedded explorers</p>\n</li>\n</ul>\n<p><strong>SDK and Storage</strong></p>\n<ul>\n<li>\n<p>Scalability</p>\n<ul>\n<li>Smooth UI and SDK experience with over 10.000 runs</li>\n</ul>\n</li>\n<li>\n<p>Runs management</p>\n<ul>\n<li>\n<p>CLI interfaces</p>\n<ul>\n<li>Reporting - runs summary and run details in a CLI compatible format</li>\n<li>Manipulations – copy, move, delete runs, params and sequences</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Integrations</strong></p>\n<ul>\n<li>\n<p>ML Frameworks:</p>\n<ul>\n<li>Shortlist: MONAI, SpaCy, Raytune</li>\n</ul>\n</li>\n<li>\n<p>Resource management tools</p>\n<ul>\n<li>Shortlist: Kubeflow, Slurm</li>\n</ul>\n</li>\n<li>\n<p>Workflow orchestration tools</p>\n</li>\n<li>\n<p>Others: Hydra, Google MLMD, Streamlit, ...</p>\n</li>\n</ul>\n<h3>On hold</h3>\n<ul>\n<li>scikit-learn integration</li>\n<li>Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: <em>Mar 21 2022</em>)</li>\n<li>Artifact storage – store files, model checkpoints, and beyond (Start: <em>Mar 21 2022</em>)</li>\n</ul>\n<h2>Community</h2>\n<h3>If you have questions</h3>\n<ol>\n<li><a href=\"https://aimstack.readthedocs.io/en/latest/\">Read the docs</a></li>\n<li><a href=\"https://github.com/aimhubio/aim/issues\">Open a feature request or report a bug</a></li>\n<li><a href=\"https://community.aimstack.io/\">Join Discord community server</a></li>\n</ol>"},"_id":"posts/post-with-new-markdown.md","_raw":{"sourceFilePath":"posts/post-with-new-markdown.md","sourceFileName":"post-with-new-markdown.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/post-with-new-markdown"},"type":"Post"},{"title":"Putin Assigned command","date":"2022-11-14T13:21:29.829Z","author":"Ashot","description":"Ukrain war","slug":"putin-assigned-command","image":"/images/310358233_2051400925065396_6449727964561625443_n.jpg","draft":false,"tags":["putin"],"categories":["politics"],"body":{"raw":"L﻿orem","html":"<p>L﻿orem</p>"},"_id":"posts/putin-assigned-command.md","_raw":{"sourceFilePath":"posts/putin-assigned-command.md","sourceFileName":"putin-assigned-command.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/putin-assigned-command"},"type":"Post"},{"title":"Test image path","date":"2023-01-14T18:17:57.545Z","author":"Ash","description":"test post","slug":"test-image-path","image":"/images/blog/sum.png","draft":false,"categories":["Test"],"body":{"raw":"<!--StartFragment-->\n\n## Where does it come from?\n\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the <h5> cites</h5> of the word in classical **literature**, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by <Highlight><Flex align='center' css={{ marginTop: '$10' }}>\n  <Icon name='back' size={20} />\n  <Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  </Text>\n</Flex></Highlight>\n\n<!--EndFragment-->\n\n```jsx\n<Flex align='center' css={{ marginTop: '$10' }}>\n  <Icon name='back' size={20} />\n  <Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  </Text>\n</Flex>\n```","html":"<h2>Where does it come from?</h2>\n<p>Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the  cites of the word in classical <strong>literature</strong>, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32</p>\n<p>The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by &#x3C;Flex align='center' css={{ marginTop: '$10' }}>\n\n&#x3C;Text size={3} css={{ fontWeight: '$3' }}>\nGo Back\n\n</p>\n<pre><code class=\"language-jsx\">&#x3C;Flex align='center' css={{ marginTop: '$10' }}>\n  &#x3C;Icon name='back' size={20} />\n  &#x3C;Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  &#x3C;/Text>\n&#x3C;/Flex>\n</code></pre>"},"_id":"posts/test-image-path.md","_raw":{"sourceFilePath":"posts/test-image-path.md","sourceFileName":"test-image-path.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/test-image-path"},"type":"Post"},{"title":"Text Highlighting In HTML 5?","date":"2020-12-28T14:09:38.000Z","author":"Rajdeep Singh","description":"Mark tag Help to High Lighting text in HTML 5","slug":"text-highlighting-in-html-5","id":14,"image":"/images/Text-Highlighting-In-HTML-5.png","draft":false,"tags":["html","html 5","html tutorial","Text Highlighting","text highlighting in html","html for beginner"],"categories":["html","html 5","html tutorial","Text Highlighting","text highlighting in html","html for beginner"],"body":{"raw":"\n\n\nText Highlighting in HTML 5, You Highlight the Text in a Paragraph heading and use it inside another tag. Add some Color Help Of CSS. So You can Use `<mark> ` tag In `<p>` tag very easily.\n\n**Syntax:**\n\n```html\n<mark> </mark>\n\n```\n\nMark tag by default CSS By Chrome Browser.\n\n```css\nmark {\n    background-color: yellow;\n    color: black;\n}\n```\n\n\n\n**Example:**\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Document</title>\n    <style>\n        mark{\n            color:white;\n            background-color: black;\n            padding: 0px 3px;\n        }\n    </style>\n</head>\n<body>\n       <p> we are write  article that contains the <mark> Mark tag </mark> for Highlighting the text. you will make it easier to see it.</p>    \n</body>\n</html>\n```\n\n---\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://medium.com/officialrajdeepsingh](https://medium.com/officialrajdeepsingh)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n\n\n\n---\n\n\n\n","html":"<p>Text Highlighting in HTML 5, You Highlight the Text in a Paragraph heading and use it inside another tag. Add some Color Help Of CSS. So You can Use <code>&#x3C;mark> </code> tag In <code>&#x3C;p></code> tag very easily.</p>\n<p><strong>Syntax:</strong></p>\n<pre><code class=\"language-html\">&#x3C;mark> &#x3C;/mark>\n\n</code></pre>\n<p>Mark tag by default CSS By Chrome Browser.</p>\n<pre><code class=\"language-css\">mark {\n    background-color: yellow;\n    color: black;\n}\n</code></pre>\n<p><strong>Example:</strong></p>\n<pre><code class=\"language-html\">&#x3C;!DOCTYPE html>\n&#x3C;html lang=\"en\">\n&#x3C;head>\n    &#x3C;meta charset=\"UTF-8\">\n    &#x3C;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    &#x3C;title>Document&#x3C;/title>\n    &#x3C;style>\n        mark{\n            color:white;\n            background-color: black;\n            padding: 0px 3px;\n        }\n    &#x3C;/style>\n&#x3C;/head>\n&#x3C;body>\n       &#x3C;p> we are write  article that contains the &#x3C;mark> Mark tag &#x3C;/mark> for Highlighting the text. you will make it easier to see it.&#x3C;/p>    \n&#x3C;/body>\n&#x3C;/html>\n</code></pre>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://medium.com/officialrajdeepsingh\">https://medium.com/officialrajdeepsingh</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>\n<hr>"},"_id":"posts/text-highlighting-in-html-5.md","_raw":{"sourceFilePath":"posts/text-highlighting-in-html-5.md","sourceFileName":"text-highlighting-in-html-5.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/text-highlighting-in-html-5"},"type":"Post"},{"title":"Title tag In HTML 5?","date":"2020-12-25T10:09:58.000Z","author":"Rajdeep Singh","description":"Title tag display page information in the browser toolbar.","slug":"title-tag-in-html-5","id":15,"image":"/images/Title-tag-In-HTML-5.jpg","draft":false,"tags":["html","html 5","html for beginner","html tutorial","title tag","html title tag"],"categories":["html","html 5","html for beginner","html tutorial","title tag","html title tag"],"body":{"raw":"\n\n\nTitle tag Use Display Page Information In Web browser On Top. The title is also Use In the Body tag.Title tag Provided a title for the Page When You Added to Favorites in The Browser. Title Tag also Displays title Information for the Page in Search Engine Results\n\n**Syntax:**\n\n```html\n<title> Heading With Title Tag </title>\n```\n\n\n\nYou Use Title Tag Inside Body By Default Title tag Display None inside Browser.\n\n```css\ntitle {\n  display: none;\n}\n```\n\n\n\nIf you use the title tag shown in the browser, add CSS To Display Title In Browser.\n\n```html\n<title  style=\"display: block\" > Heading With Title Tag </title>\n```\n\n---\n\n### Example:\n\n```html\n\n<!DOCTYPE html>\n<html>\n\t<head>\n\t\t<title> Index Page Information </title>\n\t</head>\n\t<body>\n\t\n\t\t<title  style=\"display: block\" > Heading With Title Tag </title>\n\n\t</body>\n</html>\n\n```\n\n---\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://medium.com/officialrajdeepsingh](https://medium.com/officialrajdeepsingh)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n\n\n\n","html":"<p>Title tag Use Display Page Information In Web browser On Top. The title is also Use In the Body tag.Title tag Provided a title for the Page When You Added to Favorites in The Browser. Title Tag also Displays title Information for the Page in Search Engine Results</p>\n<p><strong>Syntax:</strong></p>\n<pre><code class=\"language-html\">&#x3C;title> Heading With Title Tag &#x3C;/title>\n</code></pre>\n<p>You Use Title Tag Inside Body By Default Title tag Display None inside Browser.</p>\n<pre><code class=\"language-css\">title {\n  display: none;\n}\n</code></pre>\n<p>If you use the title tag shown in the browser, add CSS To Display Title In Browser.</p>\n<pre><code class=\"language-html\">&#x3C;title  style=\"display: block\" > Heading With Title Tag &#x3C;/title>\n</code></pre>\n<hr>\n<h3>Example:</h3>\n<pre><code class=\"language-html\">\n&#x3C;!DOCTYPE html>\n&#x3C;html>\n\t&#x3C;head>\n\t\t&#x3C;title> Index Page Information &#x3C;/title>\n\t&#x3C;/head>\n\t&#x3C;body>\n\t\n\t\t&#x3C;title  style=\"display: block\" > Heading With Title Tag &#x3C;/title>\n\n\t&#x3C;/body>\n&#x3C;/html>\n\n</code></pre>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://medium.com/officialrajdeepsingh\">https://medium.com/officialrajdeepsingh</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>"},"_id":"posts/title-tag-in-html-5.md","_raw":{"sourceFilePath":"posts/title-tag-in-html-5.md","sourceFileName":"title-tag-in-html-5.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/title-tag-in-html-5"},"type":"Post"},{"title":"title using multiple github gitlab account via ssh","date":"2022-08-16T15:36:24.548Z","author":"Man","description":"Description using multiple github gitlab account via ssh","slug":"title-using-multiple-github-gitlab-account-via-ssh","image":"/images/cat2.jpg","draft":false,"tags":["git","ssh"],"categories":["git","ssh"],"body":{"raw":"This is body text","html":"<p>This is body text</p>"},"_id":"posts/title-using-multiple-github-gitlab-account-via-ssh.md","_raw":{"sourceFilePath":"posts/title-using-multiple-github-gitlab-account-via-ssh.md","sourceFileName":"title-using-multiple-github-gitlab-account-via-ssh.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/title-using-multiple-github-gitlab-account-via-ssh"},"type":"Post"},{"title":"What is Next.js?","date":"2020-11-08T13:10:48.000Z","author":"Rajdeep Singh","description":"Basic Introduction About Next.js #SeriesStart","slug":"what-is-next-js","id":16,"image":"/images/next.js.png","draft":false,"tags":["Next.js","Next.js Tutorial","React.js","React.js Framework","javascript Framework","Next","Next.js Framework"],"categories":["Next.js","Next.js Tutorial","React.js","React.js Framework","javascript Framework","Next","Next.js Framework"],"body":{"raw":"\n\n\n****Next.js**** is a ****basically**** JavaScript ****framework**** build with ****react js****, ****webpack****, and ****babel****. Next.js helps the ****developer**** create a ****static generation**** (SSG) and ****server-side rendering**** (SSR) website easily.\n\nZEIT builds Next.js. But ****recently,**** 1 Month ago, ****ZEIT**** purchase by ****[Vercel](https://vercel.com/)****. That ****means**** ZEIT is now ****Vercel****.\n\nhttps://vercel.com/blog/zeit-is-now-vercel\n\nNext.js ****helping**** to ****optimize**** your ****website****. Recently lots of ****developers working**** with ****Next.js.****\n\n## Note:\n\n****Next****.js, ****Nuxt****.js, and ****Nest****.js are ****different**** frameworks || library.\n\n---\n\n# Feature:\n\nNext.js ****Feature**** help to developer ****solve**** the major ****problem**** in ****web development.****\n\n1. ****Pre-Rendering Support SSG And SSR****\n2. ****CSS-in-JS****\n3. ****Zero Configuration****\n4. ****Ready for Production****\n\n\n\n---\n\n# Installation:\n\n****install**** next in your ****machine**** with two ways\n\n1. Manual Setup\n2. Automatically Setup\n\n---\n\n## Manual Setup:\n\n****Install**** `next`, `react` and `react-dom` in your ****project****:\n\n```cmd\nnpm install next react react-dom\n```\n\n****Open**** `package.json` and ****add**** the following `scripts`:\n\n```javascript\n\"scripts\": { \n   \"dev\": \"next\", \n   \"build\": \"next build\",  \n   \"start\": \"next start\"\n  }\n```\n\n****After**** Installation run ****script,**** use this ****Cmd****\n\n```cmd\nnpm run dev\n\n```\n\n****Now**** Visit this ****URL**** in ****your browser**** [https://contentlayer-iota.vercel.app/](http://localhost:3000/). ****your webserver**** Now Work 😃\n\n---\n\n## Automatically Setup:\n\nWe highly ****recommend**** a ****beginner person**** to ****create a new Next.js**** app using this ****command**** `create-next-app` , this ****cmd**** Setup ****everything automatically**** for ****you****.\n\n```cmd \nnpm init next-app\n# or\nyarn create next-app\n```\n\nAfter the ****installation**** is complete.\n\n```cmd\ncd path // make sure you same folder other wish no use\nnpm run dev  // that cmd open create server and run your default browser http://localhost:3000/\n```\n\nStart the ****development**** server. After Try ****editing**** `pages/blog.tsx` and see the ****result**** on your ****browser****.\n\n## Note:\n\n****Index.js**** default next.js ****router**** path.\n\n---\n\n# Next.js Youtube Course:\n\nOn ****youtube**, a few YouTubers provide tutorials on next.js. In 2021 lots of YouTubers show interest in next.js.**\n\n\n\n## 1. Bruno Antunes:\n\n****Bruno Antunes**** provides ****a great**** path for ****Next****.js. That person ****creates**** a lot of ****tutorials**** on it **. Make ****** sure you ****watch**** it.\n\n<iframe width=\"928\" height=\"500\" src=\"https://www.youtube.com/embed/7J4iL1HDshQ?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## Youtube PlayList:\n\n<iframe width=\"928\" height=\"500\" src=\"https://www.youtube.com/embed/7J4iL1HDshQ?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n---\n\n## 2. Imran Sayed — Codeytek Academy:\n\n****Imran Sayed**** is a Great ****developer**** my ****opinion****. Imran stayed to provide a very big contribution in  next.js on youtube. Imran provides you a ****learning**** path in next.js || ****Wordpress API****|| ****apollo**** || ****graphql****.\n\n<iframe width=\"928\" height=\"500\" src=\"https://www.youtube.com/embed/X_VM4MLzCS4?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## Youtube Playlist:\n\n<iframe width=\"928\" height=\"500\" src=\"https://www.youtube.com/embed/X_VM4MLzCS4?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n---\n\n## Note:\n\nTell me about other Youtubers who work with Next.js. Then mention in  ****Comment** section website**:\n\nhttps://nextjs.org/docs/getting-started\n\nhttps://www.geeksforgeeks.org/next-js-introduction/\n\nhttps://www.freecodecamp.org/news/an-introduction-to-next-js-for-everyone-507d2d90ab54/\n\n\n\n---\n\n# Read My Other Blog Medium:\n\nhttps://medium.com/officialrajdeepsingh/best-javascript-full-tutorial-on-youtube-117e08ab357a\n\nhttps://medium.com/officialrajdeepsingh/how-to-read-local-json-file-in-react-js-564125235fc7\n\n---\n\n## Conclusion:\n\nMy ****Opinion**** next.js. Great ****Library**** To help the ****developer**** create ****server-side rendering**** (SSR) with one ****function**** || ****method**** in next.js. My big ****surprise When**** I start work with ****Wordpress**** and ****graphql. Next.js**** Provide More ****stability as compare to**** other ****React.js**** frameworks.\n\n****Now**** My first ****choice is Nextjs**** in ****frontend****-****developer.****\n\n---\n\n# Contact me\n\n* [https://www.facebook.com/officialrajdeepsingh/](https://www.facebook.com/officialrajdeepsingh/)\n* [https://www.facebook.com/groups/JavaScriptcode/](https://www.facebook.com/groups/JavaScriptcode/)\n* [https://www.facebook.com/groups/pythoncodejoin/](https://www.facebook.com/groups/pythoncodejoin/)\n* [officialrajdeepsingh@gmail.com](mailto:officialrajdeepsingh@gmail.com)\n\n# Thanks for Reading 😆\n","html":"<p><strong><strong>Next.js</strong></strong> is a <strong><strong>basically</strong></strong> JavaScript <strong><strong>framework</strong></strong> build with <strong><strong>react js</strong></strong>, <strong><strong>webpack</strong></strong>, and <strong><strong>babel</strong></strong>. Next.js helps the <strong><strong>developer</strong></strong> create a <strong><strong>static generation</strong></strong> (SSG) and <strong><strong>server-side rendering</strong></strong> (SSR) website easily.</p>\n<p>ZEIT builds Next.js. But <strong><strong>recently,</strong></strong> 1 Month ago, <strong><strong>ZEIT</strong></strong> purchase by <strong><strong><a href=\"https://vercel.com/\">Vercel</a></strong></strong>. That <strong><strong>means</strong></strong> ZEIT is now <strong><strong>Vercel</strong></strong>.</p>\n<p>https://vercel.com/blog/zeit-is-now-vercel</p>\n<p>Next.js <strong><strong>helping</strong></strong> to <strong><strong>optimize</strong></strong> your <strong><strong>website</strong></strong>. Recently lots of <strong><strong>developers working</strong></strong> with <strong><strong>Next.js.</strong></strong></p>\n<h2>Note:</h2>\n<p><strong><strong>Next</strong></strong>.js, <strong><strong>Nuxt</strong></strong>.js, and <strong><strong>Nest</strong></strong>.js are <strong><strong>different</strong></strong> frameworks || library.</p>\n<hr>\n<h1>Feature:</h1>\n<p>Next.js <strong><strong>Feature</strong></strong> help to developer <strong><strong>solve</strong></strong> the major <strong><strong>problem</strong></strong> in <strong><strong>web development.</strong></strong></p>\n<ol>\n<li><strong><strong>Pre-Rendering Support SSG And SSR</strong></strong></li>\n<li><strong><strong>CSS-in-JS</strong></strong></li>\n<li><strong><strong>Zero Configuration</strong></strong></li>\n<li><strong><strong>Ready for Production</strong></strong></li>\n</ol>\n<hr>\n<h1>Installation:</h1>\n<p><strong><strong>install</strong></strong> next in your <strong><strong>machine</strong></strong> with two ways</p>\n<ol>\n<li>Manual Setup</li>\n<li>Automatically Setup</li>\n</ol>\n<hr>\n<h2>Manual Setup:</h2>\n<p><strong><strong>Install</strong></strong> <code>next</code>, <code>react</code> and <code>react-dom</code> in your <strong><strong>project</strong></strong>:</p>\n<pre><code class=\"language-cmd\">npm install next react react-dom\n</code></pre>\n<p><strong><strong>Open</strong></strong> <code>package.json</code> and <strong><strong>add</strong></strong> the following <code>scripts</code>:</p>\n<pre><code class=\"language-javascript\">\"scripts\": { \n   \"dev\": \"next\", \n   \"build\": \"next build\",  \n   \"start\": \"next start\"\n  }\n</code></pre>\n<p><strong><strong>After</strong></strong> Installation run <strong><strong>script,</strong></strong> use this <strong><strong>Cmd</strong></strong></p>\n<pre><code class=\"language-cmd\">npm run dev\n\n</code></pre>\n<p><strong><strong>Now</strong></strong> Visit this <strong><strong>URL</strong></strong> in <strong><strong>your browser</strong></strong> <a href=\"http://localhost:3000/\">https://contentlayer-iota.vercel.app/</a>. <strong><strong>your webserver</strong></strong> Now Work 😃</p>\n<hr>\n<h2>Automatically Setup:</h2>\n<p>We highly <strong><strong>recommend</strong></strong> a <strong><strong>beginner person</strong></strong> to <strong><strong>create a new Next.js</strong></strong> app using this <strong><strong>command</strong></strong> <code>create-next-app</code> , this <strong><strong>cmd</strong></strong> Setup <strong><strong>everything automatically</strong></strong> for <strong><strong>you</strong></strong>.</p>\n<pre><code class=\"language-cmd\">npm init next-app\n# or\nyarn create next-app\n</code></pre>\n<p>After the <strong><strong>installation</strong></strong> is complete.</p>\n<pre><code class=\"language-cmd\">cd path // make sure you same folder other wish no use\nnpm run dev  // that cmd open create server and run your default browser http://localhost:3000/\n</code></pre>\n<p>Start the <strong><strong>development</strong></strong> server. After Try <strong><strong>editing</strong></strong> <code>pages/blog.tsx</code> and see the <strong><strong>result</strong></strong> on your <strong><strong>browser</strong></strong>.</p>\n<h2>Note:</h2>\n<p><strong><strong>Index.js</strong></strong> default next.js <strong><strong>router</strong></strong> path.</p>\n<hr>\n<h1>Next.js Youtube Course:</h1>\n<p>On <strong><strong>youtube</strong>, a few YouTubers provide tutorials on next.js. In 2021 lots of YouTubers show interest in next.js.</strong></p>\n<h2>1. Bruno Antunes:</h2>\n<p><strong><strong>Bruno Antunes</strong></strong> provides <strong><strong>a great</strong></strong> path for <strong><strong>Next</strong></strong>.js. That person <strong><strong>creates</strong></strong> a lot of <strong><strong>tutorials</strong></strong> on it **. Make ****** sure you <strong><strong>watch</strong></strong> it.</p>\n<h2>Youtube PlayList:</h2>\n<hr>\n<h2>2. Imran Sayed — Codeytek Academy:</h2>\n<p><strong><strong>Imran Sayed</strong></strong> is a Great <strong><strong>developer</strong></strong> my <strong><strong>opinion</strong></strong>. Imran stayed to provide a very big contribution in  next.js on youtube. Imran provides you a <strong><strong>learning</strong></strong> path in next.js || <strong><strong>Wordpress API</strong></strong>|| <strong><strong>apollo</strong></strong> || <strong><strong>graphql</strong></strong>.</p>\n<h2>Youtube Playlist:</h2>\n<hr>\n<h2>Note:</h2>\n<p>Tell me about other Youtubers who work with Next.js. Then mention in  <strong><strong>Comment</strong> section website</strong>:</p>\n<p>https://nextjs.org/docs/getting-started</p>\n<p>https://www.geeksforgeeks.org/next-js-introduction/</p>\n<p>https://www.freecodecamp.org/news/an-introduction-to-next-js-for-everyone-507d2d90ab54/</p>\n<hr>\n<h1>Read My Other Blog Medium:</h1>\n<p>https://medium.com/officialrajdeepsingh/best-javascript-full-tutorial-on-youtube-117e08ab357a</p>\n<p>https://medium.com/officialrajdeepsingh/how-to-read-local-json-file-in-react-js-564125235fc7</p>\n<hr>\n<h2>Conclusion:</h2>\n<p>My <strong><strong>Opinion</strong></strong> next.js. Great <strong><strong>Library</strong></strong> To help the <strong><strong>developer</strong></strong> create <strong><strong>server-side rendering</strong></strong> (SSR) with one <strong><strong>function</strong></strong> || <strong><strong>method</strong></strong> in next.js. My big <strong><strong>surprise When</strong></strong> I start work with <strong><strong>Wordpress</strong></strong> and <strong><strong>graphql. Next.js</strong></strong> Provide More <strong><strong>stability as compare to</strong></strong> other <strong><strong>React.js</strong></strong> frameworks.</p>\n<p><strong><strong>Now</strong></strong> My first <strong><strong>choice is Nextjs</strong></strong> in <strong><strong>frontend</strong></strong>-<strong><strong>developer.</strong></strong></p>\n<hr>\n<h1>Contact me</h1>\n<ul>\n<li><a href=\"https://www.facebook.com/officialrajdeepsingh/\">https://www.facebook.com/officialrajdeepsingh/</a></li>\n<li><a href=\"https://www.facebook.com/groups/JavaScriptcode/\">https://www.facebook.com/groups/JavaScriptcode/</a></li>\n<li><a href=\"https://www.facebook.com/groups/pythoncodejoin/\">https://www.facebook.com/groups/pythoncodejoin/</a></li>\n<li><a href=\"mailto:officialrajdeepsingh@gmail.com\">officialrajdeepsingh@gmail.com</a></li>\n</ul>\n<h1>Thanks for Reading 😆</h1>"},"_id":"posts/what-is-next-js.md","_raw":{"sourceFilePath":"posts/what-is-next-js.md","sourceFileName":"what-is-next-js.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/what-is-next-js"},"type":"Post"}]},"__N_SSG":true}