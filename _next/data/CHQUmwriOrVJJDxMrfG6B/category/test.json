{"pageProps":{"posts":[{"title":"Check image functinalty working or not","date":"2022-08-16T14:56:38.839Z","author":"Rajdeep singh","description":"react, test","slug":"check-image-functinalty-working-or-not","image":"/images/butterfly-7353884_960_720.webp","draft":false,"tags":["javascript","react"],"categories":["react","test"],"body":{"raw":"in this check Check image functinalty working or not","html":"<p>in this check Check image functinalty working or not</p>"},"_id":"posts/check-image-functinalty-working-or-not.md","_raw":{"sourceFilePath":"posts/check-image-functinalty-working-or-not.md","sourceFileName":"check-image-functinalty-working-or-not.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/check-image-functinalty-working-or-not"},"type":"Post"},{"title":"DONE","date":"2022-12-26T22:16:08.091Z","author":"Ashot","description":"aaaa","slug":"done","image":"/images/group-580.png","draft":false,"tags":["asd"],"categories":["test"],"body":{"raw":"<!--StartFragment-->\n\n## Where does it come from?\n\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\n\n<!--EndFragment-->","html":"<h2>Where does it come from?</h2>\n<p>Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.</p>\n<p>The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.</p>"},"_id":"posts/done.md","_raw":{"sourceFilePath":"posts/done.md","sourceFileName":"done.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/done"},"type":"Post"},{"title":"New Post","date":"2023-01-25T13:06:43.382Z","author":"ASh","description":"very short description","slug":"new-post","image":"/images/dynamic/aim-logo.svg","draft":false,"categories":["test"],"body":{"raw":"<div align=\"center\">\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/154338760-edfe1885-06f3-4e02-87fe-4b13a403516b.png\">\r\n  <h3>An easy-to-use & supercharged open-source experiment tracker</h3>\r\n  Aim logs your training runs, enables a beautiful UI to compare them and an API to query them programmatically.\r\n</div>\r\n\r\n<br/>\r\n\r\n<img src=\"https://user-images.githubusercontent.com/13848158/154338753-34484cda-95b8-4da8-a610-7fdf198c05fd.png\">\r\n\r\n<p align=\"center\">\r\n  <a href=\"#about-aim\"><b>About</b></a> &bull;\r\n  <a href=\"#why-use-aim\"><b>Features</b></a> &bull;\r\n  <a href=\"#demos\"><b>Demos</b></a> &bull;\r\n  <a href=\"https://github.com/aimhubio/aim/tree/main/examples\"><b>Examples</b></a> &bull;\r\n  <a href=\"#quick-start\"><b>Quick Start</b></a> &bull;\r\n  <a href=\"https://aimstack.readthedocs.io/en/latest/\"><b>Documentation</b></a> &bull;\r\n  <a href=\"#roadmap\"><b>Roadmap</b></a> &bull;\r\n  <a href=\"https://community.aimstack.io/\"><b>Discord Community</b></a> &bull;\r\n  <a href=\"https://twitter.com/aimstackio\"><b>Twitter</b></a>\r\n</p>\r\n\r\n<div align=\"center\">\r\n  \r\n  [![Platform Support](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue)]()\r\n  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aim)](https://pypi.org/project/aim/)\r\n  [![PyPI Package](https://img.shields.io/pypi/v/aim?color=yellow)](https://pypi.org/project/aim/)\r\n  [![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)\r\n  [![PyPI Downloads](https://img.shields.io/pypi/dw/aim?color=green)](https://pypi.org/project/aim/)\r\n  [![Issues](https://img.shields.io/github/issues/aimhubio/aim)](http://github.com/aimhubio/aim/issues)\r\n  \r\n</div>\r\n\r\n<div align=\"center\">\r\n  <sub>Integrates seamlessly with your favorite tools</sub>\r\n  <br/>\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354389-d0301620-77ea-4629-a743-f7aa249e14b5.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354496-b39d7b1c-63ef-40f0-9e59-c08d2c5e337c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354380-3755c741-6960-42ca-b93e-84a8791f088c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354342-7df0ef5e-63d2-4df7-b9f1-d2fc0e95f53f.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354392-afbff3de-c845-4d86-855d-53df569f91d1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354355-89210506-e7e5-4d37-b2d6-ad3fda62ef13.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354397-8af8e1d3-4067-405e-9d42-1f131663ed22.png\" width=\"60\" />\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354513-f7486146-3891-4f3f-934f-e58bbf9ce695.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354500-c0471ce6-b2ce-4172-b9e4-07a197256303.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354361-9f911785-008d-4b75-877e-651e026cf47e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354373-1879ae61-b5d1-41f0-a4f1-04b639b6f05e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354483-75d9853f-7154-4d95-8190-9ad7a73d6654.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354329-cf7c3352-a72a-478d-82a7-04e3833b03b7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354349-dcdf3bc3-d7a9-4f34-8258-4824a57f59c7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354471-518f1814-7a41-4b23-9caf-e516507343f1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165162736-2cc5da39-38aa-4093-874f-e56d0ba9cea2.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165074282-36ad18eb-1124-434d-8439-728c22cd7ac7.png\" width=\"60\" />\r\n</div>\r\n\r\n<div align=\"center\">\r\n  <br/>\r\n  <kbd>\r\n    <img width=\"650px\" src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" />\r\n  </kbd>\r\n</div>\r\n\r\n# About Aim\r\n\r\n| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n| <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337794-e9310239-6614-41b3-a95b-bb91f0bb6c4f.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337788-03fe5b31-0fa3-44af-ae79-2861707d8602.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337793-85175c78-5659-4dd0-bb2d-05017278e2fa.png\"> |\r\n\r\nAim is an open-source, self-hosted ML experiment tracking tool. \r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\r\n\r\nYou can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically. \r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.\r\n\r\n\r\nAim's mission is to democratize AI dev tools.\r\n\r\n# Why use Aim?\r\n\r\n### Compare 100s of runs in a few clicks - build models faster\r\n\r\n- Compare, group and aggregate 100s of metrics thanks to effective visualizations.\r\n- Analyze, learn correlations and patterns between hparams and metrics.\r\n- Easy pythonic search to query the runs you want to explore.\r\n\r\n### Deep dive into details of each run for easy debugging\r\n\r\n- Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.\r\n- Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.\r\n- Analyze system resource usage to effectively utilize computational resources.\r\n\r\n### Have all relevant information organised and accessible for easy governance\r\n\r\n- Centralized dashboard to holistically view all your runs, their hparams and results.\r\n- Use SDK to query/access all your runs and tracked metadata.\r\n- You own your data - Aim is open source and self hosted.\r\n\r\n# Demos\r\n\r\n| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10001/metrics?grouping=HQGdK9Xxy35e6sY1CYkCmk1WbWMN2AsCNfJJ3d1RJYLtrVPMoF5UpGiA6CF8bEJnfzRsKpqespf3AEuKSVrhUYvYk9MxzNGA9XZWYUf6phEg8AMbZGLRVDXnAPDuo8tueqsST1ZLizWzQwDYJWHUza6pyB2Eojt9uWqNHUdb858TqDRnCJzqiVJXKXEzFWUyvU8MckJo1qpqWWCTb4GpYN6DUJZx2GXDGR21e2xxd4m7PmNUnbA9B3apLttZoipJF6c3v7tNUKmb6irpqnNB3yc57tqYDa1XZuKfDxkMtyFdQ1x95K4jjsTVwhftEWLze35QNcxNXRCGGS9o9yEfTLG26GUX2zjPZFCjjMGU6vV7z1xRccK8MyoGrLSgAQCbvk68dTGBHpXUBvCRq8N&chart=FviZzVrt4fVQPjpCLr9sVGGrcR5etSroyqambiKpm3nTgpyv4eQxKuwNX9uN8UtKmzYUhUyTMBEANHmtbwjLApkvnYeNbxGNC6PVcoqi65m1XJnSrvgt8WiD89BapFAWRUwAGx6SWD7KZPsk3RQyysU7W7FjD3Q99NusxFGhsEfD6HXc7i8xH9KHDRGjLwh6x9VTtSp4FS8HEvpLSiiJoX7LCTi8pB7dXvrQ8G5w3jPsFz4qXYFdsVaCNL1BpFFZuiqQNkfbnM84gEq7UmiV1VzM4oS3AgQHxADG3kpBVp6eKTey9F1Swd4FcUkFA9QEPjgQgqwRGjkquZ2bdDDVLBnCh7JPvboP2kifCiZZ5MDdV9MMx6PKHp4DusWyWLXiHQYPkpGPWBiuccMUXDsuJaCWJbuABdY7CyiJMv1jdHYkjabygSxehPVyEDefWAtjBfv2vaeM1xv63jadbmpKYFxft7qmuT9HvVxiGvRgs4RQFxy8K4rtFBca3HNs1mDaaY81gy9MGXyw7BS5Fniu92jaJpsWDdg6Y3AQBLZtrpJy2obEZ4yzJaCVT7JUNPAyyCUNLck393VFLoEkaD9CU5npK5R7tj1c1G3gkMNQXnSXy5NpSj8deMmXV5qz3JKu1nq2caGQKcqjzy2gLkExdm674AMFjSg9yFjK6VqASXQ17NKtWRUvaYoxGbHDAFQaMKWKh8QLm22QA9mKT8NksLptWozbgDvafnQLNMvezLU5bvKV5o75PAWYiRB56RcYfEhzaB6YWdgL7TJicyY5rFi6Az8UZ7wqB3N5iMuZdpxhKn5KbZDxyuUMuvVt24i5LVPPmmwQtqxMoJ4aLo48a2YvDW6TAkdQjNjvn6KcEEz6GTixujb1YHhMUD8v4AepWKEwKz1ddEca1P2wLQjbpihCuaqbxeohnuZZLogJdUBojBEDgrnrrVpPBaLLEkGSpkJbtrsKUuEeBo1AF3yNgHftLbynGpobVF5DhmsmddmiA6c8vSTokJxHhjpnW8mAcNHBRtmVJCT7VkdHSAhNypM4Hivwfx5jCccG9LauKmCeRMDzHiA57TX9W6ttcPHSvUyQorARQAd2oeNY4H83hZjHh9Bt8iwKZRt4xK6hrTR8tif7hq8eURXrGH9Ys7TzykXK8FHHWvLNzNnYf3E4a9NkD43MjfKvMM1hj4Q2K8MHbmRCqrmFrHP5kim9shq6mhLPTgwha32nvnrBkfPQVPwpGTzKuwE&select=CdsQ7jVNkogQhRzQR3e28Ek39AZ4Ma2y37k5zJaf9EZmQhMjy8GtGm4LGU6dRFuAVG7mYww5xDrQAE74KHQ3Kk1e6661RmcmNALAUjtHyCmrTVBMCnBGNiuq1y7EzmxoodYHU1BV1rnoefQAw2kTBtbWi11hV1P4LcwFCcXfUWF6rpRC7ehEnUCTqUV4bkGVJPLcmk9mdmiGwa2YgmnSShNGPVGZiEi1rMVECyngSRVdqdZwAeXBGWFLfqF1KbZeCo4MTF4SSmFupJ9zLhYbuojEbopyFWHQ6xs3sq9epPeaQziLM4Js7oFYRmuFWUYdFqnZngmewXWmi7tQAgVqhiT6dMjG2eTdfgX6WuRSuoHALkh2XJhHA6GfZLUcxC5Ni9YyKuBTamtaYarbNNJJ8z15WWvuUkLpjgHdEpE2h924xFdu8aoZNuiQxYGvcndaW1BTGMXS5fTKPqYfe2n8Ky2HWPkcX3hEXtyawu1F9BndKNaXLPgsdAoFBArBZnSe28YtSmTa5LRucKVBAxakvv5MWMXchAmpaGFQbZyYUoMgQLcJd7Y96x6zSR7nhwr5Ar81BrmqYz2WFLuk7osUbwsc9HbSG6CQt8p6Vg2u7DjKaZXW8pjkPHAKrHWtHEDiJPJ5rj6VsdFm3\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340796-c9e91b13-8ee0-4a67-bcde-8cf3aaa7ba99.jpg\"> </a> | <a href=\"http://play.aimstack.io:10002/images?grouping=E1zQzcmtDR3wibEa1MVysTvCyZEv1T8ixkCxTWExCyMnHtX2HyiF9eszvPgfd2xdJ5TUTKGpSs1bsLVq5tHAV3uWtsZmmckn6HjNtVCMyQDJpwhiEy5tAyw&select=2NEXuD7fFoaLcwRjymjA1wLmUrGs9s3AiXcCW82C367SwJt18CAB6xzkMGowrUDuDwggE1huaPVcQJpQUsmAQx1CnGiqCUBp2jPMd5mMNPX2QKQMcmvu9ZykBNkeBvCQFPd9ERuQD2g1EjWuvyJ3H53mAZTfp94LCXvR9CUsG5ei2CjQUzfZLM6DCyUr1GPaEVnY5f1EwzicNxXuoutkBgqCqaobJ7Do4q4eHAA6ooiWU6ekS3D2sLj6qYwhVTjfGCPfbWwBiH83nFkY3fLExzdeTY2zeUHeeYikQR9S7xHbVD8WvjekdQVp8X4dNLJZxiVmEqHpPRnU3ZrYsMhE7yFAAgjJwPNUzLTt6YFrtZBcmc4rwAC2oyrqysUSEr6gzL6LcJ6yuqDGf9D5tzftHbTLDkhc8B2sCgTS&images=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\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340790-bc7b7a21-e8a1-43a1-809d-4060b5bfb60f.jpg\"> </a> |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |\r\n\r\n| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/audios\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340778-dbe19620-2f27-4298-b0cb-caf3904760f1.jpg\"> </a> | <a href=\"http://play.aimstack.io:10003/runs/7f083da898624a2c98e0f363/distributions\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340785-a7e4d9fd-d048-4207-8cd1-c4edff9cca6a.jpg\"> </a> |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |\r\n\r\n# Quick Start\r\n\r\nFollow the steps below to get started with Aim.\r\n\r\n**1. Install Aim on your training environment**\r\n\r\n```shell\r\npip3 install aim\r\n```\r\n\r\n**2. Integrate Aim with your code**\r\n\r\n```python\r\nfrom aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\r\n```\r\n\r\n_See the full list of supported trackable objects(e.g. images, text, etc) [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html)._\r\n\r\n**3. Run the training as usual and start Aim UI**\r\n\r\n```shell\r\naim up\r\n```\r\n\r\n**4. Or query runs programmatically via SDK**\r\n\r\n```python\r\nfrom aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\r\n```\r\n\r\n# Integrations\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Lightning\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Hugging Face\r\n</summary>\r\n\r\n```python\r\nfrom aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Keras & tf.keras\r\n</summary>\r\n\r\n```python\r\nimport aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate KerasTuner\r\n</summary>\r\n\r\n```python\r\nfrom aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate XGBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate CatBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost)._\r\n</details>\r\n\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate fastai\r\n</summary>\r\n\r\n```python\r\nfrom aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate LightGBM\r\n</summary>\r\n\r\n```python\r\nfrom aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Ignite\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite)._\r\n</details>\r\n\r\n# Comparisons to familiar tools\r\n\r\n### Tensorboard\r\n**Training run comparison**\r\n\r\nOrder of magnitude faster training run comparison with Aim\r\n- The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.\r\n- With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. **TensorBoard doesn't have features to group, aggregate the metrics**\r\n\r\n**Scalability**\r\n\r\n- Aim is built to handle 1000s of training runs - both on the backend and on the UI.\r\n- TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\r\n\r\n**Beloved TB visualizations to be added on Aim**\r\n\r\n- Embedding projector.\r\n- Neural network visualization.\r\n\r\n### MLFlow\r\nMLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.\r\n\r\n**Run comparison**\r\n\r\n- Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.\r\n- MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.\r\n\r\n**UI Scalability**\r\n\r\n- Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\r\n- MLflow UI becomes slow to use when there are a few hundreds of runs.\r\n\r\n### Weights and Biases\r\n\r\nHosted vs self-hosted\r\n- Weights and Biases is a hosted closed-source MLOps platform.\r\n- Aim is self-hosted, free and open-source experiment tracking tool.\r\n\r\n# Roadmap\r\n\r\n## Detailed Sprints\r\n\r\n:sparkle: The [Aim product roadmap](https://github.com/orgs/aimhubio/projects/3)\r\n\r\n- The `Backlog` contains the issues we are going to choose from and prioritize weekly\r\n- The issues are mainly prioritized by the highly-requested features\r\n\r\n## High-level roadmap\r\n\r\nThe high-level features we are going to work on the next few months\r\n\r\n### Done\r\n  - [x] Live updates (Shipped: _Oct 18 2021_)\r\n  - [x] Images tracking and visualization (Start: _Oct 18 2021_, Shipped: _Nov 19 2021_)\r\n  - [x] Distributions tracking and visualization (Start: _Nov 10 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Jupyter integration (Start: _Nov 18 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Audio tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Transcripts tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Plotly integration (Start: _Dec 1 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Colab integration (Start: _Nov 18 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Centralized tracking server (Start: _Oct 18 2021_, Shipped: _Jan 22 2022_)\r\n  - [x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: _Dec 17 2021_, Shipped: _Feb 3 2022_)\r\n  - [x] Track git info, env vars, CLI arguments, dependencies (Start: _Jan 17 2022_, Shipped: _Feb 3 2022_)\r\n  - [x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Activeloop Hub integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] PyTorch-Ignite integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Run summary and overview info(system params, CLI args, git info, ...) (Start: _Feb 14 2022_, Shipped: _Mar 9 2022_)\r\n  - [x] Add DVC related metadata into aim run (Start: _Mar 7 2022_, Shipped: _Mar 26 2022_)\r\n  - [x] Ability to attach notes to Run from UI (Start: _Mar 7 2022_, Shipped: _Apr 29 2022_)\r\n  - [x] Fairseq integration (Start: _Mar 27 2022_, Shipped: _Mar 29 2022_)\r\n  - [x] LightGBM integration (Start: _Apr 14 2022_, Shipped: _May 17 2022_)\r\n  - [x] CatBoost integration (Start: _Apr 20 2022_, Shipped: _May 17 2022_)\r\n  - [x] Run execution details(display stdout/stderr logs) (Start: _Apr 25 2022_, Shipped: _May 17 2022_)\r\n  - [x] Long sequences(up to 5M of steps) support (Start: _Apr 25 2022_, Shipped: _Jun 22 2022_)\r\n  - [x] Figures Explorer (Start: _Mar 1 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Notify on stuck runs (Start: _Jul 22 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with KerasTuner (Start: _Aug 10 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with WandB (Start: _Aug 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Stable remote tracking server (Start: _Jun 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with fast.ai (Start: _Aug 22 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Integration with MXNet (Start: _Sep 20 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Project overview page (Start: _Sep 1 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Remote tracking server scaling (Start: _Sep 11 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with PaddlePaddle (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with Optuna (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Audios Explorer (Start: _Oct 30 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Experiment page (Start: _Nov 9 2022_, Shipped: _Nov 26 2022_)\r\n\r\n### In Progress\r\n  - [ ] Aim SDK low-level interface (Start: _Aug 22 2022_, )\r\n  - [ ] HuggingFace datasets (Start: _Dec 29 2022_, )\r\n\r\n### To Do\r\n\r\n**Aim UI**\r\n\r\n- Runs management\r\n    - Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard\r\n- Explorers\r\n    - Text Explorer\r\n    - Distributions Explorer\r\n- Dashboards – customizable layouts with embedded explorers\r\n\r\n**SDK and Storage**\r\n\r\n- Scalability\r\n    - Smooth UI and SDK experience with over 10.000 runs\r\n- Runs management\r\n    - CLI interfaces\r\n        - Reporting - runs summary and run details in a CLI compatible format\r\n        - Manipulations – copy, move, delete runs, params and sequences\r\n\r\n**Integrations**\r\n\r\n- ML Frameworks:\r\n    - Shortlist: MONAI, SpaCy, Raytune\r\n- Resource management tools\r\n    - Shortlist: Kubeflow, Slurm\r\n- Workflow orchestration tools\r\n- Others: Hydra, Google MLMD, Streamlit, ...\r\n\r\n### On hold\r\n\r\n- scikit-learn integration\r\n- Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: _Mar 21 2022_)\r\n- Artifact storage – store files, model checkpoints, and beyond (Start: _Mar 21 2022_)\r\n\r\n## Community\r\n\r\n### If you have questions\r\n\r\n1. [Read the docs](https://aimstack.readthedocs.io/en/latest/)\r\n2. [Open a feature request or report a bug](https://github.com/aimhubio/aim/issues)\r\n3. [Join Discord community server](https://community.aimstack.io/)<div align=\"center\">\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/154338760-edfe1885-06f3-4e02-87fe-4b13a403516b.png\">\r\n  <h3>An easy-to-use & supercharged open-source experiment tracker</h3>\r\n  Aim logs your training runs, enables a beautiful UI to compare them and an API to query them programmatically.\r\n</div>\r\n\r\n<br/>\r\n\r\n<img src=\"https://user-images.githubusercontent.com/13848158/154338753-34484cda-95b8-4da8-a610-7fdf198c05fd.png\">\r\n\r\n<p align=\"center\">\r\n  <a href=\"#about-aim\"><b>About</b></a> &bull;\r\n  <a href=\"#why-use-aim\"><b>Features</b></a> &bull;\r\n  <a href=\"#demos\"><b>Demos</b></a> &bull;\r\n  <a href=\"https://github.com/aimhubio/aim/tree/main/examples\"><b>Examples</b></a> &bull;\r\n  <a href=\"#quick-start\"><b>Quick Start</b></a> &bull;\r\n  <a href=\"https://aimstack.readthedocs.io/en/latest/\"><b>Documentation</b></a> &bull;\r\n  <a href=\"#roadmap\"><b>Roadmap</b></a> &bull;\r\n  <a href=\"https://community.aimstack.io/\"><b>Discord Community</b></a> &bull;\r\n  <a href=\"https://twitter.com/aimstackio\"><b>Twitter</b></a>\r\n</p>\r\n\r\n<div align=\"center\">\r\n  \r\n  [![Platform Support](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue)]()\r\n  [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aim)](https://pypi.org/project/aim/)\r\n  [![PyPI Package](https://img.shields.io/pypi/v/aim?color=yellow)](https://pypi.org/project/aim/)\r\n  [![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)\r\n  [![PyPI Downloads](https://img.shields.io/pypi/dw/aim?color=green)](https://pypi.org/project/aim/)\r\n  [![Issues](https://img.shields.io/github/issues/aimhubio/aim)](http://github.com/aimhubio/aim/issues)\r\n  \r\n</div>\r\n\r\n<div align=\"center\">\r\n  <sub>Integrates seamlessly with your favorite tools</sub>\r\n  <br/>\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354389-d0301620-77ea-4629-a743-f7aa249e14b5.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354496-b39d7b1c-63ef-40f0-9e59-c08d2c5e337c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354380-3755c741-6960-42ca-b93e-84a8791f088c.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354342-7df0ef5e-63d2-4df7-b9f1-d2fc0e95f53f.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354392-afbff3de-c845-4d86-855d-53df569f91d1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354355-89210506-e7e5-4d37-b2d6-ad3fda62ef13.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354397-8af8e1d3-4067-405e-9d42-1f131663ed22.png\" width=\"60\" />\r\n  <br/>\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354513-f7486146-3891-4f3f-934f-e58bbf9ce695.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354500-c0471ce6-b2ce-4172-b9e4-07a197256303.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354361-9f911785-008d-4b75-877e-651e026cf47e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354373-1879ae61-b5d1-41f0-a4f1-04b639b6f05e.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354483-75d9853f-7154-4d95-8190-9ad7a73d6654.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354329-cf7c3352-a72a-478d-82a7-04e3833b03b7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354349-dcdf3bc3-d7a9-4f34-8258-4824a57f59c7.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354471-518f1814-7a41-4b23-9caf-e516507343f1.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165162736-2cc5da39-38aa-4093-874f-e56d0ba9cea2.png\" width=\"60\" />\r\n  <img src=\"https://user-images.githubusercontent.com/48801049/165074282-36ad18eb-1124-434d-8439-728c22cd7ac7.png\" width=\"60\" />\r\n</div>\r\n\r\n<div align=\"center\">\r\n  <br/>\r\n  <kbd>\r\n    <img width=\"650px\" src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" />\r\n  </kbd>\r\n</div>\r\n\r\n# About Aim\r\n\r\n| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n| <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337794-e9310239-6614-41b3-a95b-bb91f0bb6c4f.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337788-03fe5b31-0fa3-44af-ae79-2861707d8602.png\"> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337793-85175c78-5659-4dd0-bb2d-05017278e2fa.png\"> |\r\n\r\nAim is an open-source, self-hosted ML experiment tracking tool. \r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\r\n\r\nYou can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically. \r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.\r\n\r\n\r\nAim's mission is to democratize AI dev tools.\r\n\r\n# Why use Aim?\r\n\r\n### Compare 100s of runs in a few clicks - build models faster\r\n\r\n- Compare, group and aggregate 100s of metrics thanks to effective visualizations.\r\n- Analyze, learn correlations and patterns between hparams and metrics.\r\n- Easy pythonic search to query the runs you want to explore.\r\n\r\n### Deep dive into details of each run for easy debugging\r\n\r\n- Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.\r\n- Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.\r\n- Analyze system resource usage to effectively utilize computational resources.\r\n\r\n### Have all relevant information organised and accessible for easy governance\r\n\r\n- Centralized dashboard to holistically view all your runs, their hparams and results.\r\n- Use SDK to query/access all your runs and tracked metadata.\r\n- You own your data - Aim is open source and self hosted.\r\n\r\n# Demos\r\n\r\n| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10001/metrics?grouping=HQGdK9Xxy35e6sY1CYkCmk1WbWMN2AsCNfJJ3d1RJYLtrVPMoF5UpGiA6CF8bEJnfzRsKpqespf3AEuKSVrhUYvYk9MxzNGA9XZWYUf6phEg8AMbZGLRVDXnAPDuo8tueqsST1ZLizWzQwDYJWHUza6pyB2Eojt9uWqNHUdb858TqDRnCJzqiVJXKXEzFWUyvU8MckJo1qpqWWCTb4GpYN6DUJZx2GXDGR21e2xxd4m7PmNUnbA9B3apLttZoipJF6c3v7tNUKmb6irpqnNB3yc57tqYDa1XZuKfDxkMtyFdQ1x95K4jjsTVwhftEWLze35QNcxNXRCGGS9o9yEfTLG26GUX2zjPZFCjjMGU6vV7z1xRccK8MyoGrLSgAQCbvk68dTGBHpXUBvCRq8N&chart=FviZzVrt4fVQPjpCLr9sVGGrcR5etSroyqambiKpm3nTgpyv4eQxKuwNX9uN8UtKmzYUhUyTMBEANHmtbwjLApkvnYeNbxGNC6PVcoqi65m1XJnSrvgt8WiD89BapFAWRUwAGx6SWD7KZPsk3RQyysU7W7FjD3Q99NusxFGhsEfD6HXc7i8xH9KHDRGjLwh6x9VTtSp4FS8HEvpLSiiJoX7LCTi8pB7dXvrQ8G5w3jPsFz4qXYFdsVaCNL1BpFFZuiqQNkfbnM84gEq7UmiV1VzM4oS3AgQHxADG3kpBVp6eKTey9F1Swd4FcUkFA9QEPjgQgqwRGjkquZ2bdDDVLBnCh7JPvboP2kifCiZZ5MDdV9MMx6PKHp4DusWyWLXiHQYPkpGPWBiuccMUXDsuJaCWJbuABdY7CyiJMv1jdHYkjabygSxehPVyEDefWAtjBfv2vaeM1xv63jadbmpKYFxft7qmuT9HvVxiGvRgs4RQFxy8K4rtFBca3HNs1mDaaY81gy9MGXyw7BS5Fniu92jaJpsWDdg6Y3AQBLZtrpJy2obEZ4yzJaCVT7JUNPAyyCUNLck393VFLoEkaD9CU5npK5R7tj1c1G3gkMNQXnSXy5NpSj8deMmXV5qz3JKu1nq2caGQKcqjzy2gLkExdm674AMFjSg9yFjK6VqASXQ17NKtWRUvaYoxGbHDAFQaMKWKh8QLm22QA9mKT8NksLptWozbgDvafnQLNMvezLU5bvKV5o75PAWYiRB56RcYfEhzaB6YWdgL7TJicyY5rFi6Az8UZ7wqB3N5iMuZdpxhKn5KbZDxyuUMuvVt24i5LVPPmmwQtqxMoJ4aLo48a2YvDW6TAkdQjNjvn6KcEEz6GTixujb1YHhMUD8v4AepWKEwKz1ddEca1P2wLQjbpihCuaqbxeohnuZZLogJdUBojBEDgrnrrVpPBaLLEkGSpkJbtrsKUuEeBo1AF3yNgHftLbynGpobVF5DhmsmddmiA6c8vSTokJxHhjpnW8mAcNHBRtmVJCT7VkdHSAhNypM4Hivwfx5jCccG9LauKmCeRMDzHiA57TX9W6ttcPHSvUyQorARQAd2oeNY4H83hZjHh9Bt8iwKZRt4xK6hrTR8tif7hq8eURXrGH9Ys7TzykXK8FHHWvLNzNnYf3E4a9NkD43MjfKvMM1hj4Q2K8MHbmRCqrmFrHP5kim9shq6mhLPTgwha32nvnrBkfPQVPwpGTzKuwE&select=CdsQ7jVNkogQhRzQR3e28Ek39AZ4Ma2y37k5zJaf9EZmQhMjy8GtGm4LGU6dRFuAVG7mYww5xDrQAE74KHQ3Kk1e6661RmcmNALAUjtHyCmrTVBMCnBGNiuq1y7EzmxoodYHU1BV1rnoefQAw2kTBtbWi11hV1P4LcwFCcXfUWF6rpRC7ehEnUCTqUV4bkGVJPLcmk9mdmiGwa2YgmnSShNGPVGZiEi1rMVECyngSRVdqdZwAeXBGWFLfqF1KbZeCo4MTF4SSmFupJ9zLhYbuojEbopyFWHQ6xs3sq9epPeaQziLM4Js7oFYRmuFWUYdFqnZngmewXWmi7tQAgVqhiT6dMjG2eTdfgX6WuRSuoHALkh2XJhHA6GfZLUcxC5Ni9YyKuBTamtaYarbNNJJ8z15WWvuUkLpjgHdEpE2h924xFdu8aoZNuiQxYGvcndaW1BTGMXS5fTKPqYfe2n8Ky2HWPkcX3hEXtyawu1F9BndKNaXLPgsdAoFBArBZnSe28YtSmTa5LRucKVBAxakvv5MWMXchAmpaGFQbZyYUoMgQLcJd7Y96x6zSR7nhwr5Ar81BrmqYz2WFLuk7osUbwsc9HbSG6CQt8p6Vg2u7DjKaZXW8pjkPHAKrHWtHEDiJPJ5rj6VsdFm3\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340796-c9e91b13-8ee0-4a67-bcde-8cf3aaa7ba99.jpg\"> </a> | <a href=\"http://play.aimstack.io:10002/images?grouping=E1zQzcmtDR3wibEa1MVysTvCyZEv1T8ixkCxTWExCyMnHtX2HyiF9eszvPgfd2xdJ5TUTKGpSs1bsLVq5tHAV3uWtsZmmckn6HjNtVCMyQDJpwhiEy5tAyw&select=2NEXuD7fFoaLcwRjymjA1wLmUrGs9s3AiXcCW82C367SwJt18CAB6xzkMGowrUDuDwggE1huaPVcQJpQUsmAQx1CnGiqCUBp2jPMd5mMNPX2QKQMcmvu9ZykBNkeBvCQFPd9ERuQD2g1EjWuvyJ3H53mAZTfp94LCXvR9CUsG5ei2CjQUzfZLM6DCyUr1GPaEVnY5f1EwzicNxXuoutkBgqCqaobJ7Do4q4eHAA6ooiWU6ekS3D2sLj6qYwhVTjfGCPfbWwBiH83nFkY3fLExzdeTY2zeUHeeYikQR9S7xHbVD8WvjekdQVp8X4dNLJZxiVmEqHpPRnU3ZrYsMhE7yFAAgjJwPNUzLTt6YFrtZBcmc4rwAC2oyrqysUSEr6gzL6LcJ6yuqDGf9D5tzftHbTLDkhc8B2sCgTS&images=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\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340790-bc7b7a21-e8a1-43a1-809d-4060b5bfb60f.jpg\"> </a> |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |\r\n\r\n| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n| <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/audios\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340778-dbe19620-2f27-4298-b0cb-caf3904760f1.jpg\"> </a> | <a href=\"http://play.aimstack.io:10003/runs/7f083da898624a2c98e0f363/distributions\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340785-a7e4d9fd-d048-4207-8cd1-c4edff9cca6a.jpg\"> </a> |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |\r\n\r\n# Quick Start\r\n\r\nFollow the steps below to get started with Aim.\r\n\r\n**1. Install Aim on your training environment**\r\n\r\n```shell\r\npip3 install aim\r\n```\r\n\r\n**2. Integrate Aim with your code**\r\n\r\n```python\r\nfrom aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\r\n```\r\n\r\n_See the full list of supported trackable objects(e.g. images, text, etc) [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html)._\r\n\r\n**3. Run the training as usual and start Aim UI**\r\n\r\n```shell\r\naim up\r\n```\r\n\r\n**4. Or query runs programmatically via SDK**\r\n\r\n```python\r\nfrom aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\r\n```\r\n\r\n# Integrations\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Lightning\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Hugging Face\r\n</summary>\r\n\r\n```python\r\nfrom aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate Keras & tf.keras\r\n</summary>\r\n\r\n```python\r\nimport aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate KerasTuner\r\n</summary>\r\n\r\n```python\r\nfrom aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner)._\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>\r\n  Integrate XGBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate CatBoost\r\n</summary>\r\n\r\n```python\r\nfrom aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost)._\r\n</details>\r\n\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate fastai\r\n</summary>\r\n\r\n```python\r\nfrom aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate LightGBM\r\n</summary>\r\n\r\n```python\r\nfrom aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm)._\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\n  Integrate PyTorch Ignite\r\n</summary>\r\n\r\n```python\r\nfrom aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\r\n```\r\n\r\n_See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite)._\r\n</details>\r\n\r\n# Comparisons to familiar tools\r\n\r\n### Tensorboard\r\n**Training run comparison**\r\n\r\nOrder of magnitude faster training run comparison with Aim\r\n- The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.\r\n- With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. **TensorBoard doesn't have features to group, aggregate the metrics**\r\n\r\n**Scalability**\r\n\r\n- Aim is built to handle 1000s of training runs - both on the backend and on the UI.\r\n- TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\r\n\r\n**Beloved TB visualizations to be added on Aim**\r\n\r\n- Embedding projector.\r\n- Neural network visualization.\r\n\r\n### MLFlow\r\nMLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.\r\n\r\n**Run comparison**\r\n\r\n- Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.\r\n- MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.\r\n\r\n**UI Scalability**\r\n\r\n- Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\r\n- MLflow UI becomes slow to use when there are a few hundreds of runs.\r\n\r\n### Weights and Biases\r\n\r\nHosted vs self-hosted\r\n- Weights and Biases is a hosted closed-source MLOps platform.\r\n- Aim is self-hosted, free and open-source experiment tracking tool.\r\n\r\n# Roadmap\r\n\r\n## Detailed Sprints\r\n\r\n:sparkle: The [Aim product roadmap](https://github.com/orgs/aimhubio/projects/3)\r\n\r\n- The `Backlog` contains the issues we are going to choose from and prioritize weekly\r\n- The issues are mainly prioritized by the highly-requested features\r\n\r\n## High-level roadmap\r\n\r\nThe high-level features we are going to work on the next few months\r\n\r\n### Done\r\n  - [x] Live updates (Shipped: _Oct 18 2021_)\r\n  - [x] Images tracking and visualization (Start: _Oct 18 2021_, Shipped: _Nov 19 2021_)\r\n  - [x] Distributions tracking and visualization (Start: _Nov 10 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Jupyter integration (Start: _Nov 18 2021_, Shipped: _Dec 3 2021_)\r\n  - [x] Audio tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Transcripts tracking and visualization (Start: _Dec 6 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Plotly integration (Start: _Dec 1 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Colab integration (Start: _Nov 18 2021_, Shipped: _Dec 17 2021_)\r\n  - [x] Centralized tracking server (Start: _Oct 18 2021_, Shipped: _Jan 22 2022_)\r\n  - [x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: _Dec 17 2021_, Shipped: _Feb 3 2022_)\r\n  - [x] Track git info, env vars, CLI arguments, dependencies (Start: _Jan 17 2022_, Shipped: _Feb 3 2022_)\r\n  - [x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Activeloop Hub integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] PyTorch-Ignite integration (Start: _Feb 14 2022_, Shipped: _Feb 22 2022_)\r\n  - [x] Run summary and overview info(system params, CLI args, git info, ...) (Start: _Feb 14 2022_, Shipped: _Mar 9 2022_)\r\n  - [x] Add DVC related metadata into aim run (Start: _Mar 7 2022_, Shipped: _Mar 26 2022_)\r\n  - [x] Ability to attach notes to Run from UI (Start: _Mar 7 2022_, Shipped: _Apr 29 2022_)\r\n  - [x] Fairseq integration (Start: _Mar 27 2022_, Shipped: _Mar 29 2022_)\r\n  - [x] LightGBM integration (Start: _Apr 14 2022_, Shipped: _May 17 2022_)\r\n  - [x] CatBoost integration (Start: _Apr 20 2022_, Shipped: _May 17 2022_)\r\n  - [x] Run execution details(display stdout/stderr logs) (Start: _Apr 25 2022_, Shipped: _May 17 2022_)\r\n  - [x] Long sequences(up to 5M of steps) support (Start: _Apr 25 2022_, Shipped: _Jun 22 2022_)\r\n  - [x] Figures Explorer (Start: _Mar 1 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Notify on stuck runs (Start: _Jul 22 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with KerasTuner (Start: _Aug 10 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with WandB (Start: _Aug 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Stable remote tracking server (Start: _Jun 15 2022_, Shipped: _Aug 21 2022_)\r\n  - [x] Integration with fast.ai (Start: _Aug 22 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Integration with MXNet (Start: _Sep 20 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Project overview page (Start: _Sep 1 2022_, Shipped: _Oct 6 2022_)\r\n  - [x] Remote tracking server scaling (Start: _Sep 11 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with PaddlePaddle (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Integration with Optuna (Start: _Oct 2 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Audios Explorer (Start: _Oct 30 2022_, Shipped: _Nov 26 2022_)\r\n  - [x] Experiment page (Start: _Nov 9 2022_, Shipped: _Nov 26 2022_)\r\n\r\n### In Progress\r\n  - [ ] Aim SDK low-level interface (Start: _Aug 22 2022_, )\r\n  - [ ] HuggingFace datasets (Start: _Dec 29 2022_, )\r\n\r\n### To Do\r\n\r\n**Aim UI**\r\n\r\n- Runs management\r\n    - Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard\r\n- Explorers\r\n    - Text Explorer\r\n    - Distributions Explorer\r\n- Dashboards – customizable layouts with embedded explorers\r\n\r\n**SDK and Storage**\r\n\r\n- Scalability\r\n    - Smooth UI and SDK experience with over 10.000 runs\r\n- Runs management\r\n    - CLI interfaces\r\n        - Reporting - runs summary and run details in a CLI compatible format\r\n        - Manipulations – copy, move, delete runs, params and sequences\r\n\r\n**Integrations**\r\n\r\n- ML Frameworks:\r\n    - Shortlist: MONAI, SpaCy, Raytune\r\n- Resource management tools\r\n    - Shortlist: Kubeflow, Slurm\r\n- Workflow orchestration tools\r\n- Others: Hydra, Google MLMD, Streamlit, ...\r\n\r\n### On hold\r\n\r\n- scikit-learn integration\r\n- Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: _Mar 21 2022_)\r\n- Artifact storage – store files, model checkpoints, and beyond (Start: _Mar 21 2022_)\r\n\r\n## Community\r\n\r\n### If you have questions\r\n\r\n1. [Read the docs](https://aimstack.readthedocs.io/en/latest/)\r\n2. [Open a feature request or report a bug](https://github.com/aimhubio/aim/issues)\r\n3. [Join Discord community server](https://community.aimstack.io/)","html":"<p><a href=\"\"><img src=\"https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue\" alt=\"Platform Support\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/pyversions/aim\" alt=\"PyPI - Python Version\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/v/aim?color=yellow\" alt=\"PyPI Package\"></a>\r\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-orange.svg\" alt=\"License\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/dw/aim?color=green\" alt=\"PyPI Downloads\"></a>\r\n<a href=\"http://github.com/aimhubio/aim/issues\"><img src=\"https://img.shields.io/github/issues/aimhubio/aim\" alt=\"Issues\"></a></p>\n<h1>About Aim</h1>\n<p>| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n|  |  |  |</p>\n<p>Aim is an open-source, self-hosted ML experiment tracking tool.\r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.</p>\n<p>You can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically.\r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.</p>\n<p>Aim's mission is to democratize AI dev tools.</p>\n<h1>Why use Aim?</h1>\n<h3>Compare 100s of runs in a few clicks - build models faster</h3>\n<ul>\n<li>Compare, group and aggregate 100s of metrics thanks to effective visualizations.</li>\n<li>Analyze, learn correlations and patterns between hparams and metrics.</li>\n<li>Easy pythonic search to query the runs you want to explore.</li>\n</ul>\n<h3>Deep dive into details of each run for easy debugging</h3>\n<ul>\n<li>Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.</li>\n<li>Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.</li>\n<li>Analyze system resource usage to effectively utilize computational resources.</li>\n</ul>\n<h3>Have all relevant information organised and accessible for easy governance</h3>\n<ul>\n<li>Centralized dashboard to holistically view all your runs, their hparams and results.</li>\n<li>Use SDK to query/access all your runs and tracked metadata.</li>\n<li>You own your data - Aim is open source and self hosted.</li>\n</ul>\n<h1>Demos</h1>\n<p>| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |</p>\n<p>| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |</p>\n<h1>Quick Start</h1>\n<p>Follow the steps below to get started with Aim.</p>\n<p><strong>1. Install Aim on your training environment</strong></p>\n<pre><code class=\"language-shell\">pip3 install aim\n</code></pre>\n<p><strong>2. Integrate Aim with your code</strong></p>\n<pre><code class=\"language-python\">from aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n</code></pre>\n<p><em>See the full list of supported trackable objects(e.g. images, text, etc) <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html\">here</a>.</em></p>\n<p><strong>3. Run the training as usual and start Aim UI</strong></p>\n<pre><code class=\"language-shell\">aim up\n</code></pre>\n<p><strong>4. Or query runs programmatically via SDK</strong></p>\n<pre><code class=\"language-python\">from aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\n</code></pre>\n<h1>Integrations</h1>\n<pre><code class=\"language-python\">from aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face\">here</a>.</em></p>\n<pre><code class=\"language-python\">import aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite\">here</a>.</em></p>\n<h1>Comparisons to familiar tools</h1>\n<h3>Tensorboard</h3>\n<p><strong>Training run comparison</strong></p>\n<p>Order of magnitude faster training run comparison with Aim</p>\n<ul>\n<li>The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.</li>\n<li>With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. <strong>TensorBoard doesn't have features to group, aggregate the metrics</strong></li>\n</ul>\n<p><strong>Scalability</strong></p>\n<ul>\n<li>Aim is built to handle 1000s of training runs - both on the backend and on the UI.</li>\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\n</ul>\n<p><strong>Beloved TB visualizations to be added on Aim</strong></p>\n<ul>\n<li>Embedding projector.</li>\n<li>Neural network visualization.</li>\n</ul>\n<h3>MLFlow</h3>\n<p>MLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\n<p><strong>Run comparison</strong></p>\n<ul>\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.</li>\n<li>MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.</li>\n</ul>\n<p><strong>UI Scalability</strong></p>\n<ul>\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\n</ul>\n<h3>Weights and Biases</h3>\n<p>Hosted vs self-hosted</p>\n<ul>\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\n</ul>\n<h1>Roadmap</h1>\n<h2>Detailed Sprints</h2>\n<p>:sparkle: The <a href=\"https://github.com/orgs/aimhubio/projects/3\">Aim product roadmap</a></p>\n<ul>\n<li>The <code>Backlog</code> contains the issues we are going to choose from and prioritize weekly</li>\n<li>The issues are mainly prioritized by the highly-requested features</li>\n</ul>\n<h2>High-level roadmap</h2>\n<p>The high-level features we are going to work on the next few months</p>\n<h3>Done</h3>\n<ul>\n<li>[x] Live updates (Shipped: <em>Oct 18 2021</em>)</li>\n<li>[x] Images tracking and visualization (Start: <em>Oct 18 2021</em>, Shipped: <em>Nov 19 2021</em>)</li>\n<li>[x] Distributions tracking and visualization (Start: <em>Nov 10 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Jupyter integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Audio tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Transcripts tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Plotly integration (Start: <em>Dec 1 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Colab integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Centralized tracking server (Start: <em>Oct 18 2021</em>, Shipped: <em>Jan 22 2022</em>)</li>\n<li>[x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: <em>Dec 17 2021</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] Track git info, env vars, CLI arguments, dependencies (Start: <em>Jan 17 2022</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Activeloop Hub integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] PyTorch-Ignite integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Run summary and overview info(system params, CLI args, git info, ...) (Start: <em>Feb 14 2022</em>, Shipped: <em>Mar 9 2022</em>)</li>\n<li>[x] Add DVC related metadata into aim run (Start: <em>Mar 7 2022</em>, Shipped: <em>Mar 26 2022</em>)</li>\n<li>[x] Ability to attach notes to Run from UI (Start: <em>Mar 7 2022</em>, Shipped: <em>Apr 29 2022</em>)</li>\n<li>[x] Fairseq integration (Start: <em>Mar 27 2022</em>, Shipped: <em>Mar 29 2022</em>)</li>\n<li>[x] LightGBM integration (Start: <em>Apr 14 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] CatBoost integration (Start: <em>Apr 20 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Run execution details(display stdout/stderr logs) (Start: <em>Apr 25 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Long sequences(up to 5M of steps) support (Start: <em>Apr 25 2022</em>, Shipped: <em>Jun 22 2022</em>)</li>\n<li>[x] Figures Explorer (Start: <em>Mar 1 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Notify on stuck runs (Start: <em>Jul 22 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with KerasTuner (Start: <em>Aug 10 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with WandB (Start: <em>Aug 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Stable remote tracking server (Start: <em>Jun 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with fast.ai (Start: <em>Aug 22 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Integration with MXNet (Start: <em>Sep 20 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Project overview page (Start: <em>Sep 1 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Remote tracking server scaling (Start: <em>Sep 11 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with PaddlePaddle (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with Optuna (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Audios Explorer (Start: <em>Oct 30 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Experiment page (Start: <em>Nov 9 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n</ul>\n<h3>In Progress</h3>\n<ul>\n<li>[ ] Aim SDK low-level interface (Start: <em>Aug 22 2022</em>, )</li>\n<li>[ ] HuggingFace datasets (Start: <em>Dec 29 2022</em>, )</li>\n</ul>\n<h3>To Do</h3>\n<p><strong>Aim UI</strong></p>\n<ul>\n<li>Runs management\n<ul>\n<li>Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard</li>\n</ul>\n</li>\n<li>Explorers\n<ul>\n<li>Text Explorer</li>\n<li>Distributions Explorer</li>\n</ul>\n</li>\n<li>Dashboards – customizable layouts with embedded explorers</li>\n</ul>\n<p><strong>SDK and Storage</strong></p>\n<ul>\n<li>Scalability\n<ul>\n<li>Smooth UI and SDK experience with over 10.000 runs</li>\n</ul>\n</li>\n<li>Runs management\n<ul>\n<li>CLI interfaces\n<ul>\n<li>Reporting - runs summary and run details in a CLI compatible format</li>\n<li>Manipulations – copy, move, delete runs, params and sequences</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Integrations</strong></p>\n<ul>\n<li>ML Frameworks:\n<ul>\n<li>Shortlist: MONAI, SpaCy, Raytune</li>\n</ul>\n</li>\n<li>Resource management tools\n<ul>\n<li>Shortlist: Kubeflow, Slurm</li>\n</ul>\n</li>\n<li>Workflow orchestration tools</li>\n<li>Others: Hydra, Google MLMD, Streamlit, ...</li>\n</ul>\n<h3>On hold</h3>\n<ul>\n<li>scikit-learn integration</li>\n<li>Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: <em>Mar 21 2022</em>)</li>\n<li>Artifact storage – store files, model checkpoints, and beyond (Start: <em>Mar 21 2022</em>)</li>\n</ul>\n<h2>Community</h2>\n<h3>If you have questions</h3>\n<ol>\n<li><a href=\"https://aimstack.readthedocs.io/en/latest/\">Read the docs</a></li>\n<li><a href=\"https://github.com/aimhubio/aim/issues\">Open a feature request or report a bug</a></li>\n<li><a href=\"https://community.aimstack.io/\">Join Discord community server</a></li>\n</ol>\n<p><a href=\"\"><img src=\"https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue\" alt=\"Platform Support\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/pyversions/aim\" alt=\"PyPI - Python Version\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/v/aim?color=yellow\" alt=\"PyPI Package\"></a>\r\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-orange.svg\" alt=\"License\"></a>\r\n<a href=\"https://pypi.org/project/aim/\"><img src=\"https://img.shields.io/pypi/dw/aim?color=green\" alt=\"PyPI Downloads\"></a>\r\n<a href=\"http://github.com/aimhubio/aim/issues\"><img src=\"https://img.shields.io/github/issues/aimhubio/aim\" alt=\"Issues\"></a></p>\n<h1>About Aim</h1>\n<p>| Track and version ML runs | Visualize runs via beautiful UI | Query runs metadata via SDK |\r\n|:--------------------:|:------------------------:|:-------------------:|\r\n|  |  |  |</p>\n<p>Aim is an open-source, self-hosted ML experiment tracking tool.\r\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.</p>\n<p>You can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically.\r\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.</p>\n<p>Aim's mission is to democratize AI dev tools.</p>\n<h1>Why use Aim?</h1>\n<h3>Compare 100s of runs in a few clicks - build models faster</h3>\n<ul>\n<li>Compare, group and aggregate 100s of metrics thanks to effective visualizations.</li>\n<li>Analyze, learn correlations and patterns between hparams and metrics.</li>\n<li>Easy pythonic search to query the runs you want to explore.</li>\n</ul>\n<h3>Deep dive into details of each run for easy debugging</h3>\n<ul>\n<li>Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.</li>\n<li>Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.</li>\n<li>Analyze system resource usage to effectively utilize computational resources.</li>\n</ul>\n<h3>Have all relevant information organised and accessible for easy governance</h3>\n<ul>\n<li>Centralized dashboard to holistically view all your runs, their hparams and results.</li>\n<li>Use SDK to query/access all your runs and tracked metadata.</li>\n<li>You own your data - Aim is open source and self hosted.</li>\n</ul>\n<h1>Demos</h1>\n<p>| Machine translation | lightweight-GAN |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of a neural translation model(from WMT'19 competition). | Training logs of 'lightweight' GAN, proposed in ICLR 2021. |</p>\n<p>| FastSpeech 2 | Simple MNIST |\r\n|:---:|:---:|\r\n|    |    |\r\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\". | Simple MNIST training logs. |</p>\n<h1>Quick Start</h1>\n<p>Follow the steps below to get started with Aim.</p>\n<p><strong>1. Install Aim on your training environment</strong></p>\n<pre><code class=\"language-shell\">pip3 install aim\n</code></pre>\n<p><strong>2. Integrate Aim with your code</strong></p>\n<pre><code class=\"language-python\">from aim import Run\r\n\r\n# Initialize a new run\r\nrun = Run()\r\n\r\n# Log run parameters\r\nrun[\"hparams\"] = {\r\n    \"learning_rate\": 0.001,\r\n    \"batch_size\": 32,\r\n}\r\n\r\n# Log metrics\r\nfor i in range(10):\r\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\r\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n</code></pre>\n<p><em>See the full list of supported trackable objects(e.g. images, text, etc) <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html\">here</a>.</em></p>\n<p><strong>3. Run the training as usual and start Aim UI</strong></p>\n<pre><code class=\"language-shell\">aim up\n</code></pre>\n<p><strong>4. Or query runs programmatically via SDK</strong></p>\n<pre><code class=\"language-python\">from aim import Repo\r\n\r\nmy_repo = Repo('/path/to/aim/repo')\r\n\r\nquery = \"metric.name == 'loss'\" # Example query\r\n\r\n# Get collection of metrics\r\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\r\n    for metric in run_metrics_collection:\r\n        # Get run params\r\n        params = metric.run[...]\r\n        # Get metric values\r\n        steps, metric_values = metric.values.sparse_numpy()\n</code></pre>\n<h1>Integrations</h1>\n<pre><code class=\"language-python\">from aim.pytorch_lightning import AimLogger\r\n\r\n# ...\r\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.hugging_face import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset if training_args.do_train else None,\r\n    eval_dataset=eval_dataset if training_args.do_eval else None,\r\n    callbacks=[aim_callback],\r\n    # ...\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face\">here</a>.</em></p>\n<pre><code class=\"language-python\">import aim\r\n\r\n# ...\r\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\r\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n    \r\n    # Use aim.tensorflow.AimCallback in case of tf.keras\r\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\n])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.keras_tuner import AimCallback\r\n\r\n# ...\r\ntuner.search(\r\n    train_ds,\r\n    validation_data=test_ds,\r\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.xgboost import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\r\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.catboost import AimLogger\r\n\r\n# ...\r\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.fastai import AimCallback\r\n\r\n# ...\r\nlearn = cnn_learner(dls, resnet18, pretrained=True,\r\n                    loss_func=CrossEntropyLossFlat(),\r\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\r\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.lightgbm import AimCallback\r\n\r\n# ...\r\naim_callback = AimCallback(experiment='lgb_test')\r\naim_callback.experiment['hparams'] = params\r\n\r\ngbm = lgb.train(params,\r\n                lgb_train,\r\n                num_boost_round=20,\r\n                valid_sets=lgb_eval,\r\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.pytorch_ignite import AimLogger\r\n\r\n# ...\r\naim_logger = AimLogger()\r\n\r\naim_logger.log_params({\r\n    \"model\": model.__class__.__name__,\r\n    \"pytorch_version\": str(torch.__version__),\r\n    \"ignite_version\": str(ignite.__version__),\r\n})\r\n\r\naim_logger.attach_output_handler(\r\n    trainer,\r\n    event_name=Events.ITERATION_COMPLETED,\r\n    tag=\"train\",\r\n    output_transform=lambda loss: {'loss': loss}\r\n)\r\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite\">here</a>.</em></p>\n<h1>Comparisons to familiar tools</h1>\n<h3>Tensorboard</h3>\n<p><strong>Training run comparison</strong></p>\n<p>Order of magnitude faster training run comparison with Aim</p>\n<ul>\n<li>The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.</li>\n<li>With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. <strong>TensorBoard doesn't have features to group, aggregate the metrics</strong></li>\n</ul>\n<p><strong>Scalability</strong></p>\n<ul>\n<li>Aim is built to handle 1000s of training runs - both on the backend and on the UI.</li>\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\n</ul>\n<p><strong>Beloved TB visualizations to be added on Aim</strong></p>\n<ul>\n<li>Embedding projector.</li>\n<li>Neural network visualization.</li>\n</ul>\n<h3>MLFlow</h3>\n<p>MLFlow is an end-to-end ML Lifecycle tool.\r\nAim is focused on training tracking.\r\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\n<p><strong>Run comparison</strong></p>\n<ul>\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.</li>\n<li>MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.</li>\n</ul>\n<p><strong>UI Scalability</strong></p>\n<ul>\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\n</ul>\n<h3>Weights and Biases</h3>\n<p>Hosted vs self-hosted</p>\n<ul>\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\n</ul>\n<h1>Roadmap</h1>\n<h2>Detailed Sprints</h2>\n<p>:sparkle: The <a href=\"https://github.com/orgs/aimhubio/projects/3\">Aim product roadmap</a></p>\n<ul>\n<li>The <code>Backlog</code> contains the issues we are going to choose from and prioritize weekly</li>\n<li>The issues are mainly prioritized by the highly-requested features</li>\n</ul>\n<h2>High-level roadmap</h2>\n<p>The high-level features we are going to work on the next few months</p>\n<h3>Done</h3>\n<ul>\n<li>[x] Live updates (Shipped: <em>Oct 18 2021</em>)</li>\n<li>[x] Images tracking and visualization (Start: <em>Oct 18 2021</em>, Shipped: <em>Nov 19 2021</em>)</li>\n<li>[x] Distributions tracking and visualization (Start: <em>Nov 10 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Jupyter integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>[x] Audio tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Transcripts tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Plotly integration (Start: <em>Dec 1 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Colab integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>[x] Centralized tracking server (Start: <em>Oct 18 2021</em>, Shipped: <em>Jan 22 2022</em>)</li>\n<li>[x] Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: <em>Dec 17 2021</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] Track git info, env vars, CLI arguments, dependencies (Start: <em>Jan 17 2022</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>[x] MLFlow adaptor (visualize MLflow logs with Aim) (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Activeloop Hub integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] PyTorch-Ignite integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>[x] Run summary and overview info(system params, CLI args, git info, ...) (Start: <em>Feb 14 2022</em>, Shipped: <em>Mar 9 2022</em>)</li>\n<li>[x] Add DVC related metadata into aim run (Start: <em>Mar 7 2022</em>, Shipped: <em>Mar 26 2022</em>)</li>\n<li>[x] Ability to attach notes to Run from UI (Start: <em>Mar 7 2022</em>, Shipped: <em>Apr 29 2022</em>)</li>\n<li>[x] Fairseq integration (Start: <em>Mar 27 2022</em>, Shipped: <em>Mar 29 2022</em>)</li>\n<li>[x] LightGBM integration (Start: <em>Apr 14 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] CatBoost integration (Start: <em>Apr 20 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Run execution details(display stdout/stderr logs) (Start: <em>Apr 25 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>[x] Long sequences(up to 5M of steps) support (Start: <em>Apr 25 2022</em>, Shipped: <em>Jun 22 2022</em>)</li>\n<li>[x] Figures Explorer (Start: <em>Mar 1 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Notify on stuck runs (Start: <em>Jul 22 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with KerasTuner (Start: <em>Aug 10 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with WandB (Start: <em>Aug 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Stable remote tracking server (Start: <em>Jun 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>[x] Integration with fast.ai (Start: <em>Aug 22 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Integration with MXNet (Start: <em>Sep 20 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Project overview page (Start: <em>Sep 1 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>[x] Remote tracking server scaling (Start: <em>Sep 11 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with PaddlePaddle (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Integration with Optuna (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Audios Explorer (Start: <em>Oct 30 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>[x] Experiment page (Start: <em>Nov 9 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n</ul>\n<h3>In Progress</h3>\n<ul>\n<li>[ ] Aim SDK low-level interface (Start: <em>Aug 22 2022</em>, )</li>\n<li>[ ] HuggingFace datasets (Start: <em>Dec 29 2022</em>, )</li>\n</ul>\n<h3>To Do</h3>\n<p><strong>Aim UI</strong></p>\n<ul>\n<li>Runs management\n<ul>\n<li>Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard</li>\n</ul>\n</li>\n<li>Explorers\n<ul>\n<li>Text Explorer</li>\n<li>Distributions Explorer</li>\n</ul>\n</li>\n<li>Dashboards – customizable layouts with embedded explorers</li>\n</ul>\n<p><strong>SDK and Storage</strong></p>\n<ul>\n<li>Scalability\n<ul>\n<li>Smooth UI and SDK experience with over 10.000 runs</li>\n</ul>\n</li>\n<li>Runs management\n<ul>\n<li>CLI interfaces\n<ul>\n<li>Reporting - runs summary and run details in a CLI compatible format</li>\n<li>Manipulations – copy, move, delete runs, params and sequences</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Integrations</strong></p>\n<ul>\n<li>ML Frameworks:\n<ul>\n<li>Shortlist: MONAI, SpaCy, Raytune</li>\n</ul>\n</li>\n<li>Resource management tools\n<ul>\n<li>Shortlist: Kubeflow, Slurm</li>\n</ul>\n</li>\n<li>Workflow orchestration tools</li>\n<li>Others: Hydra, Google MLMD, Streamlit, ...</li>\n</ul>\n<h3>On hold</h3>\n<ul>\n<li>scikit-learn integration</li>\n<li>Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: <em>Mar 21 2022</em>)</li>\n<li>Artifact storage – store files, model checkpoints, and beyond (Start: <em>Mar 21 2022</em>)</li>\n</ul>\n<h2>Community</h2>\n<h3>If you have questions</h3>\n<ol>\n<li><a href=\"https://aimstack.readthedocs.io/en/latest/\">Read the docs</a></li>\n<li><a href=\"https://github.com/aimhubio/aim/issues\">Open a feature request or report a bug</a></li>\n<li><a href=\"https://community.aimstack.io/\">Join Discord community server</a></li>\n</ol>"},"_id":"posts/new-post.md","_raw":{"sourceFilePath":"posts/new-post.md","sourceFileName":"new-post.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/new-post"},"type":"Post"},{"title":"post with new markdown","date":"2023-01-25T14:33:02.283Z","author":"Ashot","description":"description","slug":"post-with-new-markdown","image":"/images/dynamic/video-thumbnail.png","draft":false,"categories":["test"],"body":{"raw":"b﻿arev dzez\n\n<div align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/13848158/154338760-edfe1885-06f3-4e02-87fe-4b13a403516b.png\"/>\n  <h3>An easy-to-use & supercharged open-source experiment tracker</h3>\n  Aim logs your training runs, enables a beautiful UI to compare them and an API to query them programmatically.\n</div>\n\n<br/>\n\n<img src=\"https://user-images.githubusercontent.com/13848158/154338753-34484cda-95b8-4da8-a610-7fdf198c05fd.png\"/>\n\n<p align=\"center\">\n  <a href=\"#about-aim\"><b>About</b></a> &bull;\n  <a href=\"#why-use-aim\"><b>Features</b></a> &bull;\n  <a href=\"#demos\"><b>Demos</b></a> &bull;\n  <a href=\"https://github.com/aimhubio/aim/tree/main/examples\"><b>Examples</b></a> &bull;\n  <a href=\"#quick-start\"><b>Quick Start</b></a> &bull;\n  <a href=\"https://aimstack.readthedocs.io/en/latest/\"><b>Documentation</b></a> &bull;\n  <a href=\"#roadmap\"><b>Roadmap</b></a> &bull;\n  <a href=\"https://community.aimstack.io/\"><b>Discord Community</b></a> &bull;\n  <a href=\"https://twitter.com/aimstackio\"><b>Twitter</b></a>\n</p>\n\n<div align=\"center\">\n  \n  \\[![Platform Support](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue)]()\n  \\[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aim)](https://pypi.org/project/aim/)\n  \\[![PyPI Package](https://img.shields.io/pypi/v/aim?color=yellow)](https://pypi.org/project/aim/)\n  \\[![License](https://img.shields.io/badge/License-Apache%202.0-orange.svg)](https://opensource.org/licenses/Apache-2.0)\n  \\[![PyPI Downloads](https://img.shields.io/pypi/dw/aim?color=green)](https://pypi.org/project/aim/)\n  \\[![Issues](https://img.shields.io/github/issues/aimhubio/aim)](http://github.com/aimhubio/aim/issues)\n  \n</div>\n\n<div align=\"center\">\n  <sub>Integrates seamlessly with your favorite tools</sub>\n  <br/>\n  <br/>\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354389-d0301620-77ea-4629-a743-f7aa249e14b5.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354496-b39d7b1c-63ef-40f0-9e59-c08d2c5e337c.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354380-3755c741-6960-42ca-b93e-84a8791f088c.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354342-7df0ef5e-63d2-4df7-b9f1-d2fc0e95f53f.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354392-afbff3de-c845-4d86-855d-53df569f91d1.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354355-89210506-e7e5-4d37-b2d6-ad3fda62ef13.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354397-8af8e1d3-4067-405e-9d42-1f131663ed22.png\" width=\"60\" />\n  <br/>\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354513-f7486146-3891-4f3f-934f-e58bbf9ce695.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354500-c0471ce6-b2ce-4172-b9e4-07a197256303.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354361-9f911785-008d-4b75-877e-651e026cf47e.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354373-1879ae61-b5d1-41f0-a4f1-04b639b6f05e.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354483-75d9853f-7154-4d95-8190-9ad7a73d6654.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354329-cf7c3352-a72a-478d-82a7-04e3833b03b7.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354349-dcdf3bc3-d7a9-4f34-8258-4824a57f59c7.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/13848158/155354471-518f1814-7a41-4b23-9caf-e516507343f1.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/48801049/165162736-2cc5da39-38aa-4093-874f-e56d0ba9cea2.png\" width=\"60\" />\n  <img src=\"https://user-images.githubusercontent.com/48801049/165074282-36ad18eb-1124-434d-8439-728c22cd7ac7.png\" width=\"60\" />\n</div>\n\n<div align=\"center\">\n  <br/>\n  <kbd>\n    <img width=\"650px\" src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" />\n  </kbd>\n</div>\n\n# About Aim\n\n| Track and version ML runs                                                                                                        | Visualize runs via beautiful UI                                                                                                  | Query runs metadata via SDK                                                                                                      |\n| -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n| <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337794-e9310239-6614-41b3-a95b-bb91f0bb6c4f.png\"/> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337788-03fe5b31-0fa3-44af-ae79-2861707d8602.png\"/> | <img width=\"600px\" src=\"https://user-images.githubusercontent.com/13848158/154337793-85175c78-5659-4dd0-bb2d-05017278e2fa.png\"/> |\n\nAim is an open-source, self-hosted ML experiment tracking tool. \nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.\n\nYou can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically. \nThat's especially useful for automations and additional analysis on a Jupyter Notebook.\n\nAim's mission is to democratize AI dev tools.\n\n# Why use Aim?\n\n### Compare 100s of runs in a few clicks - build models faster\n\n* Compare, group and aggregate 100s of metrics thanks to effective visualizations.\n* Analyze, learn correlations and patterns between hparams and metrics.\n* Easy pythonic search to query the runs you want to explore.\n\n### Deep dive into details of each run for easy debugging\n\n* Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.\n* Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.\n* Analyze system resource usage to effectively utilize computational resources.\n\n### Have all relevant information organised and accessible for easy governance\n\n* Centralized dashboard to holistically view all your runs, their hparams and results.\n* Use SDK to query/access all your runs and tracked metadata.\n* You own your data - Aim is open source and self hosted.\n\n# Demos\n\n| Machine translation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | lightweight-GAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <a href=\"http://play.aimstack.io:10001/metrics?grouping=HQGdK9Xxy35e6sY1CYkCmk1WbWMN2AsCNfJJ3d1RJYLtrVPMoF5UpGiA6CF8bEJnfzRsKpqespf3AEuKSVrhUYvYk9MxzNGA9XZWYUf6phEg8AMbZGLRVDXnAPDuo8tueqsST1ZLizWzQwDYJWHUza6pyB2Eojt9uWqNHUdb858TqDRnCJzqiVJXKXEzFWUyvU8MckJo1qpqWWCTb4GpYN6DUJZx2GXDGR21e2xxd4m7PmNUnbA9B3apLttZoipJF6c3v7tNUKmb6irpqnNB3yc57tqYDa1XZuKfDxkMtyFdQ1x95K4jjsTVwhftEWLze35QNcxNXRCGGS9o9yEfTLG26GUX2zjPZFCjjMGU6vV7z1xRccK8MyoGrLSgAQCbvk68dTGBHpXUBvCRq8N&chart=FviZzVrt4fVQPjpCLr9sVGGrcR5etSroyqambiKpm3nTgpyv4eQxKuwNX9uN8UtKmzYUhUyTMBEANHmtbwjLApkvnYeNbxGNC6PVcoqi65m1XJnSrvgt8WiD89BapFAWRUwAGx6SWD7KZPsk3RQyysU7W7FjD3Q99NusxFGhsEfD6HXc7i8xH9KHDRGjLwh6x9VTtSp4FS8HEvpLSiiJoX7LCTi8pB7dXvrQ8G5w3jPsFz4qXYFdsVaCNL1BpFFZuiqQNkfbnM84gEq7UmiV1VzM4oS3AgQHxADG3kpBVp6eKTey9F1Swd4FcUkFA9QEPjgQgqwRGjkquZ2bdDDVLBnCh7JPvboP2kifCiZZ5MDdV9MMx6PKHp4DusWyWLXiHQYPkpGPWBiuccMUXDsuJaCWJbuABdY7CyiJMv1jdHYkjabygSxehPVyEDefWAtjBfv2vaeM1xv63jadbmpKYFxft7qmuT9HvVxiGvRgs4RQFxy8K4rtFBca3HNs1mDaaY81gy9MGXyw7BS5Fniu92jaJpsWDdg6Y3AQBLZtrpJy2obEZ4yzJaCVT7JUNPAyyCUNLck393VFLoEkaD9CU5npK5R7tj1c1G3gkMNQXnSXy5NpSj8deMmXV5qz3JKu1nq2caGQKcqjzy2gLkExdm674AMFjSg9yFjK6VqASXQ17NKtWRUvaYoxGbHDAFQaMKWKh8QLm22QA9mKT8NksLptWozbgDvafnQLNMvezLU5bvKV5o75PAWYiRB56RcYfEhzaB6YWdgL7TJicyY5rFi6Az8UZ7wqB3N5iMuZdpxhKn5KbZDxyuUMuvVt24i5LVPPmmwQtqxMoJ4aLo48a2YvDW6TAkdQjNjvn6KcEEz6GTixujb1YHhMUD8v4AepWKEwKz1ddEca1P2wLQjbpihCuaqbxeohnuZZLogJdUBojBEDgrnrrVpPBaLLEkGSpkJbtrsKUuEeBo1AF3yNgHftLbynGpobVF5DhmsmddmiA6c8vSTokJxHhjpnW8mAcNHBRtmVJCT7VkdHSAhNypM4Hivwfx5jCccG9LauKmCeRMDzHiA57TX9W6ttcPHSvUyQorARQAd2oeNY4H83hZjHh9Bt8iwKZRt4xK6hrTR8tif7hq8eURXrGH9Ys7TzykXK8FHHWvLNzNnYf3E4a9NkD43MjfKvMM1hj4Q2K8MHbmRCqrmFrHP5kim9shq6mhLPTgwha32nvnrBkfPQVPwpGTzKuwE&select=CdsQ7jVNkogQhRzQR3e28Ek39AZ4Ma2y37k5zJaf9EZmQhMjy8GtGm4LGU6dRFuAVG7mYww5xDrQAE74KHQ3Kk1e6661RmcmNALAUjtHyCmrTVBMCnBGNiuq1y7EzmxoodYHU1BV1rnoefQAw2kTBtbWi11hV1P4LcwFCcXfUWF6rpRC7ehEnUCTqUV4bkGVJPLcmk9mdmiGwa2YgmnSShNGPVGZiEi1rMVECyngSRVdqdZwAeXBGWFLfqF1KbZeCo4MTF4SSmFupJ9zLhYbuojEbopyFWHQ6xs3sq9epPeaQziLM4Js7oFYRmuFWUYdFqnZngmewXWmi7tQAgVqhiT6dMjG2eTdfgX6WuRSuoHALkh2XJhHA6GfZLUcxC5Ni9YyKuBTamtaYarbNNJJ8z15WWvuUkLpjgHdEpE2h924xFdu8aoZNuiQxYGvcndaW1BTGMXS5fTKPqYfe2n8Ky2HWPkcX3hEXtyawu1F9BndKNaXLPgsdAoFBArBZnSe28YtSmTa5LRucKVBAxakvv5MWMXchAmpaGFQbZyYUoMgQLcJd7Y96x6zSR7nhwr5Ar81BrmqYz2WFLuk7osUbwsc9HbSG6CQt8p6Vg2u7DjKaZXW8pjkPHAKrHWtHEDiJPJ5rj6VsdFm3\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340796-c9e91b13-8ee0-4a67-bcde-8cf3aaa7ba99.jpg\"/> </a> | <a href=\"http://play.aimstack.io:10002/images?grouping=E1zQzcmtDR3wibEa1MVysTvCyZEv1T8ixkCxTWExCyMnHtX2HyiF9eszvPgfd2xdJ5TUTKGpSs1bsLVq5tHAV3uWtsZmmckn6HjNtVCMyQDJpwhiEy5tAyw&select=2NEXuD7fFoaLcwRjymjA1wLmUrGs9s3AiXcCW82C367SwJt18CAB6xzkMGowrUDuDwggE1huaPVcQJpQUsmAQx1CnGiqCUBp2jPMd5mMNPX2QKQMcmvu9ZykBNkeBvCQFPd9ERuQD2g1EjWuvyJ3H53mAZTfp94LCXvR9CUsG5ei2CjQUzfZLM6DCyUr1GPaEVnY5f1EwzicNxXuoutkBgqCqaobJ7Do4q4eHAA6ooiWU6ekS3D2sLj6qYwhVTjfGCPfbWwBiH83nFkY3fLExzdeTY2zeUHeeYikQR9S7xHbVD8WvjekdQVp8X4dNLJZxiVmEqHpPRnU3ZrYsMhE7yFAAgjJwPNUzLTt6YFrtZBcmc4rwAC2oyrqysUSEr6gzL6LcJ6yuqDGf9D5tzftHbTLDkhc8B2sCgTS&images=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\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340790-bc7b7a21-e8a1-43a1-809d-4060b5bfb60f.jpg\"> </a> |\n| Training logs of a neural translation model(from WMT'19 competition).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Training logs of 'lightweight' GAN, proposed in ICLR 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n\n| FastSpeech 2                                                                                                                                                                                                        | Simple MNIST                                                                                                                                                                                                               |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <a href=\"http://play.aimstack.io:10004/runs/d9e89aa7875e44b2ba85612a/audios\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340778-dbe19620-2f27-4298-b0cb-caf3904760f1.jpg\"/> </a> | <a href=\"http://play.aimstack.io:10003/runs/7f083da898624a2c98e0f363/distributions\"> <img width=\"800px\" src=\"https://user-images.githubusercontent.com/13848158/154340785-a7e4d9fd-d048-4207-8cd1-c4edff9cca6a.jpg\"/> </a> |\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\".                                                                                                                       | Simple MNIST training logs.                                                                                                                                                                                                |\n\n# Quick Start\n\nFollow the steps below to get started with Aim.\n\n**1. Install Aim on your training environment**\n\n```shell\npip3 install aim\n```\n\n**2. Integrate Aim with your code**\n\n```python\nfrom aim import Run\n\n# Initialize a new run\nrun = Run()\n\n# Log run parameters\nrun[\"hparams\"] = {\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n}\n\n# Log metrics\nfor i in range(10):\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n```\n\n*See the full list of supported trackable objects(e.g. images, text, etc) [here](https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html).*\n\n**3. Run the training as usual and start Aim UI**\n\n```shell\naim up\n```\n\n**4. Or query runs programmatically via SDK**\n\n```python\nfrom aim import Repo\n\nmy_repo = Repo('/path/to/aim/repo')\n\nquery = \"metric.name == 'loss'\" # Example query\n\n# Get collection of metrics\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\n    for metric in run_metrics_collection:\n        # Get run params\n        params = metric.run[...]\n        # Get metric values\n        steps, metric_values = metric.values.sparse_numpy()\n```\n\n# Integrations\n\n<details>\n<summary>\n  Integrate PyTorch Lightning\n</summary>\n\n```python\nfrom aim.pytorch_lightning import AimLogger\n\n# ...\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning).*\n\n</details>\n\n<details>\n<summary>\n  Integrate Hugging Face\n</summary>\n\n```python\nfrom aim.hugging_face import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    callbacks=[aim_callback],\n    # ...\n)\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face).*\n\n</details>\n\n<details>\n<summary>\n  Integrate Keras & tf.keras\n</summary>\n\n```python\nimport aim\n\n# ...\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n    \n    # Use aim.tensorflow.AimCallback in case of tf.keras\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n])\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras).*\n\n</details>\n\n<details>\n<summary>\n  Integrate KerasTuner\n</summary>\n\n```python\nfrom aim.keras_tuner import AimCallback\n\n# ...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner).*\n\n</details>\n\n<details>\n<summary>\n  Integrate XGBoost\n</summary>\n\n```python\nfrom aim.xgboost import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost).*\n\n</details>\n\n<details>\n<summary>\n  Integrate CatBoost\n</summary>\n\n```python\nfrom aim.catboost import AimLogger\n\n# ...\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost).*\n\n</details>\n\n<details>\n<summary>\n  Integrate fastai\n</summary>\n\n```python\nfrom aim.fastai import AimCallback\n\n# ...\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai).*\n\n</details>\n\n<details>\n<summary>\n  Integrate LightGBM\n</summary>\n\n```python\nfrom aim.lightgbm import AimCallback\n\n# ...\naim_callback = AimCallback(experiment='lgb_test')\naim_callback.experiment['hparams'] = params\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm).*\n\n</details>\n\n<details>\n<summary>\n  Integrate PyTorch Ignite\n</summary>\n\n```python\nfrom aim.pytorch_ignite import AimLogger\n\n# ...\naim_logger = AimLogger()\n\naim_logger.log_params({\n    \"model\": model.__class__.__name__,\n    \"pytorch_version\": str(torch.__version__),\n    \"ignite_version\": str(ignite.__version__),\n})\n\naim_logger.attach_output_handler(\n    trainer,\n    event_name=Events.ITERATION_COMPLETED,\n    tag=\"train\",\n    output_transform=lambda loss: {'loss': loss}\n)\n# ...\n```\n\n*See documentation [here](https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite).*\n\n</details>\n\n# Comparisons to familiar tools\n\n### Tensorboard\n\n**Training run comparison**\n\nOrder of magnitude faster training run comparison with Aim\n\n* The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.\n* With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. **TensorBoard doesn't have features to group, aggregate the metrics**\n\n**Scalability**\n\n* Aim is built to handle 1000s of training runs - both on the backend and on the UI.\n* TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.\n\n**Beloved TB visualizations to be added on Aim**\n\n* Embedding projector.\n* Neural network visualization.\n\n### MLFlow\n\nMLFlow is an end-to-end ML Lifecycle tool.\nAim is focused on training tracking.\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.\n\n**Run comparison**\n\n* Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.\n* MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.\n\n**UI Scalability**\n\n* Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!\n* MLflow UI becomes slow to use when there are a few hundreds of runs.\n\n### Weights and Biases\n\nHosted vs self-hosted\n\n* Weights and Biases is a hosted closed-source MLOps platform.\n* Aim is self-hosted, free and open-source experiment tracking tool.\n\n# Roadmap\n\n## Detailed Sprints\n\n:sparkle: The [Aim product roadmap](https://github.com/orgs/aimhubio/projects/3)\n\n* The `Backlog` contains the issues we are going to choose from and prioritize weekly\n* The issues are mainly prioritized by the highly-requested features\n\n## High-level roadmap\n\nThe high-level features we are going to work on the next few months\n\n### Done\n\n* Live updates (Shipped: *Oct 18 2021*)\n* Images tracking and visualization (Start: *Oct 18 2021*, Shipped: *Nov 19 2021*)\n* Distributions tracking and visualization (Start: *Nov 10 2021*, Shipped: *Dec 3 2021*)\n* Jupyter integration (Start: *Nov 18 2021*, Shipped: *Dec 3 2021*)\n* Audio tracking and visualization (Start: *Dec 6 2021*, Shipped: *Dec 17 2021*)\n* Transcripts tracking and visualization (Start: *Dec 6 2021*, Shipped: *Dec 17 2021*)\n* Plotly integration (Start: *Dec 1 2021*, Shipped: *Dec 17 2021*)\n* Colab integration (Start: *Nov 18 2021*, Shipped: *Dec 17 2021*)\n* Centralized tracking server (Start: *Oct 18 2021*, Shipped: *Jan 22 2022*)\n* Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: *Dec 17 2021*, Shipped: *Feb 3 2022*)\n* Track git info, env vars, CLI arguments, dependencies (Start: *Jan 17 2022*, Shipped: *Feb 3 2022*)\n* MLFlow adaptor (visualize MLflow logs with Aim) (Start: *Feb 14 2022*, Shipped: *Feb 22 2022*)\n* Activeloop Hub integration (Start: *Feb 14 2022*, Shipped: *Feb 22 2022*)\n* PyTorch-Ignite integration (Start: *Feb 14 2022*, Shipped: *Feb 22 2022*)\n* Run summary and overview info(system params, CLI args, git info, ...) (Start: *Feb 14 2022*, Shipped: *Mar 9 2022*)\n* Add DVC related metadata into aim run (Start: *Mar 7 2022*, Shipped: *Mar 26 2022*)\n* Ability to attach notes to Run from UI (Start: *Mar 7 2022*, Shipped: *Apr 29 2022*)\n* Fairseq integration (Start: *Mar 27 2022*, Shipped: *Mar 29 2022*)\n* LightGBM integration (Start: *Apr 14 2022*, Shipped: *May 17 2022*)\n* CatBoost integration (Start: *Apr 20 2022*, Shipped: *May 17 2022*)\n* Run execution details(display stdout/stderr logs) (Start: *Apr 25 2022*, Shipped: *May 17 2022*)\n* Long sequences(up to 5M of steps) support (Start: *Apr 25 2022*, Shipped: *Jun 22 2022*)\n* Figures Explorer (Start: *Mar 1 2022*, Shipped: *Aug 21 2022*)\n* Notify on stuck runs (Start: *Jul 22 2022*, Shipped: *Aug 21 2022*)\n* Integration with KerasTuner (Start: *Aug 10 2022*, Shipped: *Aug 21 2022*)\n* Integration with WandB (Start: *Aug 15 2022*, Shipped: *Aug 21 2022*)\n* Stable remote tracking server (Start: *Jun 15 2022*, Shipped: *Aug 21 2022*)\n* Integration with fast.ai (Start: *Aug 22 2022*, Shipped: *Oct 6 2022*)\n* Integration with MXNet (Start: *Sep 20 2022*, Shipped: *Oct 6 2022*)\n* Project overview page (Start: *Sep 1 2022*, Shipped: *Oct 6 2022*)\n* Remote tracking server scaling (Start: *Sep 11 2022*, Shipped: *Nov 26 2022*)\n* Integration with PaddlePaddle (Start: *Oct 2 2022*, Shipped: *Nov 26 2022*)\n* Integration with Optuna (Start: *Oct 2 2022*, Shipped: *Nov 26 2022*)\n* Audios Explorer (Start: *Oct 30 2022*, Shipped: *Nov 26 2022*)\n* Experiment page (Start: *Nov 9 2022*, Shipped: *Nov 26 2022*)\n\n### In Progress\n\n* Aim SDK low-level interface (Start: *Aug 22 2022*, )\n* HuggingFace datasets (Start: *Dec 29 2022*, )\n\n### To Do\n\n**Aim UI**\n\n* Runs management\n\n  * Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard\n* Explorers\n\n  * Text Explorer\n  * Distributions Explorer\n* Dashboards – customizable layouts with embedded explorers\n\n**SDK and Storage**\n\n* Scalability\n\n  * Smooth UI and SDK experience with over 10.000 runs\n* Runs management\n\n  * CLI interfaces\n\n    * Reporting - runs summary and run details in a CLI compatible format\n    * Manipulations – copy, move, delete runs, params and sequences\n\n**Integrations**\n\n* ML Frameworks:\n\n  * Shortlist: MONAI, SpaCy, Raytune\n* Resource management tools\n\n  * Shortlist: Kubeflow, Slurm\n* Workflow orchestration tools\n* Others: Hydra, Google MLMD, Streamlit, ...\n\n### On hold\n\n* scikit-learn integration\n* Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: *Mar 21 2022*)\n* Artifact storage – store files, model checkpoints, and beyond (Start: *Mar 21 2022*)\n\n## Community\n\n### If you have questions\n\n1. [Read the docs](https://aimstack.readthedocs.io/en/latest/)\n2. [Open a feature request or report a bug](https://github.com/aimhubio/aim/issues)\n3. [Join Discord community server](https://community.aimstack.io/)\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_26BhViw28s\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n","html":"<p>b﻿arev dzez</p>\n<p>[<img src=\"https://img.shields.io/badge/platform-Linux%20%7C%20macOS-blue\" alt=\"Platform Support\">]()\n[<img src=\"https://img.shields.io/pypi/pyversions/aim\" alt=\"PyPI - Python Version\">](https://pypi.org/project/aim/)\n[<img src=\"https://img.shields.io/pypi/v/aim?color=yellow\" alt=\"PyPI Package\">](https://pypi.org/project/aim/)\n[<img src=\"https://img.shields.io/badge/License-Apache%202.0-orange.svg\" alt=\"License\">](https://opensource.org/licenses/Apache-2.0)\n[<img src=\"https://img.shields.io/pypi/dw/aim?color=green\" alt=\"PyPI Downloads\">](https://pypi.org/project/aim/)\n[<img src=\"https://img.shields.io/github/issues/aimhubio/aim\" alt=\"Issues\">](http://github.com/aimhubio/aim/issues)</p>\n<h1>About Aim</h1>\n<p>| Track and version ML runs                                                                                                        | Visualize runs via beautiful UI                                                                                                  | Query runs metadata via SDK                                                                                                      |\n| -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n|  |  |  |</p>\n<p>Aim is an open-source, self-hosted ML experiment tracking tool.\nIt's good at tracking lots (1000s) of training runs and it allows you to compare them with a performant and beautiful UI.</p>\n<p>You can use not only the great Aim UI but also its SDK to query your runs' metadata programmatically.\nThat's especially useful for automations and additional analysis on a Jupyter Notebook.</p>\n<p>Aim's mission is to democratize AI dev tools.</p>\n<h1>Why use Aim?</h1>\n<h3>Compare 100s of runs in a few clicks - build models faster</h3>\n<ul>\n<li>Compare, group and aggregate 100s of metrics thanks to effective visualizations.</li>\n<li>Analyze, learn correlations and patterns between hparams and metrics.</li>\n<li>Easy pythonic search to query the runs you want to explore.</li>\n</ul>\n<h3>Deep dive into details of each run for easy debugging</h3>\n<ul>\n<li>Hyperparameters, metrics, images, distributions, audio, text - all available at hand on an intuitive UI to understand the performance of your model.</li>\n<li>Easily track plots built via your favourite visualisation tools, like plotly and matplotlib.</li>\n<li>Analyze system resource usage to effectively utilize computational resources.</li>\n</ul>\n<h3>Have all relevant information organised and accessible for easy governance</h3>\n<ul>\n<li>Centralized dashboard to holistically view all your runs, their hparams and results.</li>\n<li>Use SDK to query/access all your runs and tracked metadata.</li>\n<li>You own your data - Aim is open source and self hosted.</li>\n</ul>\n<h1>Demos</h1>\n<p>| Machine translation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | lightweight-GAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|    |    |\n| Training logs of a neural translation model(from WMT'19 competition).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Training logs of 'lightweight' GAN, proposed in ICLR 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |</p>\n<p>| FastSpeech 2                                                                                                                                                                                                        | Simple MNIST                                                                                                                                                                                                               |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|    |    |\n| Training logs of Microsoft's \"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech\".                                                                                                                       | Simple MNIST training logs.                                                                                                                                                                                                |</p>\n<h1>Quick Start</h1>\n<p>Follow the steps below to get started with Aim.</p>\n<p><strong>1. Install Aim on your training environment</strong></p>\n<pre><code class=\"language-shell\">pip3 install aim\n</code></pre>\n<p><strong>2. Integrate Aim with your code</strong></p>\n<pre><code class=\"language-python\">from aim import Run\n\n# Initialize a new run\nrun = Run()\n\n# Log run parameters\nrun[\"hparams\"] = {\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n}\n\n# Log metrics\nfor i in range(10):\n    run.track(i, name='loss', step=i, context={ \"subset\":\"train\" })\n    run.track(i, name='acc', step=i, context={ \"subset\":\"train\" })\n</code></pre>\n<p><em>See the full list of supported trackable objects(e.g. images, text, etc) <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/supported_types.html\">here</a>.</em></p>\n<p><strong>3. Run the training as usual and start Aim UI</strong></p>\n<pre><code class=\"language-shell\">aim up\n</code></pre>\n<p><strong>4. Or query runs programmatically via SDK</strong></p>\n<pre><code class=\"language-python\">from aim import Repo\n\nmy_repo = Repo('/path/to/aim/repo')\n\nquery = \"metric.name == 'loss'\" # Example query\n\n# Get collection of metrics\nfor run_metrics_collection in my_repo.query_metrics(query).iter_runs():\n    for metric in run_metrics_collection:\n        # Get run params\n        params = metric.run[...]\n        # Get metric values\n        steps, metric_values = metric.values.sparse_numpy()\n</code></pre>\n<h1>Integrations</h1>\n<pre><code class=\"language-python\">from aim.pytorch_lightning import AimLogger\n\n# ...\ntrainer = pl.Trainer(logger=AimLogger(experiment='experiment_name'))\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-lightning\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.hugging_face import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='mnli')\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    callbacks=[aim_callback],\n    # ...\n)\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-hugging-face\">here</a>.</em></p>\n<pre><code class=\"language-python\">import aim\n\n# ...\nmodel.fit(x_train, y_train, epochs=epochs, callbacks=[\n    aim.keras.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n    \n    # Use aim.tensorflow.AimCallback in case of tf.keras\n    aim.tensorflow.AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\n])\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-keras-tf-keras\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.keras_tuner import AimCallback\n\n# ...\ntuner.search(\n    train_ds,\n    validation_data=test_ds,\n    callbacks=[AimCallback(tuner=tuner, repo='.', experiment='keras_tuner_test')],\n)\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-kerastuner\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.xgboost import AimCallback\n\n# ...\naim_callback = AimCallback(repo='/path/to/logs/dir', experiment='experiment_name')\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[aim_callback])\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-xgboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.catboost import AimLogger\n\n# ...\nmodel.fit(train_data, train_labels, log_cout=AimLogger(loss_function='Logloss'), logging_level=\"Info\")\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-catboost\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.fastai import AimCallback\n\n# ...\nlearn = cnn_learner(dls, resnet18, pretrained=True,\n                    loss_func=CrossEntropyLossFlat(),\n                    metrics=accuracy, model_dir=\"/tmp/model/\",\n                    cbs=AimCallback(repo='.', experiment='fastai_test'))\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-fastai\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.lightgbm import AimCallback\n\n# ...\naim_callback = AimCallback(experiment='lgb_test')\naim_callback.experiment['hparams'] = params\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                callbacks=[aim_callback, lgb.early_stopping(stopping_rounds=5)])\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-lightgbm\">here</a>.</em></p>\n<pre><code class=\"language-python\">from aim.pytorch_ignite import AimLogger\n\n# ...\naim_logger = AimLogger()\n\naim_logger.log_params({\n    \"model\": model.__class__.__name__,\n    \"pytorch_version\": str(torch.__version__),\n    \"ignite_version\": str(ignite.__version__),\n})\n\naim_logger.attach_output_handler(\n    trainer,\n    event_name=Events.ITERATION_COMPLETED,\n    tag=\"train\",\n    output_transform=lambda loss: {'loss': loss}\n)\n# ...\n</code></pre>\n<p><em>See documentation <a href=\"https://aimstack.readthedocs.io/en/latest/quick_start/integrations.html#integration-with-pytorch-ignite\">here</a>.</em></p>\n<h1>Comparisons to familiar tools</h1>\n<h3>Tensorboard</h3>\n<p><strong>Training run comparison</strong></p>\n<p>Order of magnitude faster training run comparison with Aim</p>\n<ul>\n<li>The tracked params are first class citizens at Aim. You can search, group, aggregate via params - deeply explore all the tracked data (metrics, params, images) on the UI.</li>\n<li>With tensorboard the users are forced to record those parameters in the training run name to be able to search and compare. This causes a super-tedius comparison experience and usability issues on the UI when there are many experiments and params. <strong>TensorBoard doesn't have features to group, aggregate the metrics</strong></li>\n</ul>\n<p><strong>Scalability</strong></p>\n<ul>\n<li>Aim is built to handle 1000s of training runs - both on the backend and on the UI.</li>\n<li>TensorBoard becomes really slow and hard to use when a few hundred training runs are queried / compared.</li>\n</ul>\n<p><strong>Beloved TB visualizations to be added on Aim</strong></p>\n<ul>\n<li>Embedding projector.</li>\n<li>Neural network visualization.</li>\n</ul>\n<h3>MLFlow</h3>\n<p>MLFlow is an end-to-end ML Lifecycle tool.\nAim is focused on training tracking.\nThe main differences of Aim and MLflow are around the UI scalability and run comparison features.</p>\n<p><strong>Run comparison</strong></p>\n<ul>\n<li>Aim treats tracked parameters as first-class citizens. Users can query runs, metrics, images and filter using the params.</li>\n<li>MLFlow does have a search by tracked config, but there are no grouping, aggregation, subplotting by hyparparams and other comparison features available.</li>\n</ul>\n<p><strong>UI Scalability</strong></p>\n<ul>\n<li>Aim UI can handle several thousands of metrics at the same time smoothly with 1000s of steps. It may get shaky when you explore 1000s of metrics with 10000s of steps each. But we are constantly optimizing!</li>\n<li>MLflow UI becomes slow to use when there are a few hundreds of runs.</li>\n</ul>\n<h3>Weights and Biases</h3>\n<p>Hosted vs self-hosted</p>\n<ul>\n<li>Weights and Biases is a hosted closed-source MLOps platform.</li>\n<li>Aim is self-hosted, free and open-source experiment tracking tool.</li>\n</ul>\n<h1>Roadmap</h1>\n<h2>Detailed Sprints</h2>\n<p>:sparkle: The <a href=\"https://github.com/orgs/aimhubio/projects/3\">Aim product roadmap</a></p>\n<ul>\n<li>The <code>Backlog</code> contains the issues we are going to choose from and prioritize weekly</li>\n<li>The issues are mainly prioritized by the highly-requested features</li>\n</ul>\n<h2>High-level roadmap</h2>\n<p>The high-level features we are going to work on the next few months</p>\n<h3>Done</h3>\n<ul>\n<li>Live updates (Shipped: <em>Oct 18 2021</em>)</li>\n<li>Images tracking and visualization (Start: <em>Oct 18 2021</em>, Shipped: <em>Nov 19 2021</em>)</li>\n<li>Distributions tracking and visualization (Start: <em>Nov 10 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>Jupyter integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 3 2021</em>)</li>\n<li>Audio tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Transcripts tracking and visualization (Start: <em>Dec 6 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Plotly integration (Start: <em>Dec 1 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Colab integration (Start: <em>Nov 18 2021</em>, Shipped: <em>Dec 17 2021</em>)</li>\n<li>Centralized tracking server (Start: <em>Oct 18 2021</em>, Shipped: <em>Jan 22 2022</em>)</li>\n<li>Tensorboard adaptor - visualize TensorBoard logs with Aim (Start: <em>Dec 17 2021</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>Track git info, env vars, CLI arguments, dependencies (Start: <em>Jan 17 2022</em>, Shipped: <em>Feb 3 2022</em>)</li>\n<li>MLFlow adaptor (visualize MLflow logs with Aim) (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>Activeloop Hub integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>PyTorch-Ignite integration (Start: <em>Feb 14 2022</em>, Shipped: <em>Feb 22 2022</em>)</li>\n<li>Run summary and overview info(system params, CLI args, git info, ...) (Start: <em>Feb 14 2022</em>, Shipped: <em>Mar 9 2022</em>)</li>\n<li>Add DVC related metadata into aim run (Start: <em>Mar 7 2022</em>, Shipped: <em>Mar 26 2022</em>)</li>\n<li>Ability to attach notes to Run from UI (Start: <em>Mar 7 2022</em>, Shipped: <em>Apr 29 2022</em>)</li>\n<li>Fairseq integration (Start: <em>Mar 27 2022</em>, Shipped: <em>Mar 29 2022</em>)</li>\n<li>LightGBM integration (Start: <em>Apr 14 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>CatBoost integration (Start: <em>Apr 20 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>Run execution details(display stdout/stderr logs) (Start: <em>Apr 25 2022</em>, Shipped: <em>May 17 2022</em>)</li>\n<li>Long sequences(up to 5M of steps) support (Start: <em>Apr 25 2022</em>, Shipped: <em>Jun 22 2022</em>)</li>\n<li>Figures Explorer (Start: <em>Mar 1 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Notify on stuck runs (Start: <em>Jul 22 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Integration with KerasTuner (Start: <em>Aug 10 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Integration with WandB (Start: <em>Aug 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Stable remote tracking server (Start: <em>Jun 15 2022</em>, Shipped: <em>Aug 21 2022</em>)</li>\n<li>Integration with fast.ai (Start: <em>Aug 22 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>Integration with MXNet (Start: <em>Sep 20 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>Project overview page (Start: <em>Sep 1 2022</em>, Shipped: <em>Oct 6 2022</em>)</li>\n<li>Remote tracking server scaling (Start: <em>Sep 11 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Integration with PaddlePaddle (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Integration with Optuna (Start: <em>Oct 2 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Audios Explorer (Start: <em>Oct 30 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n<li>Experiment page (Start: <em>Nov 9 2022</em>, Shipped: <em>Nov 26 2022</em>)</li>\n</ul>\n<h3>In Progress</h3>\n<ul>\n<li>Aim SDK low-level interface (Start: <em>Aug 22 2022</em>, )</li>\n<li>HuggingFace datasets (Start: <em>Dec 29 2022</em>, )</li>\n</ul>\n<h3>To Do</h3>\n<p><strong>Aim UI</strong></p>\n<ul>\n<li>\n<p>Runs management</p>\n<ul>\n<li>Runs explorer – query and visualize runs data(images, audio, distributions, ...) in a central dashboard</li>\n</ul>\n</li>\n<li>\n<p>Explorers</p>\n<ul>\n<li>Text Explorer</li>\n<li>Distributions Explorer</li>\n</ul>\n</li>\n<li>\n<p>Dashboards – customizable layouts with embedded explorers</p>\n</li>\n</ul>\n<p><strong>SDK and Storage</strong></p>\n<ul>\n<li>\n<p>Scalability</p>\n<ul>\n<li>Smooth UI and SDK experience with over 10.000 runs</li>\n</ul>\n</li>\n<li>\n<p>Runs management</p>\n<ul>\n<li>\n<p>CLI interfaces</p>\n<ul>\n<li>Reporting - runs summary and run details in a CLI compatible format</li>\n<li>Manipulations – copy, move, delete runs, params and sequences</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Integrations</strong></p>\n<ul>\n<li>\n<p>ML Frameworks:</p>\n<ul>\n<li>Shortlist: MONAI, SpaCy, Raytune</li>\n</ul>\n</li>\n<li>\n<p>Resource management tools</p>\n<ul>\n<li>Shortlist: Kubeflow, Slurm</li>\n</ul>\n</li>\n<li>\n<p>Workflow orchestration tools</p>\n</li>\n<li>\n<p>Others: Hydra, Google MLMD, Streamlit, ...</p>\n</li>\n</ul>\n<h3>On hold</h3>\n<ul>\n<li>scikit-learn integration</li>\n<li>Cloud storage support – store runs blob(e.g. images) data on the cloud (Start: <em>Mar 21 2022</em>)</li>\n<li>Artifact storage – store files, model checkpoints, and beyond (Start: <em>Mar 21 2022</em>)</li>\n</ul>\n<h2>Community</h2>\n<h3>If you have questions</h3>\n<ol>\n<li><a href=\"https://aimstack.readthedocs.io/en/latest/\">Read the docs</a></li>\n<li><a href=\"https://github.com/aimhubio/aim/issues\">Open a feature request or report a bug</a></li>\n<li><a href=\"https://community.aimstack.io/\">Join Discord community server</a></li>\n</ol>"},"_id":"posts/post-with-new-markdown.md","_raw":{"sourceFilePath":"posts/post-with-new-markdown.md","sourceFileName":"post-with-new-markdown.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/post-with-new-markdown"},"type":"Post"},{"title":"Test image path","date":"2023-01-14T18:17:57.545Z","author":"Ash","description":"test post","slug":"test-image-path","image":"/images/blog/sum.png","draft":false,"categories":["Test"],"body":{"raw":"<!--StartFragment-->\n\n## Where does it come from?\n\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the <h5> cites</h5> of the word in classical **literature**, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by <Highlight><Flex align='center' css={{ marginTop: '$10' }}>\n  <Icon name='back' size={20} />\n  <Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  </Text>\n</Flex></Highlight>\n\n<!--EndFragment-->\n\n```jsx\n<Flex align='center' css={{ marginTop: '$10' }}>\n  <Icon name='back' size={20} />\n  <Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  </Text>\n</Flex>\n```","html":"<h2>Where does it come from?</h2>\n<p>Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the  cites of the word in classical <strong>literature</strong>, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32</p>\n<p>The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by &#x3C;Flex align='center' css={{ marginTop: '$10' }}>\n\n&#x3C;Text size={3} css={{ fontWeight: '$3' }}>\nGo Back\n\n</p>\n<pre><code class=\"language-jsx\">&#x3C;Flex align='center' css={{ marginTop: '$10' }}>\n  &#x3C;Icon name='back' size={20} />\n  &#x3C;Text size={3} css={{ fontWeight: '$3' }}>\n    Go Back\n  &#x3C;/Text>\n&#x3C;/Flex>\n</code></pre>"},"_id":"posts/test-image-path.md","_raw":{"sourceFilePath":"posts/test-image-path.md","sourceFileName":"test-image-path.md","sourceFileDir":"posts","contentType":"markdown","flattenedPath":"posts/test-image-path"},"type":"Post"}]},"__N_SSG":true}